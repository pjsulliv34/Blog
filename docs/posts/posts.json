[
  {
    "path": "posts/crime-in-the-time-of-covid/",
    "title": "Crime in the Time of Covid",
    "description": "A closer look into the relationship between crime and covid.",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-04",
    "categories": [],
    "contents": "\r\r\nIntroduction\r\r\nIn this hectic time of Covid 19 the world is changing, and its changing quickly. So quickly in fact that we barely have enough data to understand how these pandemic is really affecting our society. In this paper I will take a closer look into the effect of the covid Pandemic on Crime rates in the United States. I’ve obtained Crime Statistics from the Federal Bureau of Investigation (FBI), and Covid Statistics from CDC.\r\r\nUtilizing the FBI’s and the CDC’s publicly available data, I’m proposing that Crime rates in the are United States are inversely related to the current covid 19 Hype in the United States. In other words, the stronger the covid 19 hype in a particular state, the lower the expected crimes.\r\r\nMethod\r\r\nGather, Clean and Prep\r\r\nThe first Step is to read in the Data. I obtained the Arrests by State records for the United States for 2020 and 2019. After I read in the data, the next step is to clean the data in order to prepare for the analysis\r\r\nWhen pulling in the data, I noticed multiple issues. For example the column that included the State name was not showing the correct state for each row. The State names showed numbers when they should only have the name. The crime descriptor columns were written in ways that didn’t make sense. To fix these issues I used the following code below. Since the 2019 and 2020 csv sheets are in the same format, once I fixed one sheet, I just needed to repeat the same steps for the 2019 code.\r\r\n\r\r\n\r\r\n# 2020 Cleaning\r\r\nUSA_2020_crimes <- crime_2020 %>% fill(State)\r\r\nUSA_2020_crimes<-  USA_2020_crimes %>% mutate(States = removeNumbers(State))\r\r\nCrime_data <- rename(USA_2020_crimes,  \"Property Crimes\" = \"Property\\ncrime2\", \r\r\n\"Rape\" = \"Rape3\", \r\r\n\"Aggravated Assault\" = \"Aggravated\\nassault\",\r\r\n \"Drug and Abuse Violations\" = \"Drug \\nabuse\\nviolations\", \r\r\n\"Violent Crimes\" = \"Violent\\ncrime2\", \"Murder\" = \"Murder and\\nnonnegligent\\nmanslaughter\" )\r\r\nCrime_data$States[Crime_data$States == \"FLORIDA, \"]<- \"FLORIDA\"\r\r\n\r\r\n\r\r\n#2019 Cleaning\r\r\nUSA_2019_crimes <- crime_2019 %>% fill(State)\r\r\nUSA_2019_crimes<-  USA_2019_crimes %>% mutate(States = removeNumbers(State))\r\r\nCrime_data_2019 <- rename(USA_2019_crimes,  \"Property Crimes\" = \"Property\\ncrime2\", \r\r\n\"Rape\" = \"Rape3\",\r\r\n\"Aggravated Assault\" = \"Aggravated\\nassault\",\r\r\n\"Drug and Abuse Violations\" = \"Drug \\nabuse\\nviolations\",\r\r\n\"Violent Crimes\" = \"Violent\\ncrime2\", \"Murder\" = \"Murder and\\nnonnegligent\\nmanslaughter\" )\r\r\nCrime_data_2019$States[Crime_data_2019$States == \"FLORIDA, \" ] <- \"FLORIDA\"\r\r\n\r\r\n\r\r\n\r\r\nInitial Plots\r\r\nNow that the data is cleaned, I can now take initial glimpses into the data I’ve collected. To perform this I used the package “usmap”. The way this package works is that you use the fips_info from the usmap package to tell the plot “usmap” function which states, citys and regions you plan to map. For this project, I am focusing on all States. So before I can plot my crime data, I first need to join the fips_info with my crime data. That way the plot us map function knows which state to correctly link what ever values I plan on plotting. For this project I decided to focus on only a few crimes, Property crimes, Aggravated Assault, and Murder.\r\r\n\r\r\n\r\r\nlibrary(usmap)\r\r\nstate_info <- fips_info()\r\r\nstate_info <- state_info %>% mutate(\"States\" = toupper(full)) \r\r\ncrimes <- c(\"Property Crimes\", \"Aggravated Assault\",\"Murder\")\r\r\n\r\r\n# Plotting 2020 Crimes\r\r\nfor (crime in crimes){\r\r\n  x <-  Crime_data %>% filter(X2 ==\"Total all ages\") %>% select(States,crime) %>% \r\r\n    distinct(States, .keep_all = TRUE)\r\r\n  y <- full_join(x,state_info, by = \"States\")\r\r\n  z <- plot_usmap(data = y, values = crime, labels = TRUE, color = \"gray\")+ \r\r\nscale_fill_continuous(low = \"white\", high = \"blue\", name = paste(crime), \r\r\nlabel = scales::comma)+\r\r\n    labs(title = paste(crime,\"in the US\"), subtitle = \"2020 Crime DATA\")+\r\r\n    theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) +\r\r\ntheme(legend.position = \"right\")\r\r\n  print(z)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n# Plotting 2019 \r\r\nfor (crime in crimes){\r\r\n  x <-  Crime_data_2019 %>% filter(X2 ==\"Total all ages\") %>% select(States,crime) %>% distinct(States, .keep_all = TRUE)\r\r\n  y <- full_join(x,state_info, by = \"States\")\r\r\n  z <- plot_usmap(data = y, values = crime, labels = TRUE, color = \"gray\")+ scale_fill_continuous(low = \"white\", high = \"blue\", name = paste(crime), label = scales::comma)+\r\r\n    labs(title = paste(crime,\"in the US\"), subtitle = \"2019 Crime DATA\")+\r\r\n    theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n  print(z)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n2020 and 2019 Results\r\r\nIn all three figures for both 2020 and 2019, CA has the highest amount of crimes. There may be a lot of crime there, but this could also be due to the amount of population. Due to this issue, with out having the per-capita results, this data could be misleading. In order to really understand whats going on here I will need to dive deeper into the data.\r\r\nFuther Analysis\r\r\nTo truly understand what is happening, the total crimes themselves itself don’t matter. Instead I want to see the % change of crimes from 2019 to 2020. I will do this for all three crimes listed. Once I have the % change in crimes listed, then I can compare agains the covid data.\r\r\n\r\r\n\r\r\nCrime_data_2020_1 <- Crime_data %>% filter(X2 ==\"Total all ages\") %>% select(States,`Property Crimes`,`Aggravated Assault`,Murder) %>% distinct(States, .keep_all = TRUE)\r\r\n\r\r\nCrime_data_2019_1 <- Crime_data_2019 %>%  filter(X2 ==\"Total all ages\") %>% select(States,`Property Crimes`,`Aggravated Assault`,Murder) %>% distinct(States, .keep_all = TRUE) %>% rename(\"Property crime 2019\" = `Property Crimes`, \"Aggravated Assault 2019\"= `Aggravated Assault`, \"Murder 2019\" = Murder)\r\r\n\r\r\n# Join DATA and Create Percent change columns\r\r\n\r\r\nCrime_Change_data <- inner_join(Crime_data_2019_1,Crime_data_2020_1, by = \"States\")\r\r\nCrime_Change_data <- Crime_Change_data %>% mutate(\r\r\n  \"Property Crime % Change\" = (`Property Crimes` - `Property crime 2019`)/(`Property crime 2019`),\r\r\n  \"Aggravated Assualt % Change\" = (`Aggravated Assault` - `Aggravated Assault 2019`)/ (`Aggravated Assault 2019`),\r\r\n  \"Murder % Change\" = (Murder - `Murder 2019`)/`Murder 2019`\r\r\n)\r\r\n\r\r\n\r\r\n\r\r\nThe code block above shows how I combined the 2020 and 2019 data and then created a calculated column for each crime.\r\r\nPlot Percent Change of Crime DATA\r\r\n\r\r\n\r\r\ncrimes <- c(\"Property Crime % Change\", \"Aggravated Assualt % Change\",\"Murder % Change\")\r\r\n\r\r\n# Plotting 2020 Crimes\r\r\nfor (crime in crimes){\r\r\n  y <- full_join(Crime_Change_data,state_info, by = \"States\")\r\r\n  z <- plot_usmap(data = y, values = crime, labels = TRUE, color = \"gray\")+ \r\r\n     scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \r\r\n   midpoint = 0, space = \"Lab\", \r\r\n   name=\"% Change\")+\r\r\n    #scale_fill_continuous(low = \"yellow\", high = \"blue\", name = paste(crime), label = scales::comma)+\r\r\n    labs(title = paste(crime,\"in the US\"), subtitle = \"2020 Crime DATA\")+\r\r\n    theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n  print(z)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nThese results are much better then the previous maps above. As we can see California and Texas are no longer in the top states for any of the crimes.\r\r\nFor Property crimes we an increase in Georgia and West Virginia, and a decrease in Pennsylvania and Delaware. For Aggravated Assault, Pennsylvania was the lowest, with others showing little to no increase. It looks like Georgia and Alabama were the only states with an increase from 2019 to 2020 for aggravated assault. It looks like murder did increase country wide except for PA and NM.\r\r\nTo truly understand these changes, I will also give a table of the top and bottom states for each crimes % change.\r\r\n\r\r\n\r\r\nlibrary(knitr)\r\r\n\r\r\ncrimes <- c(\"Property Crime % Change\", \"Aggravated Assualt % Change\",\"Murder % Change\")\r\r\n\r\r\nCrime_Change_data %>% arrange(`Property Crime % Change`) %>% select(States,`Property Crime % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Decrease in Property Crime\")\r\r\n\r\r\n\r\r\nTable 1: Largest Decrease in Property Crime\r\r\nStates\r\r\nProperty Crime % Change\r\r\nPENNSYLVANIA\r\r\n-0.9700970\r\r\nMARYLAND\r\r\n-0.9150567\r\r\nALABAMA\r\r\n-0.6379310\r\r\nHAWAII\r\r\n-0.5644000\r\r\nMISSISSIPPI\r\r\n-0.5119228\r\r\n\r\r\nCrime_Change_data %>% arrange(desc(`Property Crime % Change`)) %>% select(States, `Property Crime % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Increase in Property Crime\")\r\r\n\r\r\n\r\r\nTable 1: Largest Increase in Property Crime\r\r\nStates\r\r\nProperty Crime % Change\r\r\nWEST VIRGINIA\r\r\n0.8412447\r\r\nGEORGIA\r\r\n0.5394141\r\r\nWYOMING\r\r\n0.0521376\r\r\nMISSOURI\r\r\n0.0196389\r\r\nNORTH CAROLINA\r\r\n-0.0105206\r\r\n\r\r\nCrime_Change_data %>% arrange(`Aggravated Assualt % Change`) %>% select(States,`Aggravated Assualt % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Decrease in Assault\")\r\r\n\r\r\n\r\r\nTable 1: Largest Decrease in Assault\r\r\nStates\r\r\nAggravated Assualt % Change\r\r\nPENNSYLVANIA\r\r\n-0.9767876\r\r\nMARYLAND\r\r\n-0.9482368\r\r\nHAWAII\r\r\n-0.4385965\r\r\nKENTUCKY\r\r\n-0.3846154\r\r\nMISSISSIPPI\r\r\n-0.3435028\r\r\n\r\r\nCrime_Change_data %>% arrange(desc(`Aggravated Assualt % Change`)) %>% select(States, `Aggravated Assualt % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Increase in Assault\")\r\r\n\r\r\n\r\r\nTable 1: Largest Increase in Assault\r\r\nStates\r\r\nAggravated Assualt % Change\r\r\nALABAMA\r\r\n6.0909091\r\r\nGEORGIA\r\r\n1.7259380\r\r\nSOUTH DAKOTA\r\r\n0.8287129\r\r\nNORTH DAKOTA\r\r\n0.4169381\r\r\nWEST VIRGINIA\r\r\n0.3105968\r\r\n\r\r\nCrime_Change_data %>% arrange(`Murder % Change`) %>% select(States,`Murder % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Decrease in Murder\")\r\r\n\r\r\n\r\r\nTable 1: Largest Decrease in Murder\r\r\nStates\r\r\nMurder % Change\r\r\nPENNSYLVANIA\r\r\n-0.9714286\r\r\nMARYLAND\r\r\n-0.9370079\r\r\nNEW HAMPSHIRE\r\r\n-0.8666667\r\r\nNEW MEXICO\r\r\n-0.5918367\r\r\nKENTUCKY\r\r\n-0.3229814\r\r\n\r\r\nCrime_Change_data %>% arrange(desc(`Murder % Change`)) %>% select(States, `Murder % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Increase in Murder\")\r\r\n\r\r\n\r\r\nTable 1: Largest Increase in Murder\r\r\nStates\r\r\nMurder % Change\r\r\nDISTRICT OF COLUMBIA\r\r\nInf\r\r\nGEORGIA\r\r\n1.8857143\r\r\nALABAMA\r\r\n1.0000000\r\r\nWEST VIRGINIA\r\r\n0.8750000\r\r\nINDIANA\r\r\n0.8309859\r\r\n\r\r\nResults\r\r\nThe Tables above show the top 5 states per crime for largest increase in percentage and the top 5 states for the largest decrease in percentage. The largest decrease for all three crimes is from Pennsylvania and that was at 1%. The largest increase was for Alabama, which was an 6% increase in aggravated assault.\r\r\nMethod Continued\r\r\nNow that I have a good idea of What states saw the largest increases and decreases in crime. I will now correlate that over to Covid Data provided by CDC.\r\r\nCovid DATA\r\r\nWhen dealing with Covid, there are many different metrics that can be utilized. We could look at daily counts of infected, daily deaths, hospitalizations, or even total vaccinations. For this analysis I am only going to focus on the amount of deaths recorded in 2020 by state.\r\r\nThe plan is to compare the % change by the three crimes listed above by yearly total of deaths caused by Covid 19.\r\r\nCleaning and Prepping DATA\r\r\n\r\r\n\r\r\n#str(covid_data)\r\r\ncovid_data$`End Date` <- as.Date(covid_data$`End Date`,\"%m/%d/%Y\")\r\r\n#str(covid_data)\r\r\n#covid_data\r\r\nfiltered_covid <- covid_data %>% filter(`End Date`< \"2021-01-01\") %>% filter(`Place of Death` == \"Total - All Places of Death\" & State != \"United States\" & Group == \"By Year\") %>% mutate(\"States\" = toupper(State) ) %>% select(States, `COVID-19 Deaths`)\r\r\n\r\r\n\r\r\n\r\r\nI First needed to clean and prep the data. I changed the dates column from a character type to a date type and filtered out the unnecessary information. I then plotted the deaths and normalized that data across all states using the plot “usmap” function.\r\r\n\r\r\n\r\r\n covid_mapdata <- full_join(filtered_covid,state_info, by = \"States\")\r\r\ndata_map <- covid_mapdata %>% select(fips, `COVID-19 Deaths`) %>% rename(\"Deaths\" = `COVID-19 Deaths`)\r\r\n\r\r\ndata_map <- data.frame(data_map)\r\r\n\r\r\n\r\r\nplot_usmap(data = data_map, values = \"Deaths\",labels = TRUE, color = \"gray\")+\r\r\n  scale_fill_continuous(low = \"white\", high = \"blue\", name = \"Covid Fatalities\", label = scales::comma)+\r\r\n    labs(title = \"Covid Fatalitys in 2020\")+\r\r\n   theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nIt looks like California and Texas had the largest amount of deaths around 30K. This info can be misleading, since states with lower populations will most likely have lower deaths but those deaths could be a higher percentage of the total population. To rectify this I will bring in the 2020 population data from the US Census for 2020, and I will create a field that is the amount of deaths per 100K population.\r\r\n\r\r\n\r\r\npopulation_2020 <- readxl::read_xlsx(\"2020 population.xlsx\")\r\r\n\r\r\n\r\r\n\r\r\npopulation_2020$State <- toupper(population_2020$State)\r\r\npopulation_2020 <- rename(population_2020, \"States\" = State)\r\r\n\r\r\nCovid_data_updated <- inner_join(covid_mapdata, population_2020, by =\"States\")\r\r\nCovid_data_updated <- Covid_data_updated %>% mutate(\r\r\n  Per_capita = (`COVID-19 Deaths`/`2020 Census`)*100000\r\r\n)\r\r\n\r\r\n\r\r\nplot_usmap(data = Covid_data_updated, values = \"Per_capita\",labels = TRUE, color = \"gray\")+\r\r\n  scale_fill_continuous(low = \"white\", high = \"blue\", name = \"Covid Fatalities\", label = scales::comma)+\r\r\n    labs(title = \"Covid Fatalitys in 2020\")+\r\r\n   theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nAs we can see, the chart looks much different than the previous one. It looks like North Dakota, South Dakota and Delaware had the highest fatalities per 100k population. Hawaii had the lowest. I will now create a data table to look at the top and bottom % per crime vs the top and bottom percentage per fatalities.\r\r\n\r\r\n\r\r\nTop_covid <- Covid_data_updated %>% arrange(desc(Per_capita)) %>% select(States,Per_capita) %>% slice(1:5)\r\r\n\r\r\nLow_covid <- Covid_data_updated %>% arrange((Per_capita)) %>% select(States,Per_capita) %>% slice(1:5)\r\r\n\r\r\n\r\r\nTop_covid %>% kable(caption = \"Highest Covid Fatalities by State\")\r\r\n\r\r\n\r\r\nTable 2: Highest Covid Fatalities by State\r\r\nStates\r\r\nPer_capita\r\r\nNEW JERSEY\r\r\n195.3172\r\r\nNORTH DAKOTA\r\r\n194.1999\r\r\nSOUTH DAKOTA\r\r\n193.4210\r\r\nRHODE ISLAND\r\r\n174.7801\r\r\nCONNECTICUT\r\r\n174.4619\r\r\n\r\r\nLow_covid %>% kable(caption = \"Lowest Covid Fatalities by State\")\r\r\n\r\r\n\r\r\nTable 2: Lowest Covid Fatalities by State\r\r\nStates\r\r\nPer_capita\r\r\nVERMONT\r\r\n22.54784\r\r\nHAWAII\r\r\n25.08124\r\r\nMAINE\r\r\n34.13197\r\r\nALASKA\r\r\n34.63364\r\r\nOREGON\r\r\n38.04349\r\r\n\r\r\nx1<- Crime_Change_data %>% arrange(`Property Crime % Change`) %>% select(States,`Property Crime % Change`) %>% slice(1:5) \r\r\n\r\r\ny_1<-Crime_Change_data %>% arrange(desc(`Property Crime % Change`)) %>% select(States, `Property Crime % Change`) %>% slice(1:5) \r\r\n\r\r\nx2<- Crime_Change_data %>% arrange(`Aggravated Assualt % Change`) %>% select(States,`Aggravated Assualt % Change`) %>% slice(1:5) \r\r\ny2<- Crime_Change_data %>% arrange(desc(`Aggravated Assualt % Change`)) %>% select(States, `Aggravated Assualt % Change`) %>% slice(1:5)\r\r\n\r\r\nx3<- Crime_Change_data %>% arrange(`Murder % Change`) %>% select(States,`Murder % Change`) %>% slice(1:5)\r\r\ny3<- Crime_Change_data %>% arrange(desc(`Murder % Change`)) %>% select(States, `Murder % Change`) %>% slice(1:5)\r\r\n\r\r\n\r\r\n\r\r\ncbind(Top_covid,x1,x2,x3) \r\r\n\r\r\n\r\r\n        States Per_capita       States Property Crime % Change\r\r\n1   NEW JERSEY   195.3172 PENNSYLVANIA              -0.9700970\r\r\n2 NORTH DAKOTA   194.1999     MARYLAND              -0.9150567\r\r\n3 SOUTH DAKOTA   193.4210      ALABAMA              -0.6379310\r\r\n4 RHODE ISLAND   174.7801       HAWAII              -0.5644000\r\r\n5  CONNECTICUT   174.4619  MISSISSIPPI              -0.5119228\r\r\n        States Aggravated Assualt % Change        States\r\r\n1 PENNSYLVANIA                  -0.9767876  PENNSYLVANIA\r\r\n2     MARYLAND                  -0.9482368      MARYLAND\r\r\n3       HAWAII                  -0.4385965 NEW HAMPSHIRE\r\r\n4     KENTUCKY                  -0.3846154    NEW MEXICO\r\r\n5  MISSISSIPPI                  -0.3435028      KENTUCKY\r\r\n  Murder % Change\r\r\n1      -0.9714286\r\r\n2      -0.9370079\r\r\n3      -0.8666667\r\r\n4      -0.5918367\r\r\n5      -0.3229814\r\r\n\r\r\ncbind(Low_covid,y_1,y2,y3)\r\r\n\r\r\n\r\r\n   States Per_capita         States Property Crime % Change\r\r\n1 VERMONT   22.54784  WEST VIRGINIA              0.84124473\r\r\n2  HAWAII   25.08124        GEORGIA              0.53941411\r\r\n3   MAINE   34.13197        WYOMING              0.05213764\r\r\n4  ALASKA   34.63364       MISSOURI              0.01963886\r\r\n5  OREGON   38.04349 NORTH CAROLINA             -0.01052062\r\r\n         States Aggravated Assualt % Change               States\r\r\n1       ALABAMA                   6.0909091 DISTRICT OF COLUMBIA\r\r\n2       GEORGIA                   1.7259380              GEORGIA\r\r\n3  SOUTH DAKOTA                   0.8287129              ALABAMA\r\r\n4  NORTH DAKOTA                   0.4169381        WEST VIRGINIA\r\r\n5 WEST VIRGINIA                   0.3105968              INDIANA\r\r\n  Murder % Change\r\r\n1             Inf\r\r\n2       1.8857143\r\r\n3       1.0000000\r\r\n4       0.8750000\r\r\n5       0.8309859\r\r\n\r\r\n\r\r\n\r\r\nlibrary(knitr)\r\r\n\r\r\nunemployment_data <- read_csv(\"unemployement_rates.csv\")\r\r\nunemployment_data$States = toupper(unemployment_data$States)\r\r\nunemployment_data <- unemployment_data %>% mutate(UnemploymentRate_change = (`unemployment rate 2020`- `unemployment rate 2019`)/`unemployment rate 2020`)\r\r\n\r\r\nunemployment_data %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 12\r\r\n  States     `Pop 2019` `pop 2020` `labor force 201~ `labor force 202~\r\r\n  <chr>           <dbl>      <dbl>             <dbl>             <dbl>\r\r\n1 UNITED ST~     259175     260329            163539            160742\r\r\n2 NORTHEAST       45145      45097             28598             28013\r\r\n3 NEW ENGLA~      12136      12162              8072              7841\r\r\n4 CONNECTIC~       2885       2883              1917              1873\r\r\n5 MAINE            1112       1118               696               677\r\r\n6 MASSACHUS~       5636       5648              3782              3658\r\r\n# ... with 7 more variables: employed 2019 <dbl>,\r\r\n#   employed 2020 <dbl>, Unemployed 2019 <dbl>,\r\r\n#   unemployed 2020 <dbl>, unemployment rate 2019 <dbl>,\r\r\n#   unemployment rate 2020 <dbl>, UnemploymentRate_change <dbl>\r\r\n\r\r\nCrime_Change_data %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 10\r\r\n  States     `Property crime 20~ `Aggravated Assault 20~ `Murder 2019`\r\r\n  <chr>                    <dbl>                   <dbl>         <dbl>\r\r\n1 ALABAMA                    522                      11             4\r\r\n2 ALASKA                    3006                    2077            46\r\r\n3 ARIZONA                  29328                    8967           294\r\r\n4 ARKANSAS                 13433                    4120           166\r\r\n5 CALIFORNIA               88854                   83584          1284\r\r\n6 COLORADO                 25261                    6226           152\r\r\n# ... with 6 more variables: Property Crimes <dbl>,\r\r\n#   Aggravated Assault <dbl>, Murder <dbl>,\r\r\n#   Property Crime % Change <dbl>, Aggravated Assualt % Change <dbl>,\r\r\n#   Murder % Change <dbl>\r\r\n\r\r\nCovid_data_updated %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 9\r\r\n  States    `COVID-19 Deaths` abbr  fips  full      Rank `2020 Census`\r\r\n  <chr>                 <dbl> <chr> <chr> <chr>    <dbl>         <dbl>\r\r\n1 ALABAMA                6706 AL    01    Alabama     24       5024279\r\r\n2 ALASKA                  254 AK    02    Alaska      48        733391\r\r\n3 ARIZONA                9321 AZ    04    Arizona     14       7151502\r\r\n4 ARKANSAS               4027 AR    05    Arkansas    33       3011524\r\r\n5 CALIFORN~             33524 CA    06    Califor~     1      39538223\r\r\n6 COLORADO               5073 CO    08    Colorado    21       5773714\r\r\n# ... with 2 more variables: Percent of Total <dbl>, Per_capita <dbl>\r\r\n\r\r\nTotal_DATA_1 <- inner_join(Crime_Change_data,unemployment_data, by = \"States\")\r\r\nTotal_DATA_2 <- inner_join(Total_DATA_1, Covid_data_updated, by = \"States\")\r\r\nTotal_DATA_2 %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 29\r\r\n  States     `Property crime 20~ `Aggravated Assault 20~ `Murder 2019`\r\r\n  <chr>                    <dbl>                   <dbl>         <dbl>\r\r\n1 ALABAMA                    522                      11             4\r\r\n2 ALASKA                    3006                    2077            46\r\r\n3 ARIZONA                  29328                    8967           294\r\r\n4 ARKANSAS                 13433                    4120           166\r\r\n5 CALIFORNIA               88854                   83584          1284\r\r\n6 COLORADO                 25261                    6226           152\r\r\n# ... with 25 more variables: Property Crimes <dbl>,\r\r\n#   Aggravated Assault <dbl>, Murder <dbl>,\r\r\n#   Property Crime % Change <dbl>, Aggravated Assualt % Change <dbl>,\r\r\n#   Murder % Change <dbl>, Pop 2019 <dbl>, pop 2020 <dbl>,\r\r\n#   labor force 2019 <dbl>, labor force 2020 <dbl>,\r\r\n#   employed 2019 <dbl>, employed 2020 <dbl>, Unemployed 2019 <dbl>,\r\r\n#   unemployed 2020 <dbl>, unemployment rate 2019 <dbl>,\r\r\n#   unemployment rate 2020 <dbl>, UnemploymentRate_change <dbl>,\r\r\n#   COVID-19 Deaths <dbl>, abbr <chr>, fips <chr>, full <chr>,\r\r\n#   Rank <dbl>, 2020 Census <dbl>, Percent of Total <dbl>,\r\r\n#   Per_capita <dbl>\r\r\n\r\r\ncorr_data<- Total_DATA_2 %>% select(`Property Crime % Change`:`Murder % Change`,UnemploymentRate_change,`COVID-19 Deaths`,Per_capita)\r\r\ncorr_data <- corr_data %>% rename(\"COVID 19 Deaths Per Capita\" = Per_capita)\r\r\ncor(corr_data) %>% round(3)%>%kable()\r\r\n\r\r\n\r\r\n\r\r\nProperty Crime % Change\r\r\nAggravated Assualt % Change\r\r\nMurder % Change\r\r\nUnemploymentRate_change\r\r\nCOVID-19 Deaths\r\r\nCOVID 19 Deaths Per Capita\r\r\nProperty Crime % Change\r\r\n1.000\r\r\n0.098\r\r\n0.651\r\r\n-0.148\r\r\n-0.112\r\r\n-0.112\r\r\nAggravated Assualt % Change\r\r\n0.098\r\r\n1.000\r\r\n0.552\r\r\n-0.069\r\r\n-0.046\r\r\n0.103\r\r\nMurder % Change\r\r\n0.651\r\r\n0.552\r\r\n1.000\r\r\n-0.030\r\r\n-0.002\r\r\n0.056\r\r\nUnemploymentRate_change\r\r\n-0.148\r\r\n-0.069\r\r\n-0.030\r\r\n1.000\r\r\n0.262\r\r\n-0.114\r\r\nCOVID-19 Deaths\r\r\n-0.112\r\r\n-0.046\r\r\n-0.002\r\r\n0.262\r\r\n1.000\r\r\n0.192\r\r\nCOVID 19 Deaths Per Capita\r\r\n-0.112\r\r\n0.103\r\r\n0.056\r\r\n-0.114\r\r\n0.192\r\r\n1.000\r\r\n\r\r\n\r\r\n\r\r\nlibrary(reshape2)\r\r\n# Create a heatmap for cor matrix\r\r\ncorr_matrix <- cor(corr_data)\r\r\nmelted <- melt(corr_matrix)\r\r\nmelted %>% head()\r\r\n\r\r\n\r\r\n                         Var1                    Var2       value\r\r\n1     Property Crime % Change Property Crime % Change  1.00000000\r\r\n2 Aggravated Assualt % Change Property Crime % Change  0.09833195\r\r\n3             Murder % Change Property Crime % Change  0.65074643\r\r\n4     UnemploymentRate_change Property Crime % Change -0.14820711\r\r\n5             COVID-19 Deaths Property Crime % Change -0.11166874\r\r\n6  COVID 19 Deaths Per Capita Property Crime % Change -0.11235439\r\r\n\r\r\nmelted$Var1 <- as.character(melted$Var1)\r\r\nmelted$Var2 <- as.character(melted$Var2)\r\r\n\r\r\n\r\r\nggplot(data = melted, aes(Var2, Var1, fill = value))+\r\r\n geom_tile(color = \"white\")+\r\r\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \r\r\n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \r\r\n   name=\"Corr Matrix\") +\r\r\n  theme_minimal()+ \r\r\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \r\r\n    size = 12, hjust = 1))+\r\r\n coord_fixed()+\r\r\n  xlab(\"\")+\r\r\n  ylab(\"\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n# Corr Matrix''''\r\r\ncorr_data%>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 6\r\r\n  `Property Crime ~ `Aggravated Ass~ `Murder % Chang~ UnemploymentRat~\r\r\n              <dbl>            <dbl>            <dbl>            <dbl>\r\r\n1           -0.638           6.09               1                0.492\r\r\n2           -0.203           0.0197            -0.283            0.308\r\r\n3           -0.0922          0.0181             0.156            0.380\r\r\n4           -0.187           0.122              0.175            0.426\r\r\n5           -0.179          -0.00353            0.241            0.584\r\r\n6           -0.151           0.0895             0.553            0.630\r\r\n# ... with 2 more variables: COVID-19 Deaths <dbl>,\r\r\n#   COVID 19 Deaths Per Capita <dbl>\r\r\n\r\r\n pivot_data <- corr_data %>% pivot_longer(!c(`COVID-19 Deaths`,`COVID 19 Deaths Per Capita`),\r\r\n                                          \r\r\n    names_to = \"Factor\", \r\r\n    values_to = \"Percent_Change\",\r\r\n  )\r\r\n\r\r\n pivot_data\r\r\n\r\r\n\r\r\n# A tibble: 200 x 4\r\r\n   `COVID-19 Deaths` `COVID 19 Deaths Pe~ Factor        Percent_Change\r\r\n               <dbl>                <dbl> <chr>                  <dbl>\r\r\n 1              6706                133.  Property Cri~        -0.638 \r\r\n 2              6706                133.  Aggravated A~         6.09  \r\r\n 3              6706                133.  Murder % Cha~         1     \r\r\n 4              6706                133.  Unemployment~         0.492 \r\r\n 5               254                 34.6 Property Cri~        -0.203 \r\r\n 6               254                 34.6 Aggravated A~         0.0197\r\r\n 7               254                 34.6 Murder % Cha~        -0.283 \r\r\n 8               254                 34.6 Unemployment~         0.308 \r\r\n 9              9321                130.  Property Cri~        -0.0922\r\r\n10              9321                130.  Aggravated A~         0.0181\r\r\n# ... with 190 more rows\r\r\n\r\r\n pivot_data %>% ggplot()+\r\r\n   geom_point(aes(x = `COVID-19 Deaths`, y = Percent_Change ), color = \"deepskyblue2\")+geom_smooth(aes(x = `COVID-19 Deaths`, y = Percent_Change), se = F)+\r\r\n   facet_wrap(~Factor, scales = \"free\")\r\r\n\r\r\n\r\r\n\r\r\n  pivot_data\r\r\n\r\r\n\r\r\n# A tibble: 200 x 4\r\r\n   `COVID-19 Deaths` `COVID 19 Deaths Pe~ Factor        Percent_Change\r\r\n               <dbl>                <dbl> <chr>                  <dbl>\r\r\n 1              6706                133.  Property Cri~        -0.638 \r\r\n 2              6706                133.  Aggravated A~         6.09  \r\r\n 3              6706                133.  Murder % Cha~         1     \r\r\n 4              6706                133.  Unemployment~         0.492 \r\r\n 5               254                 34.6 Property Cri~        -0.203 \r\r\n 6               254                 34.6 Aggravated A~         0.0197\r\r\n 7               254                 34.6 Murder % Cha~        -0.283 \r\r\n 8               254                 34.6 Unemployment~         0.308 \r\r\n 9              9321                130.  Property Cri~        -0.0922\r\r\n10              9321                130.  Aggravated A~         0.0181\r\r\n# ... with 190 more rows\r\r\n\r\r\n pivot_data %>% ggplot()+\r\r\n   geom_point(aes(x = `COVID 19 Deaths Per Capita`, y = Percent_Change ), color = \"deepskyblue2\")+geom_smooth(aes(x = `COVID 19 Deaths Per Capita`, y = Percent_Change), se = F)+\r\r\n   facet_wrap(~Factor, scales = \"free\")\r\r\n\r\r\n\r\r\n\r\r\n pivot_data \r\r\n\r\r\n\r\r\n# A tibble: 200 x 4\r\r\n   `COVID-19 Deaths` `COVID 19 Deaths Pe~ Factor        Percent_Change\r\r\n               <dbl>                <dbl> <chr>                  <dbl>\r\r\n 1              6706                133.  Property Cri~        -0.638 \r\r\n 2              6706                133.  Aggravated A~         6.09  \r\r\n 3              6706                133.  Murder % Cha~         1     \r\r\n 4              6706                133.  Unemployment~         0.492 \r\r\n 5               254                 34.6 Property Cri~        -0.203 \r\r\n 6               254                 34.6 Aggravated A~         0.0197\r\r\n 7               254                 34.6 Murder % Cha~        -0.283 \r\r\n 8               254                 34.6 Unemployment~         0.308 \r\r\n 9              9321                130.  Property Cri~        -0.0922\r\r\n10              9321                130.  Aggravated A~         0.0181\r\r\n# ... with 190 more rows\r\r\n\r\r\n\r\r\n\r\r\nlibrary(texreg); library(lmtest)\r\r\n\r\r\n# Run Regression Analysis\r\r\n\r\r\n# Outcome Variable - Crime\r\r\n# Assualt\r\r\n\r\r\nlpm_assault <- lm(`Aggravated Assualt % Change`  ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita` +\r\r\n                    UnemploymentRate_change , data = corr_data)\r\r\n\r\r\n\r\r\n\r\r\nlpm_Property_crime <- lm(`Property Crime % Change` ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita`\r\r\n                         +UnemploymentRate_change, data = corr_data)\r\r\n\r\r\n\r\r\nlpm_Murder<- lm(`Murder % Change` ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita` \r\r\n                +UnemploymentRate_change, data = corr_data)\r\r\n\r\r\n\r\r\n\r\r\nscreenreg(list(lpm_assault,lpm_Property_crime,lpm_Murder), custom.header = list(\"Crime LPM's\" = 1:3),custom.model.names = c(\"Assault\",\"Murder\",\"Property\"))\r\r\n\r\r\n\r\r\n\r\r\n=======================================================\r\r\n                                     Crime LPM's       \r\r\n                              -------------------------\r\r\n                              Assault  Murder  Property\r\r\n-------------------------------------------------------\r\r\n(Intercept)                    0.10     0.08    0.14   \r\r\n                              (0.83)   (0.24)  (0.43)  \r\r\n`COVID-19 Deaths`             -0.00    -0.00   -0.00   \r\r\n                              (0.00)   (0.00)  (0.00)  \r\r\n`COVID 19 Deaths Per Capita`   0.00    -0.00    0.00   \r\r\n                              (0.00)   (0.00)  (0.00)  \r\r\nUnemploymentRate_change       -0.39    -0.40   -0.10   \r\r\n                              (1.41)   (0.41)  (0.73)  \r\r\n-------------------------------------------------------\r\r\nR^2                            0.02     0.04    0.00   \r\r\nAdj. R^2                      -0.05    -0.02   -0.06   \r\r\nNum. obs.                     50       50      50      \r\r\n=======================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\nsummary(lpm_assault)\r\r\n\r\r\n\r\r\n\r\r\nCall:\r\r\nlm(formula = `Aggravated Assualt % Change` ~ `COVID-19 Deaths` + \r\r\n    `COVID 19 Deaths Per Capita` + UnemploymentRate_change, data = corr_data)\r\r\n\r\r\nResiduals:\r\r\n    Min      1Q  Median      3Q     Max \r\r\n-1.0910 -0.2011 -0.0874  0.0075  5.9100 \r\r\n\r\r\nCoefficients:\r\r\n                               Estimate Std. Error t value Pr(>|t|)\r\r\n(Intercept)                   1.020e-01  8.286e-01   0.123    0.903\r\r\n`COVID-19 Deaths`            -6.872e-06  1.926e-05  -0.357    0.723\r\r\n`COVID 19 Deaths Per Capita`  2.367e-03  3.281e-03   0.721    0.474\r\r\nUnemploymentRate_change      -3.885e-01  1.414e+00  -0.275    0.785\r\r\n\r\r\nResidual standard error: 0.962 on 46 degrees of freedom\r\r\nMultiple R-squared:  0.01673,   Adjusted R-squared:  -0.04739 \r\r\nF-statistic: 0.2609 on 3 and 46 DF,  p-value: 0.8531\r\r\n\r\r\n\r\r\n\r\r\n# Assualt\r\r\nLpm_unemployment <- lm(UnemploymentRate_change  ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita` + `Property Crime % Change`+`Murder % Change`+`Aggravated Assualt % Change` , data = corr_data)\r\r\n\r\r\nsummary(Lpm_unemployment)\r\r\n\r\r\n\r\r\n\r\r\nCall:\r\r\nlm(formula = UnemploymentRate_change ~ `COVID-19 Deaths` + `COVID 19 Deaths Per Capita` + \r\r\n    `Property Crime % Change` + `Murder % Change` + `Aggravated Assualt % Change`, \r\r\n    data = corr_data)\r\r\n\r\r\nResiduals:\r\r\n     Min       1Q   Median       3Q      Max \r\r\n-0.22078 -0.05388 -0.01327  0.05639  0.22991 \r\r\n\r\r\nCoefficients:\r\r\n                                Estimate Std. Error t value Pr(>|t|)\r\r\n(Intercept)                    5.005e-01  4.224e-02  11.849 2.78e-15\r\r\n`COVID-19 Deaths`              3.508e-06  1.949e-06   1.800   0.0787\r\r\n`COVID 19 Deaths Per Capita`  -4.620e-04  3.426e-04  -1.349   0.1843\r\r\n`Property Crime % Change`     -1.083e-01  7.765e-02  -1.395   0.1699\r\r\n`Murder % Change`              5.317e-02  5.187e-02   1.025   0.3109\r\r\n`Aggravated Assualt % Change` -1.603e-02  2.021e-02  -0.793   0.4318\r\r\n                                 \r\r\n(Intercept)                   ***\r\r\n`COVID-19 Deaths`             .  \r\r\n`COVID 19 Deaths Per Capita`     \r\r\n`Property Crime % Change`        \r\r\n`Murder % Change`                \r\r\n`Aggravated Assualt % Change`    \r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nResidual standard error: 0.1003 on 44 degrees of freedom\r\r\nMultiple R-squared:  0.1363,    Adjusted R-squared:  0.03815 \r\r\nF-statistic: 1.389 on 5 and 44 DF,  p-value: 0.247\r\r\n\r\r\nResults\r\r\nFor this project, I decided to just focus on property crimes, murder, and aggravated assault as the crimes of interest in this analysis. When property crimes, most of the states saw a decrease in property crime in 2020 when compared to 2021. Pennsylvania saw the largest decrease in property crime while Georgia and West Virginia saw the largest increases in property crime. It appears that most of the states saw an increase in murders from 2019 to 2020, with Georgia seeing the largest increase. There was very little percent change in either direction across all states for aggravated assault, except for Alabama, where we saw a 6% increase in aggravated assault. The correlation Matrix shows a high correlation between murder percent change and aggravated assault percent change. It also looks like murder has a high correlation with property crime percent change. COVID-19 deaths shows a negative correlation with property crimes and a positive correlation with unemployment rate. COVID-19 deaths per-capita, surprisingly, has a negative correlation with unemployment rate. I created three unrestricted linear regression models, one for each crime. When looking at Figure 8, the murder linear model explained the largest amount of variance at 4%. None of the models had any statistically significant covariates.\r\r\nConclusion\r\r\nCrime rates in the United States are inversely proportional to the number of COVID-19 deaths for the crimes analyzed in this project. When deaths per capita were introduced, COVID-19 deaths per capita were proportional to assault and property crimes but inversely related to murder. Surprisingly, unemployment rates seem to be inversely proportional to crime rates.\r\r\nIt should be noted that none of the models have any covariates that are statistically significant (all p-values were >.05). For future research, I would recommend creating a larger sample size and looking into new crimes. It would also be beneficial to create smaller groups by states and regions and incorporating local policies such as stay-at-home orders.\r\r\nReferences\r\r\nCrime, https://crime-data-explorer.fr.cloud.gov/pages/home. “Table 1. Employment Status of the Civilian Noninstitutional Population 16 Years of Age and over by Region, Division, and State, 2019-20 Annual Averages.” U.S. Bureau of Labor Statistics, U.S. Bureau of Labor Statistics, 3 Mar. 2021, https://www.bls.gov/news.release/srgune.t01.htm. “Provisional COVID-19 Deaths by Place of Death and State.” Centers for Disease Control and Prevention, Centers for Disease Control and Prevention, https://data.cdc.gov/NCHS/Provisional-COVID-19-Deaths-by-Place-of-Death-and-/uggs-hy5q.\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/crime-in-the-time-of-covid/distill-preview.png",
    "last_modified": "2022-02-04T10:10:34-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-03-networkanalysishw1/",
    "title": "NetworkAnalysisHw1",
    "description": "A closer Look into Airport Data.",
    "author": [
      {
        "name": "Peter",
        "url": {}
      }
    ],
    "date": "2022-02-03",
    "categories": [],
    "contents": "\r\n\r\nLoad in DATA. This is the Airport DATA from the Google drive.\r\n\r\nLooking at Nodes and Edges:\r\n\r\n\r\nls()\r\n\r\n\r\n[1] \"network_edgelist\" \"network_igraph\"   \"network_nodes\"   \r\n[4] \"network_statnet\" \r\n\r\nvcount(network_igraph)\r\n\r\n\r\n[1] 755\r\n\r\necount(network_igraph)\r\n\r\n\r\n[1] 23473\r\n\r\nprint(network_statnet)\r\n\r\n\r\n Network attributes:\r\n  vertices = 755 \r\n  directed = TRUE \r\n  hyper = FALSE \r\n  loops = FALSE \r\n  multiple = FALSE \r\n  bipartite = FALSE \r\n  total edges= 8228 \r\n    missing edges= 0 \r\n    non-missing edges= 8228 \r\n\r\n Vertex attribute names: \r\n    City Distance vertex.names \r\n\r\n Edge attribute names not shown \r\n\r\n#print(network_igraph)\r\n\r\n\r\n\r\nRight off the bat, it looks like the igraph and statnet variables are showing different edges. The network igraph is showing 755 nodes and 23473 edges. The network statnet is showing 755 nodes, and 8228 edges. \r\nWeighted, Directed, Single Mode Network?\r\n\r\n\r\nis_bipartite(network_igraph)\r\n\r\n\r\n[1] FALSE\r\n\r\nis_directed(network_igraph)\r\n\r\n\r\n[1] TRUE\r\n\r\nis_weighted(network_igraph)\r\n\r\n\r\n[1] FALSE\r\n\r\nUsing the Network Igraph set, we have a single mode network, which is directed, and is not weighted.\r\n\r\nLooking at Vertex and Edge Attributes:\r\n\r\n\r\nvertex_attr_names(network_igraph)\r\n\r\n\r\n[1] \"name\"     \"City\"     \"Position\"\r\n\r\nnetwork::list.vertex.attributes(network_statnet)\r\n\r\n\r\n[1] \"City\"         \"Distance\"     \"na\"           \"vertex.names\"\r\n\r\nedge_attr_names(network_igraph)\r\n\r\n\r\n[1] \"Carrier\"    \"Departures\" \"Seats\"      \"Passengers\" \"Aircraft\"  \r\n[6] \"Distance\"  \r\n\r\nnetwork::list.edge.attributes(network_statnet)\r\n\r\n\r\n[1] \"Aircraft\"   \"Carrier\"    \"Departures\" \"Distance\"   \"na\"        \r\n[6] \"Passangers\" \"Seats\"      \"weight\"    \r\n\r\nIgraph Attribute Names: name, City, Position\r\nIgraph edge names: Carrier, Departures, Seats, Passengers, Aircraft, Distance\r\nStatnet attribute names: City, Distance, na, vertex.names\r\nstatnet edge names: Aircraft, Carrier, Departures, Distance, na, Passangers, Seats, weight\r\n\r\nAccessing Attribute DATA:\r\n\r\n\r\nV(network_igraph)$name %>% head()\r\n\r\n\r\n[1] \"BGR\" \"BOS\" \"ANC\" \"JFK\" \"LAS\" \"MIA\"\r\n\r\nV(network_igraph)$City %>% head()\r\n\r\n\r\n[1] \"Bangor, ME\"    \"Boston, MA\"    \"Anchorage, AK\" \"New York, NY\" \r\n[5] \"Las Vegas, NV\" \"Miami, FL\"    \r\n\r\nV(network_igraph)$Position %>% head()\r\n\r\n\r\n[1] \"N444827 W0684941\" \"N422152 W0710019\" \"N611028 W1495947\"\r\n[4] \"N403823 W0734644\" \"N360449 W1150908\" \"N254736 W0801726\"\r\n\r\n(network_igraph)$Carrier %>% head()\r\n\r\n\r\nNULL\r\n\r\nhead(network_statnet %v% \"vertex.names\")\r\n\r\n\r\n[1] \"1G4\" \"A23\" \"A27\" \"A29\" \"ABE\" \"ABI\"\r\n\r\nhead(network_statnet %v% \"City\")\r\n\r\n\r\n[1] \"Bangor, ME\"    \"Boston, MA\"    \"Anchorage, AK\" \"New York, NY\" \r\n[5] \"Las Vegas, NV\" \"Miami, FL\"    \r\n\r\nhead(network_statnet %e% \"weight\")\r\n\r\n\r\n[1] \"193\"  \"253\"  \"141\"  \"3135\" \"4097\" \"1353\"\r\n\r\n\r\nSummarizing Attribute DATA\r\n\r\n\r\nsummary(E(network_igraph)$Distance)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n      0     223     496     639     903    6089 \r\n\r\nsummary(network_statnet %e% \"Distance\")\r\n\r\n\r\n   Length     Class      Mode \r\n     8228 character character \r\n\r\nThe way the summary function worked on the statnet set makes me think the statnet dataset is incorrectly set up at the moment.  #### Dyad Census\r\n\r\n\r\ndyad.census(network_igraph)\r\n\r\n\r\n$mut\r\n[1] 10449\r\n\r\n$asym\r\n[1] 2574\r\n\r\n$null\r\n[1] 271612\r\n\r\nsna::dyad.census(network_statnet)\r\n\r\n\r\n      Mut Asym   Null\r\n[1,] 3605 1018 280012\r\n\r\n\r\nTriad Census\r\n\r\n\r\ntriad.census(network_igraph)\r\n\r\n\r\n [1] 68169544   665870  2427052     1445     1289     2465    15322\r\n [8]    19171       91       39   114868      202      376      558\r\n[15]     6422    18671\r\n\r\nsna::triad.census(network_statnet)\r\n\r\n\r\n          003    012     102 021D 021U 021C  111D  111U 030T 030C\r\n[1,] 68169544 712579 2380343 1445 1289 2465 15322 19171   91   39\r\n        201 120D 120U 120C  210   300\r\n[1,] 114868  202  376  558 6422 18671\r\n\r\n\r\nTransivity\r\n\r\n\r\ntransitivity(network_igraph)\r\n\r\n\r\n[1] 0.3384609\r\n\r\ngtrans(network_statnet)\r\n\r\n\r\n[1] 0.3266617\r\n\r\nThe transitivity for igraph and statnet data sets were pretty close. \r\nLocal Transivity\r\n\r\n\r\nfirst_five_names <- V(network_igraph)$name %>% head(5)\r\nfirst_five_names\r\n\r\n\r\n[1] \"BGR\" \"BOS\" \"ANC\" \"JFK\" \"LAS\"\r\n\r\nfirst_five_transivity <- transitivity(network_igraph, type = \"local\", vids = V(network_igraph)[first_five_names])\r\n\r\ncbind(first_five_names,first_five_transivity)\r\n\r\n\r\n     first_five_names first_five_transivity\r\n[1,] \"BGR\"            \"0.581818181818182\"  \r\n[2,] \"BOS\"            \"0.35292389068469\"   \r\n[3,] \"ANC\"            \"0.0824960338445267\" \r\n[4,] \"JFK\"            \"0.385964912280702\"  \r\n[5,] \"LAS\"            \"0.223852116875373\"  \r\n\r\ntransitivity(network_igraph, type = \"global\")\r\n\r\n\r\n[1] 0.3384609\r\n\r\ntransitivity(network_igraph, type = \"average\")\r\n\r\n\r\n[1] 0.6452844\r\n\r\nLA seems to have low transivity while BGR has the highest at .58.\r\n\r\nDistances in the Network\r\n\r\n\r\ndistances(network_igraph, \"BGR\",\"BOS\")\r\n\r\n\r\n    BOS\r\nBGR   1\r\n\r\ndistances(network_igraph,\"BOS\", \"ANC\")\r\n\r\n\r\n    ANC\r\nBOS   2\r\n\r\naverage.path.length(network_igraph)\r\n\r\n\r\n[1] 3.52743\r\n\r\naverage.path.length(network_igraph, directed = F)\r\n\r\n\r\n[1] 3.447169\r\n\r\n\r\nIdentifying Isolates\r\n\r\n\r\nnames(igraph::components(network_igraph))\r\n\r\n\r\n[1] \"membership\" \"csize\"      \"no\"        \r\n\r\ncomponents(network_igraph)$no\r\n\r\n\r\n[1] 6\r\n\r\ncomponents(network_igraph)$csize\r\n\r\n\r\n[1] 745   2   2   3   2   1\r\n\r\ncomponents(network_igraph)$membership %>% head()\r\n\r\n\r\nBGR BOS ANC JFK LAS MIA \r\n  1   1   1   1   1   1 \r\n\r\n#Isolates\r\nisolates(network_statnet)\r\n\r\n\r\n[1] 166\r\n\r\nas.vector(network_statnet %v% \"vertex.names\")[c(isolates(network_statnet))]\r\n\r\n\r\n[1] \"DET\"\r\n\r\nDetroit seems to be the only isolate.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-04T10:11:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Intro Message",
    "description": "Welcome to my Blog.",
    "author": [
      {
        "name": "Peter",
        "url": {}
      }
    ],
    "date": "2022-02-01",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-03T15:08:59-05:00",
    "input_file": {}
  }
]
