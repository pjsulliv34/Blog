[
  {
    "path": "posts/machine-learning-hw-2/",
    "title": "Machine Learning Hw 2",
    "description": "Hw 2",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-03-13",
    "categories": [],
    "contents": "\r\r\nISLR Ch. 4, Exercise 16\r\r\nUsing the Boston data set, fit classification models in order to predict whether a given census tract has a crime rate above or below the median. Explore logistic regression, LDA, naive Bayes, and KNN models using various subsets of the predictors. Describe your findings. Hint: You will have to create the response variable yourself, using the variables that are contained in the Boston data set.\r\r\n\r\r\n\r\r\nsummary(Boston$crim)\r\r\n\r\r\n\r\r\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \r\r\n 0.00632  0.08204  0.25651  3.61352  3.67708 88.97620 \r\r\n\r\r\nBoston$response <- as.factor(ifelse(Boston$crim > median(Boston$crim),\"Above\",\"Below\"))\r\r\nBoston %>% filter(response==\"Above\") %>% head()\r\r\n\r\r\n\r\r\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat\r\r\n1 0.62976  0  8.14    0 0.538 5.949 61.8 4.7075   4 307      21  8.26\r\r\n2 0.63796  0  8.14    0 0.538 6.096 84.5 4.4619   4 307      21 10.26\r\r\n3 0.62739  0  8.14    0 0.538 5.834 56.5 4.4986   4 307      21  8.47\r\r\n4 1.05393  0  8.14    0 0.538 5.935 29.3 4.4986   4 307      21  6.58\r\r\n5 0.78420  0  8.14    0 0.538 5.990 81.7 4.2579   4 307      21 14.67\r\r\n6 0.80271  0  8.14    0 0.538 5.456 36.6 3.7965   4 307      21 11.69\r\r\n  medv response\r\r\n1 20.4    Above\r\r\n2 18.2    Above\r\r\n3 19.9    Above\r\r\n4 23.1    Above\r\r\n5 17.5    Above\r\r\n6 20.2    Above\r\r\n\r\r\nSet up Training and Test Sets\r\r\n\r\r\n\r\r\nset.seed(2)\r\r\n\r\r\ntrain <- sample(1:nrow(Boston), round((nrow(Boston)/4)*3,0))\r\r\n\r\r\nBoston.test <- Boston[-train,]\r\r\nboston.response <- Boston$response[-train]\r\r\nboston.train <- Boston[train,]\r\r\n\r\r\n\r\r\n\r\r\nLogistic Regression\r\r\n\r\r\n\r\r\nglm.fits <- glm(response ~ zn + indus+chas+nox+rm+age+dis+rad+tax, family = binomial, data = Boston, subset = train)\r\r\nsummary(glm.fits)\r\r\n\r\r\n\r\r\n\r\r\nCall:\r\r\nglm(formula = response ~ zn + indus + chas + nox + rm + age + \r\r\n    dis + rad + tax, family = binomial, data = Boston, subset = train)\r\r\n\r\r\nDeviance Residuals: \r\r\n     Min        1Q    Median        3Q       Max  \r\r\n-3.05155  -0.00772   0.00850   0.28098   1.80684  \r\r\n\r\r\nCoefficients:\r\r\n              Estimate Std. Error z value Pr(>|z|)    \r\r\n(Intercept)  24.861723   4.752476   5.231 1.68e-07 ***\r\r\nzn            0.061220   0.031442   1.947 0.051524 .  \r\r\nindus         0.008342   0.047150   0.177 0.859563    \r\r\nchas         -0.125309   0.847579  -0.148 0.882467    \r\r\nnox         -34.478533   6.935623  -4.971 6.65e-07 ***\r\r\nrm           -0.515107   0.353771  -1.456 0.145381    \r\r\nage          -0.011939   0.009914  -1.204 0.228468    \r\r\ndis          -0.338121   0.202467  -1.670 0.094918 .  \r\r\nrad          -0.594197   0.153996  -3.859 0.000114 ***\r\r\ntax           0.005827   0.002710   2.151 0.031508 *  \r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\n(Dispersion parameter for binomial family taken to be 1)\r\r\n\r\r\n    Null deviance: 526.53  on 379  degrees of freedom\r\r\nResidual deviance: 175.75  on 370  degrees of freedom\r\r\nAIC: 195.75\r\r\n\r\r\nNumber of Fisher Scoring iterations: 9\r\r\n\r\r\n#coef(glm.fits)\r\r\n#summary(glm.fits)$coef\r\r\n\r\r\n\r\r\nglm.probs <-predict(glm.fits, type = \"response\", Boston.test)\r\r\n#glm.probs %>% head()\r\r\n\r\r\ncontrasts(Boston$response)\r\r\n\r\r\n\r\r\n      Below\r\r\nAbove     0\r\r\nBelow     1\r\r\n\r\r\n#length(glm.probs)\r\r\n#dim(Boston)\r\r\nglm.pred <- rep(\"Above\",126)\r\r\nglm.pred[glm.probs >.5 ] = \"Below\"\r\r\n#Boston$response %>% length()\r\r\ntable(glm.pred,boston.response)\r\r\n\r\r\n\r\r\n        boston.response\r\r\nglm.pred Above Below\r\r\n   Above    55     3\r\r\n   Below    13    55\r\r\n\r\r\nmean(glm.pred==boston.response)\r\r\n\r\r\n\r\r\n[1] 0.8730159\r\r\n\r\r\nacc <- mean(glm.pred==boston.response)\r\r\n\r\r\nPrediction_Accuracy <- tibble(\"Model\" = \"Logistic Regression\", \"Accuracy\" = acc)\r\r\n\r\r\n\r\r\n\r\r\nThis means we were correct 87% of the time using the logistic analysis in predicting whether the crime level would be above the median using the variables chosen.\r\r\nLDA (Linear Discriminant Analysis)\r\r\n\r\r\n\r\r\nlibrary(MASS)\r\r\n\r\r\nlda.fit <- lda(response ~ zn + indus+chas+nox+rm+age+dis+rad+tax, data = Boston)\r\r\n#summary(lda.fit)\r\r\nlda.fit\r\r\n\r\r\n\r\r\nCall:\r\r\nlda(response ~ zn + indus + chas + nox + rm + age + dis + rad + \r\r\n    tax, data = Boston)\r\r\n\r\r\nPrior probabilities of groups:\r\r\nAbove Below \r\r\n  0.5   0.5 \r\r\n\r\r\nGroup means:\r\r\n             zn     indus       chas       nox       rm      age\r\r\nAbove  1.201581 15.271265 0.08695652 0.6384190 6.174874 85.83953\r\r\nBelow 21.525692  7.002292 0.05138340 0.4709711 6.394395 51.31028\r\r\n           dis       rad      tax\r\r\nAbove 2.498489 14.940711 510.7312\r\r\nBelow 5.091596  4.158103 305.7431\r\r\n\r\r\nCoefficients of linear discriminants:\r\r\n               LD1\r\r\nzn     0.004312445\r\r\nindus -0.014204281\r\r\nchas  -0.029319347\r\r\nnox   -7.380732435\r\r\nrm    -0.253144266\r\r\nage   -0.010855144\r\r\ndis    0.010054637\r\r\nrad   -0.084063067\r\r\ntax    0.001263545\r\r\n\r\r\nlda.pred <- predict(lda.fit,Boston)\r\r\nlda.class <- lda.pred$class\r\r\n\r\r\ntable(lda.class,Boston$response)\r\r\n\r\r\n\r\r\n         \r\r\nlda.class Above Below\r\r\n    Above   194    14\r\r\n    Below    59   239\r\r\n\r\r\nmean(lda.class==Boston$response)\r\r\n\r\r\n\r\r\n[1] 0.8557312\r\r\n\r\r\nacc <- mean(lda.class==Boston$response)\r\r\n\r\r\nPrediction_Accuracy <- rbind(Prediction_Accuracy,  c(\"LDA\", acc))\r\r\n\r\r\n\r\r\n\r\r\nFor LDA, 86% of predictions were correct. \r\r\nNaive Bayes\r\r\n\r\r\n\r\r\nlibrary(e1071)\r\r\n\r\r\nnb.fit <- naiveBayes(response ~ zn + indus+chas+nox+rm+age+dis+rad+tax, family = binomial, data = Boston, subset = train)\r\r\n#nb.fit\r\r\n\r\r\nnb.class <- predict(nb.fit, Boston.test)\r\r\ntable(nb.class, boston.response)\r\r\n\r\r\n\r\r\n        boston.response\r\r\nnb.class Above Below\r\r\n   Above    52    11\r\r\n   Below    16    47\r\r\n\r\r\nmean(nb.class == boston.response)\r\r\n\r\r\n\r\r\n[1] 0.7857143\r\r\n\r\r\nacc <- mean(lda.class==Boston$response)\r\r\n\r\r\nPrediction_Accuracy <- rbind(Prediction_Accuracy, c(\"Naive Bayes\", acc))\r\r\n\r\r\n\r\r\n\r\r\nUsing Naive Bayes, we were able to get a prediction accuracy of 78%. \r\r\nK nearest Neighbors\r\r\n\r\r\n\r\r\nlibrary(class)\r\r\nattach(Boston)\r\r\nset.seed(2)\r\r\n#Boston %>% dim()\r\r\ntrain.x <- Boston[train,colnames(Boston) %in% c(\"zn\",\"indus\",\"chas\",\"nox\",\"rm\",\"age\",\"dis\",\"rad\",\"tax\")]\r\r\n\r\r\ntest.x <- Boston[-train,colnames(Boston) %in% c(\"zn\",\"indus\",\"chas\",\"nox\",\"rm\",\"age\",\"dis\",\"rad\",\"tax\")]\r\r\ntrain.response <- Boston$response[train]\r\r\ntest.response <- Boston$response[-train]\r\r\n#train.x %>% dim()\r\r\n#test.x %>% dim()\r\r\n#train.response %>% length()\r\r\n\r\r\nknn.pred <- knn(train.x,test.x, train.response, k =1)\r\r\n#knn.pred %>% length()\r\r\n#test.response %>% length()\r\r\n#knn.pred %>% length()\r\r\n#test.response %>% length()\r\r\n\r\r\ntable(knn.pred,test.response)\r\r\n\r\r\n\r\r\n        test.response\r\r\nknn.pred Above Below\r\r\n   Above    66     7\r\r\n   Below     2    51\r\r\n\r\r\npaste(\"Percent Accuracy with K = 1\",mean(knn.pred == test.response))\r\r\n\r\r\n\r\r\n[1] \"Percent Accuracy with K = 1 0.928571428571429\"\r\r\n\r\r\nknn.pred <- knn(train.x,test.x, train.response, k =3)\r\r\npaste(\"Percent Accuracy with K =3\",mean(knn.pred == test.response))\r\r\n\r\r\n\r\r\n[1] \"Percent Accuracy with K =3 0.968253968253968\"\r\r\n\r\r\nknn.pred <- knn(train.x,test.x, train.response, k =5)\r\r\npaste(\"Percent Accuracy with K = 5\",mean(knn.pred == test.response))\r\r\n\r\r\n\r\r\n[1] \"Percent Accuracy with K = 5 0.952380952380952\"\r\r\n\r\r\nx<- NULL\r\r\ny <- NULL\r\r\nfor(i in 1:100){\r\r\n  knn.pred <- knn(train.x,test.x, train.response, k =i)\r\r\n  x<- rbind(x,mean(knn.pred == test.response))\r\r\n  y <- rbind(y,paste(i))\r\r\n}\r\r\ndata.frame(\"Kvalue\" = y, \"Accuracy\" = x) %>% arrange(desc(x)) %>% head() %>% kable()\r\r\n\r\r\n\r\r\nKvalue\r\r\nAccuracy\r\r\n3\r\r\n0.9682540\r\r\n4\r\r\n0.9682540\r\r\n6\r\r\n0.9603175\r\r\n5\r\r\n0.9523810\r\r\n1\r\r\n0.9285714\r\r\n7\r\r\n0.9285714\r\r\n\r\r\nknn.pred <- knn(train.x,test.x, train.response, k =3)\r\r\nacc <- mean(knn.pred == test.response)\r\r\n\r\r\nPrediction_Accuracy <- rbind(Prediction_Accuracy, c(\"KNN\", acc))\r\r\n\r\r\n\r\r\n\r\r\nIn order to set up the KNN model, I needed to create 4 data sets. A training and Testing data set, and a test response, and training response. I then decided to run the model and use various values of K. The first bit of code is a bit inefficient in deciding the best value of K. This can be seen in the first 3 predictions for K = 1, 3, and 5.\r\r\nIn order to find the best value of K, I decided to create a loop that would try every K value from 1 to 100 and store the models % accuracy in a vector named x. I then created a dataframe to visualize the x values and the corresponding K values used.\r\r\nUsing KNN, I created 100 models with a k value form 1 to 100. The top 6 are listed above. The K value of 3 was the best value for K.\r\r\n\r\r\n\r\r\nPrediction_Accuracy %>% arrange(desc(Accuracy)) %>% kable()\r\r\n\r\r\n\r\r\nModel\r\r\nAccuracy\r\r\nKNN\r\r\n0.968253968253968\r\r\nLogistic Regression\r\r\n0.873015873015873\r\r\nLDA\r\r\n0.855731225296443\r\r\nNaive Bayes\r\r\n0.855731225296443\r\r\n\r\r\nI would recommend using the KNN to predict whether a crime level will be higher than the median for the Boston data set. It has the highest accuracy at 97%.\r\r\n\r\r\nISLR Ch. 5, Exercise 3 (or substitute advanced: Exercise 2)\r\r\n\r\r\nA.\r\r\nK fold is implemented by dividing the set of cases into a number of groups which is dictated by K. Each group will have an equal length.\r\r\nThe first group is the validation set. The model is trained on the other groups. So if we had a value of 10 for K. Group 1 would be the validation set and the 9 other groups would be the training groups. A Mean squared error is calculated for the group. This process will be carried out for each group. At the end we will have a MSE for each group. Finally, we take the average of all the MSE’s and that gives us the k fold CV estimate.\r\r\nB.\r\r\nThe Validation Set Approach:\r\r\nIf K is set to low, then it approaches the same as using the validation set approach. Test Error rate will most likely be higher in the validation approach when compared to kfold validation.\r\r\nValidation set approach will most likely have a high bias and cause over fitting when compared to the k fold approach.\r\r\nLOOCV:\r\r\nComputation advantage when using kfold versus LOOCV. K fold will require less computation/resources/time, then compared to the LOOCV.\r\r\nLOOCV has less bias, which means less overfitting when applied to other datasets, but will have more variance when compared to K fold and when compared to the validation set approach.\r\r\n\r\r\nISLR Ch. 5, Exercise 5\r\r\n\r\r\n\r\r\nset.seed(5)\r\r\n\r\r\nData <- Default\r\r\n#Data %>% head()\r\r\n#Data %>% names()\r\r\n\r\r\ntrain <-  sample(1:nrow(Data), round((nrow(Data)/4)*3,0))\r\r\n#train %>% length()\r\r\n#Data %>% dim()\r\r\n\r\r\n\r\r\n### Create training Sets\r\r\nData.test <- Data[-train,]\r\r\nData.response <- Data$default[-train]\r\r\n\r\r\n\r\r\n# Fit model\r\r\nglm.fits <- glm(default ~ income+balance, family = binomial, data = Data, subset = train)\r\r\n#summary(glm.fits)\r\r\n#coef(glm.fits)\r\r\n#summary(glm.fits)$coef\r\r\nglm.probs <-predict(glm.fits, type = \"response\", Data.test)\r\r\n\r\r\n\r\r\ncontrasts(Data$default)\r\r\n\r\r\n\r\r\n    Yes\r\r\nNo    0\r\r\nYes   1\r\r\n\r\r\n#length(glm.probs)\r\r\n#dim(Data.test)\r\r\nglm.pred <- rep(\"No\",2500)\r\r\nglm.pred[glm.probs >.5 ] = \"Yes\"\r\r\n\r\r\ntable(glm.pred,Data.response)\r\r\n\r\r\n\r\r\n        Data.response\r\r\nglm.pred   No  Yes\r\r\n     No  2417   48\r\r\n     Yes    7   28\r\r\n\r\r\nmean(glm.pred==Data.response)\r\r\n\r\r\n\r\r\n[1] 0.978\r\r\n\r\r\npaste0(\"The test Error Rate is : \",(1-(mean(glm.pred==Data.response)))*100,\"%\" )\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate is : 2.2%\"\r\r\n\r\r\n\r\r\nFor this problem, we will run the sample function 3 times to get a different training set. I will fit the model, and then I will calculate the test error rate again. For ease, I will create a loop. I create a new seed each time I run the loop.\r\r\n\r\r\n\r\r\nfor(i in 1:3){\r\r\nset.seed(i)\r\r\ntrain <-  sample(1:nrow(Data), round((nrow(Data)/4)*3,0))\r\r\n# Create training Sets\r\r\nData.test <- Data[-train,]\r\r\nData.response <- Data$default[-train]\r\r\n# Fit model\r\r\nglm.fits <- glm(default ~ income+balance, family = binomial, data = Data, subset = train)\r\r\nglm.probs <-predict(glm.fits, type = \"response\", Data.test)\r\r\nglm.pred <- rep(\"No\",2500)\r\r\nglm.pred[glm.probs >.5 ] = \"Yes\"\r\r\nmean(glm.pred==Data.response)\r\r\nprint(paste0(\"The test Error Rate is : \",(1-(mean(glm.pred==Data.response)))*100,\"%\" ))\r\r\n}\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate is : 2.6%\"\r\r\n[1] \"The test Error Rate is : 2.12%\"\r\r\n[1] \"The test Error Rate is : 2.6%\"\r\r\n\r\r\nWhen Running the model with 3 different seeds. I saw that the error rate fluctuated from 2.12 % up to 2.6%.\r\r\n\r\r\n\r\r\n\r\r\nglm.fits <- glm(default ~ ., family = binomial, data = Data, subset = train)\r\r\nglm.probs <-predict(glm.fits, type = \"response\", Data.test)\r\r\nglm.pred <- rep(\"No\",2500)\r\r\nglm.pred[glm.probs >.5 ] = \"Yes\"\r\r\n\r\r\ntable(glm.pred,Data.response)\r\r\n\r\r\n\r\r\n        Data.response\r\r\nglm.pred   No  Yes\r\r\n     No  2416   54\r\r\n     Yes   13   17\r\r\n\r\r\nmean(glm.pred==Data.response)\r\r\n\r\r\n\r\r\n[1] 0.9732\r\r\n\r\r\npaste0(\"The test Error Rate is : \",(1-(mean(glm.pred==Data.response)))*100,\"%\" )\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate is : 2.68%\"\r\r\n\r\r\nUsing the dummy variable is about the same as the last error rate calculated in part c (2.68). I’m comparing it to that result, since the current sample seed is the one used in that loop.\r\r\n\r\r\nISLR Ch. 5, Exercise 9\r\r\n\r\r\n\r\r\n\r\r\nu_ <- Boston$medv %>% mean()\r\r\npaste(\"The population mean of the medv:\",u_)\r\r\n\r\r\n\r\r\n[1] \"The population mean of the medv: 22.5328063241107\"\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nsd_error <- Boston$medv%>%sd()/(length(Boston)^(1/2))\r\r\n\r\r\npaste(\"The Standard error of the sample mean is:\", sd_error)\r\r\n\r\r\n\r\r\n[1] \"The Standard error of the sample mean is: 2.45802946039096\"\r\r\n\r\r\n\r\r\n\r\r\nlibrary(boot)\r\r\n\r\r\nalpha.fn <- function(data, index){\r\r\n  medv <-data$medv[index]\r\r\n  mean_1 <- mean(medv)\r\r\n \r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n#Boston$medv %>% length()\r\r\n\r\r\n\r\r\nboot(Boston, alpha.fn,10000)\r\r\n\r\r\n\r\r\n\r\r\nORDINARY NONPARAMETRIC BOOTSTRAP\r\r\n\r\r\n\r\r\nCall:\r\r\nboot(data = Boston, statistic = alpha.fn, R = 10000)\r\r\n\r\r\n\r\r\nBootstrap Statistics :\r\r\n    original       bias    std. error\r\r\nt1* 22.53281 0.0006395257    0.407591\r\r\n\r\r\nThe standard error is lower using the Bootstrap when compared to the answer from part c.\r\r\n\r\r\n\r\r\n\r\r\nt.test(Boston$medv)\r\r\n\r\r\n\r\r\n\r\r\n    One Sample t-test\r\r\n\r\r\ndata:  Boston$medv\r\r\nt = 55.111, df = 505, p-value < 2.2e-16\r\r\nalternative hypothesis: true mean is not equal to 0\r\r\n95 percent confidence interval:\r\r\n 21.72953 23.33608\r\r\nsample estimates:\r\r\nmean of x \r\r\n 22.53281 \r\r\n\r\r\nmean_boot <- 22.53281\r\r\nstd.boot <- .4137494\r\r\n\r\r\nupperbound <- mean_boot + 2*std.boot\r\r\nlowerbound <- mean_boot -2*std.boot\r\r\n\r\r\npaste0(\"The confidence interval using the boot: [\",lowerbound,\" , \",upperbound,\"]\")\r\r\n\r\r\n\r\r\n[1] \"The confidence interval using the boot: [21.7053112 , 23.3603088]\"\r\r\n\r\r\n\r\r\nThe boot and the t test confidence intervals are very close, almost identical.\r\r\n\r\r\n\r\r\n\r\r\npaste(\"The median value of medv in the population\", Boston$medv %>% median())\r\r\n\r\r\n\r\r\n[1] \"The median value of medv in the population 21.2\"\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nalpha.median <- function(data, index){\r\r\n  medv <-data$medv[index]\r\r\n  median_1 <- median(medv)\r\r\n\r\r\n}\r\r\n\r\r\nboot(Boston, alpha.median, 10000)\r\r\n\r\r\n\r\r\n\r\r\nORDINARY NONPARAMETRIC BOOTSTRAP\r\r\n\r\r\n\r\r\nCall:\r\r\nboot(data = Boston, statistic = alpha.median, R = 10000)\r\r\n\r\r\n\r\r\nBootstrap Statistics :\r\r\n    original    bias    std. error\r\r\nt1*     21.2 -0.013115   0.3808402\r\r\n\r\r\nThe standard error of the median is .3768. This standard error is very small for the original median of 21.2.\r\r\n\r\r\n\r\r\n\r\r\nquantile(Boston$medv, probs = c(.1,.2,.5,.75,.9))\r\r\n\r\r\n\r\r\n  10%   20%   50%   75%   90% \r\r\n12.75 15.30 21.20 25.00 34.80 \r\r\n\r\r\npaste(\"The 10th percentile is :\", quantile(Boston$medv, probs = .1),\"%\")\r\r\n\r\r\n\r\r\n[1] \"The 10th percentile is : 12.75 %\"\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nalpha.quantile <- function(data, index){\r\r\n  medv <-data$medv[index]\r\r\n  perc_10 <- quantile(medv, probs = .1)\r\r\n\r\r\n}\r\r\n\r\r\nboot(Boston, alpha.quantile, 10000)\r\r\n\r\r\n\r\r\n\r\r\nORDINARY NONPARAMETRIC BOOTSTRAP\r\r\n\r\r\n\r\r\nCall:\r\r\nboot(data = Boston, statistic = alpha.quantile, R = 10000)\r\r\n\r\r\n\r\r\nBootstrap Statistics :\r\r\n    original   bias    std. error\r\r\nt1*    12.75 0.007025   0.4981081\r\r\n\r\r\ndetach(Boston)\r\r\n\r\r\n\r\r\n\r\r\nThe standard error for the 10% quantile is .5 for a original value of 12.75.\r\r\n\r\r\nISLR Ch. 6, Exercise 2\r\r\nReferenced Article: https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\r\r\nI found this article helpful in understanding accuracy with reference to bias/variance tradeoff.\r\r\nFigure 2.7 in Textbook also helped with flexibility of different models.\r\r\n\r\r\nA.\r\r\nI\r\r\nMore flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\r\r\nIncorrect. Lasso is less flexible than Least squares.\r\r\nII\r\r\nMore flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\r\r\nIncorrect - less flexible than OLS\r\r\nIII\r\r\nLess flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\r\r\nCorrect - Low bias and high variance will cause overfitting, but will have a higher prediction accuracy than underfitting the data. since the variance is not increasing, we will have more coefficents that explain the response being measuared, and lamda will not go to zero for those predictors.\r\r\nIV\r\r\nLess flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\r\r\nIncorrect - Increase in variance < then decrease in bias would give a situation of high variance and high bias.\r\r\nAlso this is not how the lasso works, lambda will get rid of the variables that have high variance, by setting their cofficents to\r\r\nB\r\r\nRepeat (a) for ridge regression relative to least squares.\r\r\ni\r\r\nIncorrect - Less Flexible\r\r\nII\r\r\nIncorrect - Less Flexible\r\r\nIII\r\r\nCorrect - As lamda increases bias, flexibility decreases, which means we get a decreased variance and increased bias.\r\r\nSlight increase in bias with less variance increases prediction accuracy.\r\r\nIV\r\r\nIncorrect. Larger increase in varriance associated with smaller decrease in bias would not improve prediction accuracy.\r\r\nC\r\r\nRepeat (a) for non-linear methods relative to least squares.\r\r\ni\r\r\nCorrect - More flexible model, Slight increase in bias with less variance increases prediction accuracy.\r\r\nii-iV\r\r\nIncorrect.\r\r\n\r\r\nISLR Ch. 6, Exercise 9\r\r\n(omit e & f) (requires time and effort; please collaborate & use Piazza)\r\r\nIn this exercise, we will predict the number of applications received using the other variables in the College data set.\r\r\nA\r\r\nSplit the data set into a training set and a test set.\r\r\n\r\r\n\r\r\nlibrary(ISLR2)\r\r\n\r\r\nset.seed(1)\r\r\n\r\r\n#College %>% dim()\r\r\n#College %>% names()\r\r\n#College %>% head()\r\r\n#?College\r\r\n\r\r\ntrain <-  sample(1:nrow(College), round((nrow(College)/4)*3,0))\r\r\n#train %>% length()\r\r\n#train %>% head()\r\r\n\r\r\n\r\r\n\r\r\n# Create Test actuals and training Sets\r\r\n\r\r\nCollege.test <- College[-train,]\r\r\nCollege.response <- College$Apps[-train]\r\r\ncollege.train <- College[train,]\r\r\n\r\r\n\r\r\n\r\r\nAfter looking into College data set, I identified that our response variable will be the column Apps. The training set will be the college dataframe filtered on the train index.\r\r\nB\r\r\nFit a linear model using least squares on the training set, and report the test error obtained.\r\r\n\r\r\n\r\r\nattach(College)\r\r\nlm.fit <- lm(Apps ~., data = College, subset = train)\r\r\nlm.pred <- predict(lm.fit, College.test, type = \"response\")\r\r\nlm_mse <- mean((College.response -lm.pred )^2)\r\r\nlm_mse\r\r\n\r\r\n\r\r\n[1] 1389123\r\r\n\r\r\nMSE_dataframe <- data.frame(\"model\" = \"Least Square\", \"MSE\" = lm_mse)\r\r\n\r\r\n\r\r\n\r\r\nC\r\r\nFit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.\r\r\n\r\r\n\r\r\nlibrary(glmnet)\r\r\n\r\r\n\r\r\nx <- model.matrix(Apps~.,College)[,-1]\r\r\ny <- College$Apps\r\r\ngrid <- 10^seq(10,-2, length = 100)\r\r\n\r\r\nridge.mod <- glmnet(x[train,], y[train], alpha =0, lambda = grid )\r\r\ncv.out <- cv.glmnet(x[train,], y[train],alpha = 0)\r\r\nplot(cv.out)\r\r\n\r\r\n\r\r\n\r\r\nlamda_best <- cv.out$lambda.min\r\r\nlamda_best\r\r\n\r\r\n\r\r\n[1] 364.3384\r\r\n\r\r\nridge.pred <- predict(ridge.mod, s = lamda_best, newx = x[-train,])\r\r\nridge_mse <- mean((ridge.pred - y[-train])^2)\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"ridge\",ridge_mse))\r\r\n\r\r\npaste(\"The test Error Rate is : \", mean((ridge.pred - y[-train])^2))\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate is :  1204431.39092091\"\r\r\n\r\r\nD\r\r\nFit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\r\r\n\r\r\n\r\r\nlasso.mod <- glmnet(x[train,],y[train],alpha = 1, lambda = grid)\r\r\ncv.out <- cv.glmnet(x[train,], y[train], alpha = 1)\r\r\nplot(cv.out)\r\r\n\r\r\n\r\r\n\r\r\nlamda_best <- cv.out$lambda.min\r\r\nlamda_best\r\r\n\r\r\n\r\r\n[1] 2.133937\r\r\n\r\r\nlasso.pred <- predict(lasso.mod, s= lamda_best, newx = x[-train,])\r\r\npaste(\"The test error rate is \",mean((lasso.pred - y[-train])^2))\r\r\n\r\r\n\r\r\n[1] \"The test error rate is  1374692.34765744\"\r\r\n\r\r\nlasso_mse <- mean((lasso.pred - y[-train])^2)\r\r\n\r\r\nout <- glmnet(x,y, alpha = 1 ,lambda = grid)\r\r\nlamda.coef <- predict(out, type = \"coefficients\", s= lamda_best)[1:18,]\r\r\npaste(\"The number of non-zero coeficients:\",lamda.coef[lamda.coef !=0] %>% length())\r\r\n\r\r\n\r\r\n[1] \"The number of non-zero coeficients: 18\"\r\r\n\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"Lasso\",lasso_mse))\r\r\n\r\r\n\r\r\n\r\r\nNone of the coefficients are zero.\r\r\nE\r\r\nFit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.\r\r\n\r\r\n\r\r\nlibrary(pls)\r\r\nset.seed(10)\r\r\npcr.fit <- pcr(Apps ~ . , data = College, subset = train, scale = TRUE, validation = \"CV\")\r\r\nvalidationplot(pcr.fit, val.type = \"MSEP\")\r\r\n\r\r\n\r\r\n\r\r\npcr.pred <- predict(pcr.fit,College.test, ncomp = 17)\r\r\npcr_mse<- mean((pcr.pred - College.response)^2)\r\r\npaste(\"The MSE error is \", pcr_mse)\r\r\n\r\r\n\r\r\n[1] \"The MSE error is  1389123.27022729\"\r\r\n\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"PCR\",pcr_mse))\r\r\n\r\r\n\r\r\n\r\r\nFrom the validation plot, the lowest MSEP for the number of components is around 16,17, and 18.I decided to use 17 as my value of M.\r\r\nF\r\r\nFit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.\r\r\n\r\r\n\r\r\npls.fit <- plsr(Apps~., data = College, subset = train, scale = TRUE, validation = \"CV\")\r\r\n\r\r\nvalidationplot(pls.fit, val.type = \"MSEP\")\r\r\n\r\r\n\r\r\n\r\r\npls.pred_7<- predict(pls.fit, College.test, ncomp = 7)\r\r\npls.pred_8 <- predict(pls.fit, College.test, ncomp = 8)\r\r\n\r\r\npaste(\"The MSE error using M as 7: \", mean((pls.pred_7- College.response)^2))\r\r\n\r\r\n\r\r\n[1] \"The MSE error using M as 7:  1336641.93400001\"\r\r\n\r\r\npaste(\"The MSE error using M as 8: \", mean((pls.pred_8- College.response)^2))\r\r\n\r\r\n\r\r\n[1] \"The MSE error using M as 8:  1362535.73332994\"\r\r\n\r\r\npls_mse <- mean((pls.pred_8- College.response)^2)\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"PLS\",pls_mse))\r\r\n\r\r\n\r\r\n\r\r\nThe number of components ranges from 7 to 8. I will predict using both values of M.\r\r\nG\r\r\nComment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?\r\r\n\r\r\n\r\r\nMSE_dataframe %>% arrange(MSE) %>% kable()\r\r\n\r\r\n\r\r\nmodel\r\r\nMSE\r\r\nridge\r\r\n1204431.39092091\r\r\nPLS\r\r\n1362535.73332994\r\r\nLasso\r\r\n1374692.34765744\r\r\nLeast Square\r\r\n1389123.27022729\r\r\nPCR\r\r\n1389123.27022729\r\r\n\r\r\nMSE_dataframe$MSE <- as.numeric(MSE_dataframe$MSE)\r\r\n\r\r\nMSE_dataframe %>% ggplot2::ggplot(aes(x = model, y = MSE)) + geom_col(fill = \"deepskyblue3\")+ theme(axis.text.y = element_blank())+labs(title = \"MSE per Model\")+geom_text(aes(label = round(as.numeric(MSE),0)), vjust = 2)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nThe Ridge regression model had the lowest MSE, with PLS coming up the second lowest MSE, following Lasso and the PCR and Least square in last. There is not too much of a difference between the MSE’s, it looks larger due to the scale on the graph.\r\r\n\r\r\nISLR Ch. 8, Exercise 4\r\r\n\r\r\nA.\r\r\n\r\r\nB\r\r\n\r\r\n\r\r\nISLR Ch. 8, Exercise 7\r\r\n\r\r\n\r\r\n\r\r\nlibrary(randomForest); library(ggplot2)\r\r\nset.seed(1)\r\r\ntrain <- sample (1: nrow (Boston), (nrow (Boston) / 4)*3)\r\r\nboston.response_test <- Boston$medv[-train]\r\r\ndf <- tibble(\"predictors\" = c(),\"ntree\" = double(), \"MSE\" = double())\r\r\n\r\r\n\r\r\nfor(i in seq(1, 13,2)){\r\r\n  for(x in seq(1, 600,25)){\r\r\n  bag.boston <- randomForest(medv ~., data = Boston, subset = train, mtry = i,ntree = x)\r\r\nyhat.bag <- predict(bag.boston, newdata = Boston[-train,])\r\r\nmse_1 <- (mean((yhat.bag- boston.response_test)^2))\r\r\ndf <-df %>% add_row(predictors = as.character(i),ntree = x, MSE = mse_1)\r\r\n  }\r\r\n}\r\r\n\r\r\nplot<- df %>% ggplot()+geom_line(aes(x = ntree, y = MSE, group = predictors,color = predictors, linetype = predictors))\r\r\n\r\r\nplotly::ggplotly(plot)\r\r\n\r\r\n\r\r\n\r\r\n{\"x\":{\"data\":[{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[36.4030132360704,20.1672870413967,18.7087560009328,19.1354783627998,18.496417690562,19.1775878361241,19.2898228736319,18.4637276207247,18.2145217932484,19.3112908727648,18.2604710744005,18.8832646753751,18.3325016116797,18.5013258158174,18.5583502300008,18.0282923967408,18.8787594530618,18.7177851193332,19.1305364079644,18.3444224181792,18.4552723600292,18.0868986971722,18.4964681541742,17.8802581871862],\"text\":[\"ntree:   1<br />MSE: 36.40301<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree:  26<br />MSE: 20.16729<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree:  51<br />MSE: 18.70876<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree:  76<br />MSE: 19.13548<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 101<br />MSE: 18.49642<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 126<br />MSE: 19.17759<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 151<br />MSE: 19.28982<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 176<br />MSE: 18.46373<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 201<br />MSE: 18.21452<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 226<br />MSE: 19.31129<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 251<br />MSE: 18.26047<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 276<br />MSE: 18.88326<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 301<br />MSE: 18.33250<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 326<br />MSE: 18.50133<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 351<br />MSE: 18.55835<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 376<br />MSE: 18.02829<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 401<br />MSE: 18.87876<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 426<br />MSE: 18.71779<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 451<br />MSE: 19.13054<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 476<br />MSE: 18.34442<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 501<br />MSE: 18.45527<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 526<br />MSE: 18.08690<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 551<br />MSE: 18.49647<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 576<br />MSE: 17.88026<br />predictors: 1<br />predictors: 1<br />predictors: 1\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"1\",\"legendgroup\":\"1\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[27.8376998031496,16.381568998522,16.151450555939,15.1073078911924,15.6034328258243,15.5810081577421,15.3438551453429,14.683301877896,15.2702784168452,15.3317851316579,15.0863588551852,15.5714223302754,14.9814345841696,15.3187311977802,15.526801532571,15.007413021855,15.3519339589946,15.4099309083397,15.3871500250433,15.4012488824985,15.0536589769078,15.584226152949,15.0936646237656,15.1259037440271],\"text\":[\"ntree:   1<br />MSE: 27.83770<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree:  26<br />MSE: 16.38157<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree:  51<br />MSE: 16.15145<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree:  76<br />MSE: 15.10731<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 101<br />MSE: 15.60343<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 126<br />MSE: 15.58101<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 151<br />MSE: 15.34386<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 176<br />MSE: 14.68330<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 201<br />MSE: 15.27028<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 226<br />MSE: 15.33179<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 251<br />MSE: 15.08636<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 276<br />MSE: 15.57142<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 301<br />MSE: 14.98143<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 326<br />MSE: 15.31873<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 351<br />MSE: 15.52680<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 376<br />MSE: 15.00741<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 401<br />MSE: 15.35193<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 426<br />MSE: 15.40993<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 451<br />MSE: 15.38715<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 476<br />MSE: 15.40125<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 501<br />MSE: 15.05366<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 526<br />MSE: 15.58423<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 551<br />MSE: 15.09366<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 576<br />MSE: 15.12590<br />predictors: 11<br />predictors: 11<br />predictors: 11\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(196,154,0,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"name\":\"11\",\"legendgroup\":\"11\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[23.1885851487314,16.3271876644626,16.1047661877473,16.3791585132342,16.6291119647191,16.4217080410046,16.6730086205132,16.0405735001123,16.0220958864348,16.0493263788259,16.0666392771661,15.5211045158706,15.6571753152715,15.6984812880089,15.6301215460534,15.6347657250359,15.9207285686653,15.8682693914913,15.9122302907001,16.160446247243,15.7876944857743,16.1627848489863,15.9502883447923,16.185244746838],\"text\":[\"ntree:   1<br />MSE: 23.18859<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree:  26<br />MSE: 16.32719<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree:  51<br />MSE: 16.10477<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree:  76<br />MSE: 16.37916<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 101<br />MSE: 16.62911<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 126<br />MSE: 16.42171<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 151<br />MSE: 16.67301<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 176<br />MSE: 16.04057<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 201<br />MSE: 16.02210<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 226<br />MSE: 16.04933<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 251<br />MSE: 16.06664<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 276<br />MSE: 15.52110<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 301<br />MSE: 15.65718<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 326<br />MSE: 15.69848<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 351<br />MSE: 15.63012<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 376<br />MSE: 15.63477<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 401<br />MSE: 15.92073<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 426<br />MSE: 15.86827<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 451<br />MSE: 15.91223<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 476<br />MSE: 16.16045<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 501<br />MSE: 15.78769<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 526<br />MSE: 16.16278<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 551<br />MSE: 15.95029<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 576<br />MSE: 16.18524<br />predictors: 13<br />predictors: 13<br />predictors: 13\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(83,180,0,1)\",\"dash\":\"dot\"},\"hoveron\":\"points\",\"name\":\"13\",\"legendgroup\":\"13\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[24.2867815122725,12.3097981555951,13.2899636404797,12.5922919059138,11.901794350884,11.8469873720801,11.4910846625509,11.8795585953659,11.4774907434225,12.1788916340457,11.5700539828036,11.5894458676662,11.2340814879293,11.6815794427441,11.7074766939527,11.2447940285054,11.2516583730643,11.6852042911835,11.5307976987236,11.5382852856446,11.5206350639931,11.5700853879932,11.3575994037437,11.3967207815116],\"text\":[\"ntree:   1<br />MSE: 24.28678<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree:  26<br />MSE: 12.30980<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree:  51<br />MSE: 13.28996<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree:  76<br />MSE: 12.59229<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 101<br />MSE: 11.90179<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 126<br />MSE: 11.84699<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 151<br />MSE: 11.49108<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 176<br />MSE: 11.87956<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 201<br />MSE: 11.47749<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 226<br />MSE: 12.17889<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 251<br />MSE: 11.57005<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 276<br />MSE: 11.58945<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 301<br />MSE: 11.23408<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 326<br />MSE: 11.68158<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 351<br />MSE: 11.70748<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 376<br />MSE: 11.24479<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 401<br />MSE: 11.25166<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 426<br />MSE: 11.68520<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 451<br />MSE: 11.53080<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 476<br />MSE: 11.53829<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 501<br />MSE: 11.52064<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 526<br />MSE: 11.57009<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 551<br />MSE: 11.35760<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 576<br />MSE: 11.39672<br />predictors: 3<br />predictors: 3<br />predictors: 3\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,192,148,1)\",\"dash\":\"dashdot\"},\"hoveron\":\"points\",\"name\":\"3\",\"legendgroup\":\"3\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[31.3454156338068,14.2111065693747,12.2659125711638,11.6653790175755,11.9028340711658,11.9682225145091,11.8588202019782,11.7650163287732,11.5963491624945,11.7296883364314,12.3432801031288,11.7395155861203,11.9830501030999,11.9127830303761,12.1803461640039,12.6475849384718,12.0449384438374,12.4057677552547,12.2842212042761,12.3943910987525,12.0961226067077,11.8386920877585,11.8533340005994,12.2655681649624],\"text\":[\"ntree:   1<br />MSE: 31.34542<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree:  26<br />MSE: 14.21111<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree:  51<br />MSE: 12.26591<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree:  76<br />MSE: 11.66538<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 101<br />MSE: 11.90283<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 126<br />MSE: 11.96822<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 151<br />MSE: 11.85882<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 176<br />MSE: 11.76502<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 201<br />MSE: 11.59635<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 226<br />MSE: 11.72969<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 251<br />MSE: 12.34328<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 276<br />MSE: 11.73952<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 301<br />MSE: 11.98305<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 326<br />MSE: 11.91278<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 351<br />MSE: 12.18035<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 376<br />MSE: 12.64758<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 401<br />MSE: 12.04494<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 426<br />MSE: 12.40577<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 451<br />MSE: 12.28422<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 476<br />MSE: 12.39439<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 501<br />MSE: 12.09612<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 526<br />MSE: 11.83869<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 551<br />MSE: 11.85333<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 576<br />MSE: 12.26557<br />predictors: 5<br />predictors: 5<br />predictors: 5\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,182,235,1)\",\"dash\":\"longdash\"},\"hoveron\":\"points\",\"name\":\"5\",\"legendgroup\":\"5\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[23.7007902012248,13.1657455337221,14.0937209565992,12.6350737975176,14.1611095307225,13.4832536439717,12.7699077265545,13.2423457680251,12.9255826430514,13.0021587712479,12.6451985361649,13.5994099184761,12.7978397312921,12.9726545106415,12.7602862562019,13.0227192839547,12.900307636034,13.5060756503014,12.6563400858669,13.3638675355722,12.9117074954309,12.93785098855,13.0279022924108,12.8812600065364],\"text\":[\"ntree:   1<br />MSE: 23.70079<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree:  26<br />MSE: 13.16575<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree:  51<br />MSE: 14.09372<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree:  76<br />MSE: 12.63507<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 101<br />MSE: 14.16111<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 126<br />MSE: 13.48325<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 151<br />MSE: 12.76991<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 176<br />MSE: 13.24235<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 201<br />MSE: 12.92558<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 226<br />MSE: 13.00216<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 251<br />MSE: 12.64520<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 276<br />MSE: 13.59941<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 301<br />MSE: 12.79784<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 326<br />MSE: 12.97265<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 351<br />MSE: 12.76029<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 376<br />MSE: 13.02272<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 401<br />MSE: 12.90031<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 426<br />MSE: 13.50608<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 451<br />MSE: 12.65634<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 476<br />MSE: 13.36387<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 501<br />MSE: 12.91171<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 526<br />MSE: 12.93785<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 551<br />MSE: 13.02790<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 576<br />MSE: 12.88126<br />predictors: 7<br />predictors: 7<br />predictors: 7\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(165,138,255,1)\",\"dash\":\"longdashdot\"},\"hoveron\":\"points\",\"name\":\"7\",\"legendgroup\":\"7\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[18.7287790026247,13.8109805731246,13.8730105994801,14.9131231682735,14.7543041460434,14.1643088771201,13.8650545046038,14.0235497026901,13.5187483572158,14.5877501213954,14.4669967470558,14.0755375096726,14.1877815448516,14.220098355586,14.4249112372478,14.1968511526347,14.0710383780131,14.1670186876829,13.957019898319,13.9256491173259,14.7120238348951,14.3069928578327,13.7578946432719,14.2319280108093],\"text\":[\"ntree:   1<br />MSE: 18.72878<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree:  26<br />MSE: 13.81098<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree:  51<br />MSE: 13.87301<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree:  76<br />MSE: 14.91312<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 101<br />MSE: 14.75430<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 126<br />MSE: 14.16431<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 151<br />MSE: 13.86505<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 176<br />MSE: 14.02355<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 201<br />MSE: 13.51875<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 226<br />MSE: 14.58775<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 251<br />MSE: 14.46700<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 276<br />MSE: 14.07554<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 301<br />MSE: 14.18778<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 326<br />MSE: 14.22010<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 351<br />MSE: 14.42491<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 376<br />MSE: 14.19685<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 401<br />MSE: 14.07104<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 426<br />MSE: 14.16702<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 451<br />MSE: 13.95702<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 476<br />MSE: 13.92565<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 501<br />MSE: 14.71202<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 526<br />MSE: 14.30699<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 551<br />MSE: 13.75789<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 576<br />MSE: 14.23193<br />predictors: 9<br />predictors: 9<br />predictors: 9\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(251,97,215,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"name\":\"9\",\"legendgroup\":\"9\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":27.8721461187215,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":37.2602739726027},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-27.75,604.75],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"200\",\"400\",\"600\"],\"tickvals\":[3.5527136788005e-15,200,400,600],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"200\",\"400\",\"600\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"ntree\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[9.97563490052224,37.6614598234775],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\"],\"tickvals\":[10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"MSE\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":0.959399606299213},\"annotations\":[{\"text\":\"predictors\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"6678612a186\":{\"x\":{},\"y\":{},\"colour\":{},\"linetype\":{},\"type\":\"scatter\"}},\"cur_data\":\"6678612a186\",\"visdat\":{\"6678612a186\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\r\nWhen starting off with less trees, we see that the MSE’s for model is higher. As we increase trees, the MSE drops significantly for all Models. The model that only uses one predictor seems to have the highest MSE ranging from 30 - 25. The models with 3, 5 and 7 seem to have the lowest amount of MSE. Theres no need to have more than 100 Ntrees to get the lowest MSE.\r\r\n\r\r\nISLR Ch. 8, Exercise 9\r\r\nA\r\r\n\r\r\nCreate a training set containing a random sample of 800 observations, and a test set containing the remaining observations.\r\r\n\r\r\n\r\r\n## OJ dataset\r\r\nlibrary(ISLR2)\r\r\nset.seed(1)\r\r\n\r\r\ntrain <- sample(1:nrow(OJ), 800)\r\r\n#train %>% length()\r\r\nOJ.test <- OJ[-train,]\r\r\nOJ.response <- OJ$Purchase[-train]\r\r\nOJ.train <- OJ[train,]\r\r\nOJ.train.response <- OJ$Purchase[train]\r\r\n\r\r\n\r\r\n#OJ.test %>% dim()\r\r\n#OJ.train %>% dim()\r\r\n#OJ %>% dim()\r\r\n#OJ.response %>% length()\r\r\n#OJ.train.response %>% length()\r\r\n\r\r\n\r\r\n\r\r\nB\r\r\nFit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?\r\r\n\r\r\n\r\r\nlibrary(tree)\r\r\nattach(OJ)\r\r\ntree.oj <- tree(Purchase ~.,data = OJ, subset = train)\r\r\nsummary(tree.oj)\r\r\n\r\r\n\r\r\n\r\r\nClassification tree:\r\r\ntree(formula = Purchase ~ ., data = OJ, subset = train)\r\r\nVariables actually used in tree construction:\r\r\n[1] \"LoyalCH\"       \"PriceDiff\"     \"SpecialCH\"     \"ListPriceDiff\"\r\r\n[5] \"PctDiscMM\"    \r\r\nNumber of terminal nodes:  9 \r\r\nResidual mean deviance:  0.7432 = 587.8 / 791 \r\r\nMisclassification error rate: 0.1588 = 127 / 800 \r\r\n\r\r\nThe training error rate is 16% There are 8 terminal nodes for this tree. The residual mean deviance is 75%.\r\r\nC\r\r\nType in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.\r\r\n\r\r\n\r\r\ntree.oj\r\r\n\r\r\n\r\r\nnode), split, n, deviance, yval, (yprob)\r\r\n      * denotes terminal node\r\r\n\r\r\n 1) root 800 1073.00 CH ( 0.60625 0.39375 )  \r\r\n   2) LoyalCH < 0.5036 365  441.60 MM ( 0.29315 0.70685 )  \r\r\n     4) LoyalCH < 0.280875 177  140.50 MM ( 0.13559 0.86441 )  \r\r\n       8) LoyalCH < 0.0356415 59   10.14 MM ( 0.01695 0.98305 ) *\r\r\n       9) LoyalCH > 0.0356415 118  116.40 MM ( 0.19492 0.80508 ) *\r\r\n     5) LoyalCH > 0.280875 188  258.00 MM ( 0.44149 0.55851 )  \r\r\n      10) PriceDiff < 0.05 79   84.79 MM ( 0.22785 0.77215 )  \r\r\n        20) SpecialCH < 0.5 64   51.98 MM ( 0.14062 0.85938 ) *\r\r\n        21) SpecialCH > 0.5 15   20.19 CH ( 0.60000 0.40000 ) *\r\r\n      11) PriceDiff > 0.05 109  147.00 CH ( 0.59633 0.40367 ) *\r\r\n   3) LoyalCH > 0.5036 435  337.90 CH ( 0.86897 0.13103 )  \r\r\n     6) LoyalCH < 0.764572 174  201.00 CH ( 0.73563 0.26437 )  \r\r\n      12) ListPriceDiff < 0.235 72   99.81 MM ( 0.50000 0.50000 )  \r\r\n        24) PctDiscMM < 0.196197 55   73.14 CH ( 0.61818 0.38182 ) *\r\r\n        25) PctDiscMM > 0.196197 17   12.32 MM ( 0.11765 0.88235 ) *\r\r\n      13) ListPriceDiff > 0.235 102   65.43 CH ( 0.90196 0.09804 ) *\r\r\n     7) LoyalCH > 0.764572 261   91.20 CH ( 0.95785 0.04215 ) *\r\r\n\r\r\nThe terminal nodes are shown by the asterix at the end. Lets look at branch 7. This ends in a terminal node. The split is LoyaCH > .764572. There are 123.80 observations in this branch. The prediction for this branch is CH.\r\r\nD\r\r\nCreate a plot of the tree, and interpret the results.\r\r\n\r\r\n\r\r\nplot(tree.oj)\r\r\ntext(tree.oj, pretty = 0)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nThe criteria most used are loyal CH and price diff.\r\r\ne\r\r\nPredict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?\r\r\n\r\r\n\r\r\ntree.pred <- predict(tree.oj, OJ.test, type = \"class\")\r\r\nx <- table(tree.pred, OJ.response)\r\r\n\r\r\n\r\r\ntable(tree.pred,OJ.response) %>% kable()\r\r\n\r\r\n\r\r\n\r\r\nCH\r\r\nMM\r\r\nCH\r\r\n160\r\r\n38\r\r\nMM\r\r\n8\r\r\n64\r\r\n\r\r\npaste(\"The Test error rate is :\", round(1-(x[1,1]+x[2,2])/sum(x),4),\"%\")\r\r\n\r\r\n\r\r\n[1] \"The Test error rate is : 0.1704 %\"\r\r\n\r\r\nF, G, H\r\r\nApply the cv.tree() function to the training set in order to determine the optimal tree size.\r\r\nProduce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.\r\r\nWhich tree size corresponds to the lowest cross-validated classification error rate?\r\r\n\r\r\n\r\r\ncv.oj <- cv.tree(tree.oj, FUN = prune.misclass )\r\r\n\r\r\n\r\r\npar(mfrow = c(1,2))\r\r\nplot(cv.oj$size, cv.oj$dev, type = \"b\", xlab = \"Number of Trees\", ylab = \"Crossvalidation Error\")\r\r\nplot(cv.oj$k, cv.oj$dev, type = \"b\", xlab = \"Cross complexity Parameter\", ylab = \"Crossvalidation Error\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nAccording to the figure above, the Crossvalidation error was the lowest with the number of trees at 8.\r\r\nI\r\r\nProduce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.\r\r\n\r\r\n\r\r\nprune.oj <- prune.misclass(tree.oj, best = 8)\r\r\nplot(prune.oj)\r\r\ntext(prune.oj, pretty = 0)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nJ\r\r\nCompare the training error rates between the pruned and unpruned trees. Which is higher?\r\r\n\r\r\n\r\r\nprune.oj.pred <- predict(prune.oj, OJ.train, type = \"class\")\r\r\nprune.oj.pred %>% length()\r\r\n\r\r\n\r\r\n[1] 800\r\r\n\r\r\nOJ.train.response %>% length()\r\r\n\r\r\n\r\r\n[1] 800\r\r\n\r\r\nx<- table(prune.oj.pred,OJ.train.response)\r\r\n\r\r\n\r\r\n\r\r\n table(prune.oj.pred,OJ.train.response)%>% kable()\r\r\n\r\r\n\r\r\n\r\r\nCH\r\r\nMM\r\r\nCH\r\r\n450\r\r\n92\r\r\nMM\r\r\n35\r\r\n223\r\r\n\r\r\npaste(\"The Test error rate is :\", round(1-(x[1,1]+x[2,2])/sum(x),3),\"%\")\r\r\n\r\r\n\r\r\n[1] \"The Test error rate is : 0.159 %\"\r\r\n\r\r\nThe training error rate is slightly less at 15% compared to the 16 % shown in the original training set.\r\r\nK\r\r\nCompare the test error rates between the pruned and unpruned trees. Which is higher?\r\r\n\r\r\n\r\r\nprune.test.pred <- predict(prune.oj, OJ.test, type = \"class\")\r\r\n\r\r\nx<- table(prune.test.pred,OJ.response)\r\r\n\r\r\n\r\r\n\r\r\n table(prune.test.pred,OJ.response)%>% kable()\r\r\n\r\r\n\r\r\n\r\r\nCH\r\r\nMM\r\r\nCH\r\r\n160\r\r\n38\r\r\nMM\r\r\n8\r\r\n64\r\r\n\r\r\npaste(\"The Test error rate is :\", round(1-(x[1,1]+x[2,2])/sum(x),4),\"%\")\r\r\n\r\r\n\r\r\n[1] \"The Test error rate is : 0.1704 %\"\r\r\n\r\r\nThe test error from the pruned tree is the same as the unpruned tree. We only removed one branch from the original 9 and the CV error rates looked pretty similar for both.\r\r\n\r\r\nISLR Ch. 8, Exercise 10\r\r\nWe now use boosting to predict Salary in the Hitters data set.\r\r\nA\r\r\nRemove the observations for whom the salary information is unknown, and then log-transform the salaries.\r\r\n\r\r\n\r\r\nHitters %>% filter(is.na(Hitters$Salary)) %>% count()\r\r\n\r\r\n\r\r\n   n\r\r\n1 59\r\r\n\r\r\nHitters %>% dim()\r\r\n\r\r\n\r\r\n[1] 322  20\r\r\n\r\r\nHitters_new <- Hitters %>% filter(!is.na(Hitters$Salary))\r\r\nHitters_new %>% dim()\r\r\n\r\r\n\r\r\n[1] 263  20\r\r\n\r\r\nUsing !is.na(), I filtered the dataset to filter out all rows with an NA salary.\r\r\nNext we will log transform the Salaries\r\r\n\r\r\n\r\r\nHitters_new$Salary <- log(Hitters_new$Salary)\r\r\n\r\r\n\r\r\n\r\r\nB\r\r\nCreate a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.\r\r\n\r\r\n\r\r\nset.seed(10)\r\r\n\r\r\ntrain <- sample(1:nrow(Hitters_new), 200)\r\r\n#train %>% length()\r\r\n\r\r\nHitters.test <- Hitters_new[-train,]\r\r\nHitters.response <- Hitters_new$Salary[-train]\r\r\nHitters.train <- Hitters_new[train,]\r\r\nHitters.train.response <- Hitters_new$Salary[train]\r\r\n\r\r\n\r\r\n#Hitters.test %>% dim()\r\r\n#Hitters_new %>% dim()\r\r\n#Hitters.train.response%>% length()\r\r\n#Hitters.response %>% length()\r\r\n\r\r\n\r\r\n\r\r\nC\r\r\nPerform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter λ. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.\r\r\n\r\r\n\r\r\nlibrary(gbm)\r\r\n\r\r\n## Example Boost function, summary, and way to calculate MSE\r\r\nboost.Hitters <- gbm(Salary~.,data = Hitters_new[train,],\r\r\n                     distribution = \"gaussian\", n.trees = 1000,\r\r\n                     interaction.depth = 4, shrinkage = .001)\r\r\n\r\r\nsummary(boost.Hitters)\r\r\n\r\r\n\r\r\n\r\r\n                var     rel.inf\r\r\nCAtBat       CAtBat 28.48705371\r\r\nCRuns         CRuns 26.44960088\r\r\nCHits         CHits 11.53406819\r\r\nCRBI           CRBI  9.37181852\r\r\nCWalks       CWalks  6.10917041\r\r\nHits           Hits  3.05905465\r\r\nCHmRun       CHmRun  2.19862661\r\r\nWalks         Walks  2.17115968\r\r\nHmRun         HmRun  2.01444003\r\r\nAtBat         AtBat  1.94060681\r\r\nYears         Years  1.86440763\r\r\nRBI             RBI  1.73382350\r\r\nPutOuts     PutOuts  1.62680896\r\r\nRuns           Runs  0.89988120\r\r\nErrors       Errors  0.16759670\r\r\nAssists     Assists  0.12090202\r\r\nDivision   Division  0.11322110\r\r\nNewLeague NewLeague  0.08105077\r\r\nLeague       League  0.05670863\r\r\n\r\r\nyhat.boost <- predict(boost.Hitters, newdata = Hitters_new[-train,], n.trees = 1000)\r\r\nyhat.boost %>% length()\r\r\n\r\r\n\r\r\n[1] 63\r\r\n\r\r\npaste(\"The MSE test error rate is\",mean((yhat.boost- Hitters.response)^2))\r\r\n\r\r\n\r\r\n[1] \"The MSE test error rate is 0.420831510594534\"\r\r\n\r\r\nUsing the method above, I can now calculate the MSE using the boost model, yhat predictions and the actual results on the training set. Next I will create a loop that will run through multiple values of shrinage and add the calculated MSE’s to a dataframe. I will then plot those values.\r\r\n\r\r\n\r\r\nboost_df <- tibble(\"Shrinkage\" = double(), \"MSE\" = double())\r\r\n\r\r\n\r\r\nfor(i in seq(.0001, .25,.01)){\r\r\n  \r\r\n  boost.Hitters <- gbm(Salary~.,data = Hitters_new[train,],\r\r\n                     distribution = \"gaussian\", n.trees = 1000,\r\r\n                     interaction.depth = 4, shrinkage = i)\r\r\n  yhat.boost <- predict(boost.Hitters, newdata = Hitters_new[train,], n.trees = 1000)\r\r\nmse_1 <- mean((yhat.boost- Hitters.train.response)^2)\r\r\n\r\r\n\r\r\nboost_df <- boost_df %>% add_row(Shrinkage = i, MSE = mse_1)\r\r\n  \r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nplot<- boost_df %>% ggplot()+geom_line(aes(x = Shrinkage, y = MSE))+xlab(\"Shrinkage (lambda)\")\r\r\n\r\r\nplotly::ggplotly(plot)\r\r\n\r\r\n\r\r\n\r\r\n{\"x\":{\"data\":[{\"x\":[0.0001,0.0101,0.0201,0.0301,0.0401,0.0501,0.0601,0.0701,0.0801,0.0901,0.1001,0.1101,0.1201,0.1301,0.1401,0.1501,0.1601,0.1701,0.1801,0.1901,0.2001,0.2101,0.2201,0.2301,0.2401],\"y\":[0.665861327231243,0.0379234798261602,0.0144307205324894,0.0072596447854902,0.00344975971057255,0.00168453714329211,0.000835282591719583,0.000417665506786859,0.000285515859136994,0.000131718353511751,9.27714113673313e-05,4.16776791624866e-05,2.44783876823639e-05,1.1576696795739e-05,8.58353457180432e-06,3.28766728356092e-06,2.91449594811509e-06,1.37631946441906e-06,8.32728768275394e-07,5.78267535350603e-07,3.53992245551131e-07,1.19388320961587e-07,8.84088622687979e-08,7.50714592385369e-08,1.44906879843596e-08],\"text\":[\"Shrinkage: 0.0001<br />MSE: 6.658613e-01\",\"Shrinkage: 0.0101<br />MSE: 3.792348e-02\",\"Shrinkage: 0.0201<br />MSE: 1.443072e-02\",\"Shrinkage: 0.0301<br />MSE: 7.259645e-03\",\"Shrinkage: 0.0401<br />MSE: 3.449760e-03\",\"Shrinkage: 0.0501<br />MSE: 1.684537e-03\",\"Shrinkage: 0.0601<br />MSE: 8.352826e-04\",\"Shrinkage: 0.0701<br />MSE: 4.176655e-04\",\"Shrinkage: 0.0801<br />MSE: 2.855159e-04\",\"Shrinkage: 0.0901<br />MSE: 1.317184e-04\",\"Shrinkage: 0.1001<br />MSE: 9.277141e-05\",\"Shrinkage: 0.1101<br />MSE: 4.167768e-05\",\"Shrinkage: 0.1201<br />MSE: 2.447839e-05\",\"Shrinkage: 0.1301<br />MSE: 1.157670e-05\",\"Shrinkage: 0.1401<br />MSE: 8.583535e-06\",\"Shrinkage: 0.1501<br />MSE: 3.287667e-06\",\"Shrinkage: 0.1601<br />MSE: 2.914496e-06\",\"Shrinkage: 0.1701<br />MSE: 1.376319e-06\",\"Shrinkage: 0.1801<br />MSE: 8.327288e-07\",\"Shrinkage: 0.1901<br />MSE: 5.782675e-07\",\"Shrinkage: 0.2001<br />MSE: 3.539922e-07\",\"Shrinkage: 0.2101<br />MSE: 1.193883e-07\",\"Shrinkage: 0.2201<br />MSE: 8.840886e-08\",\"Shrinkage: 0.2301<br />MSE: 7.507146e-08\",\"Shrinkage: 0.2401<br />MSE: 1.449069e-08\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":27.8721461187215,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.0119,0.2521],\"tickmode\":\"array\",\"ticktext\":[\"0.00\",\"0.05\",\"0.10\",\"0.15\",\"0.20\",\"0.25\"],\"tickvals\":[0,0.05,0.1,0.15,0.2,0.25],\"categoryorder\":\"array\",\"categoryarray\":[\"0.00\",\"0.05\",\"0.10\",\"0.15\",\"0.20\",\"0.25\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Shrinkage (lambda)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.0332930511463398,0.699154392868271],\"tickmode\":\"array\",\"ticktext\":[\"0.0\",\"0.2\",\"0.4\",\"0.6\"],\"tickvals\":[0,0.2,0.4,0.6],\"categoryorder\":\"array\",\"categoryarray\":[\"0.0\",\"0.2\",\"0.4\",\"0.6\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"MSE\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"6678351a7809\":{\"x\":{},\"y\":{},\"type\":\"scatter\"}},\"cur_data\":\"6678351a7809\",\"visdat\":{\"6678351a7809\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\r\nD\r\r\nProduce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.\r\r\nFor this, I will do the exact steps as done in part C, but I will use the test set instead of the training set when making predicitons.\r\r\n\r\r\n\r\r\nboost_df_test <- tibble(\"Shrinkage\" = double(), \"MSE\" = double())\r\r\n\r\r\n\r\r\nfor(i in seq(.0001, .25,.01)){\r\r\n  \r\r\n  boost.Hitters <- gbm(Salary~.,data = Hitters_new[train,],\r\r\n                     distribution = \"gaussian\", n.trees = 1000,\r\r\n                     interaction.depth = 4, shrinkage = i)\r\r\n  yhat.boost <- predict(boost.Hitters, newdata = Hitters_new[-train,], n.trees = 1000)\r\r\nmse_1 <- mean((yhat.boost- Hitters.test$Salary)^2)\r\r\n\r\r\nboost_df_test <- boost_df_test %>% add_row(Shrinkage = i, MSE = mse_1)\r\r\n  \r\r\n}\r\r\n\r\r\n\r\r\nplot<- boost_df_test %>% ggplot()+geom_line(aes(x = Shrinkage, y = MSE))+xlab(\"Shrinkage (lambda)\")\r\r\n\r\r\nplotly::ggplotly(plot)\r\r\n\r\r\n\r\r\n\r\r\n{\"x\":{\"data\":[{\"x\":[0.0001,0.0101,0.0201,0.0301,0.0401,0.0501,0.0601,0.0701,0.0801,0.0901,0.1001,0.1101,0.1201,0.1301,0.1401,0.1501,0.1601,0.1701,0.1801,0.1901,0.2001,0.2101,0.2201,0.2301,0.2401],\"y\":[0.776404949431054,0.377468213068746,0.389109787753448,0.397285920230495,0.398521711253916,0.392998980345278,0.411981165955552,0.411056091167511,0.40978317154764,0.424797677427592,0.424781638545013,0.403721699223818,0.442243230922348,0.411557203950517,0.434448046097555,0.436631619422199,0.415804628242507,0.442882398988851,0.442741487743061,0.441862464416142,0.398775869459989,0.447842056881034,0.424421198723408,0.409074194786413,0.448232315646583],\"text\":[\"Shrinkage: 0.0001<br />MSE: 0.7764049\",\"Shrinkage: 0.0101<br />MSE: 0.3774682\",\"Shrinkage: 0.0201<br />MSE: 0.3891098\",\"Shrinkage: 0.0301<br />MSE: 0.3972859\",\"Shrinkage: 0.0401<br />MSE: 0.3985217\",\"Shrinkage: 0.0501<br />MSE: 0.3929990\",\"Shrinkage: 0.0601<br />MSE: 0.4119812\",\"Shrinkage: 0.0701<br />MSE: 0.4110561\",\"Shrinkage: 0.0801<br />MSE: 0.4097832\",\"Shrinkage: 0.0901<br />MSE: 0.4247977\",\"Shrinkage: 0.1001<br />MSE: 0.4247816\",\"Shrinkage: 0.1101<br />MSE: 0.4037217\",\"Shrinkage: 0.1201<br />MSE: 0.4422432\",\"Shrinkage: 0.1301<br />MSE: 0.4115572\",\"Shrinkage: 0.1401<br />MSE: 0.4344480\",\"Shrinkage: 0.1501<br />MSE: 0.4366316\",\"Shrinkage: 0.1601<br />MSE: 0.4158046\",\"Shrinkage: 0.1701<br />MSE: 0.4428824\",\"Shrinkage: 0.1801<br />MSE: 0.4427415\",\"Shrinkage: 0.1901<br />MSE: 0.4418625\",\"Shrinkage: 0.2001<br />MSE: 0.3987759\",\"Shrinkage: 0.2101<br />MSE: 0.4478421\",\"Shrinkage: 0.2201<br />MSE: 0.4244212\",\"Shrinkage: 0.2301<br />MSE: 0.4090742\",\"Shrinkage: 0.2401<br />MSE: 0.4482323\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":27.8721461187215,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.0119,0.2521],\"tickmode\":\"array\",\"ticktext\":[\"0.00\",\"0.05\",\"0.10\",\"0.15\",\"0.20\",\"0.25\"],\"tickvals\":[0,0.05,0.1,0.15,0.2,0.25],\"categoryorder\":\"array\",\"categoryarray\":[\"0.00\",\"0.05\",\"0.10\",\"0.15\",\"0.20\",\"0.25\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Shrinkage (lambda)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.357521376250631,0.796351786249169],\"tickmode\":\"array\",\"ticktext\":[\"0.4\",\"0.5\",\"0.6\",\"0.7\"],\"tickvals\":[0.4,0.5,0.6,0.7],\"categoryorder\":\"array\",\"categoryarray\":[\"0.4\",\"0.5\",\"0.6\",\"0.7\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"MSE\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"667862e3182\":{\"x\":{},\"y\":{},\"type\":\"scatter\"}},\"cur_data\":\"667862e3182\",\"visdat\":{\"667862e3182\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\r\nboost_df_test %>% summarise(min(MSE))\r\r\n\r\r\n\r\r\n# A tibble: 1 x 1\r\r\n  `min(MSE)`\r\r\n       <dbl>\r\r\n1      0.377\r\r\n\r\r\nwhich.min(boost_df_test$MSE)\r\r\n\r\r\n\r\r\n[1] 2\r\r\n\r\r\nboost_df_test[which.min(boost_df_test$MSE),]\r\r\n\r\r\n\r\r\n# A tibble: 1 x 2\r\r\n  Shrinkage   MSE\r\r\n      <dbl> <dbl>\r\r\n1    0.0101 0.377\r\r\n\r\r\nMin MSE can be found at .0101 Lamda.\r\r\nE\r\r\nCompare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.\r\r\nFor this I decided to use multiple models from ch. 3 and ch. 6. See below:\r\r\n\r\r\n\r\r\nlibrary(ggplot2)\r\r\n\r\r\n## Linear Model\r\r\n\r\r\nlm.fit <- lm(Salary ~., data = Hitters_new, subset = train)\r\r\nlm.pred <- predict(lm.fit, Hitters_new[-train,], type = \"response\")\r\r\nlm_mse <- mean((lm.pred - Hitters.response)^2)\r\r\npaste(\"The Linear Model MSE is: \",lm_mse)\r\r\n\r\r\n\r\r\n[1] \"The Linear Model MSE is:  0.576163818320103\"\r\r\n\r\r\nMSE_dataframe <- data.frame(\"model\" = \"Least Square\", \"MSE\" = lm_mse)\r\r\n\r\r\n\r\r\n\r\r\n## Boost\r\r\n boost.Hitters <- gbm(Salary~.,data = Hitters_new[train,],\r\r\n                     distribution = \"gaussian\", n.trees = 1000,\r\r\n                     interaction.depth = 4, shrinkage = .0101)\r\r\n  yhat.boost <- predict(boost.Hitters, newdata = Hitters_new[-train,], n.trees = 1000)\r\r\nmse_1 <- mean((yhat.boost- Hitters.response)^2)\r\r\n\r\r\n\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"Boosting\",mse_1))\r\r\n\r\r\npaste(\"The test Error Rate for Boosting is : \", mean((yhat.boost - Hitters.response)^2))\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate for Boosting is :  0.382735329466136\"\r\r\n\r\r\n## Ridge\r\r\nset.seed(10)\r\r\nx <- model.matrix(Salary~.,Hitters_new)[,-1]\r\r\ny <- Hitters_new$Salary\r\r\ngrid <- 10^seq(10,-2, length = 100)\r\r\n\r\r\nridge.mod <- glmnet(x[train,], y[train], alpha =0, lambda = grid )\r\r\ncv.out <- cv.glmnet(x[train,], y[train],alpha = 0)\r\r\nlamda_best <- cv.out$lambda.min\r\r\nridge.pred <- predict(ridge.mod, s = lamda_best, newx = x[-train,])\r\r\nridge_mse <- mean((ridge.pred - y[-train])^2)\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"ridge\",ridge_mse))\r\r\n\r\r\npaste(\"The test Error Rate for Ridge is : \", mean((ridge.pred - y[-train])^2))\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate for Ridge is :  0.581396338527786\"\r\r\n\r\r\n## Lasso\r\r\n\r\r\nlasso.mod <- glmnet(x[train,],y[train],alpha = 1, lambda = grid)\r\r\ncv.out <- cv.glmnet(x[train,], y[train], alpha = 1)\r\r\nlamda_best <- cv.out$lambda.min\r\r\nlasso.pred <- predict(lasso.mod, s= lamda_best, newx = x[-train,])\r\r\npaste(\"The test error rate for lasso \",mean((lasso.pred - y[-train])^2))\r\r\n\r\r\n\r\r\n[1] \"The test error rate for lasso  0.575921346257875\"\r\r\n\r\r\nlasso_mse <- mean((lasso.pred - y[-train])^2)\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"Lasso\",lasso_mse))\r\r\n\r\r\n\r\r\n\r\r\n## PCR\r\r\n\r\r\npcr.fit <- pcr(Salary ~ . , data = Hitters_new, subset = train, scale = TRUE, validation = \"CV\")\r\r\nvalidationplot(pcr.fit,val.type = \"MSEP\")\r\r\n\r\r\n\r\r\n\r\r\npcr.pred <- predict(pcr.fit,Hitters.test, ncomp = 10)\r\r\npcr_mse<- mean((pcr.pred - Hitters.response)^2)\r\r\n# lowest MSEP at 10 componenets.\r\r\npaste(\"The MSE error for PCR is \", pcr_mse)\r\r\n\r\r\n\r\r\n[1] \"The MSE error for PCR is  0.619877318874639\"\r\r\n\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"PCR\",pcr_mse))\r\r\n\r\r\n\r\r\n\r\r\n## PLS\r\r\n\r\r\npls.fit <- plsr(Salary~., data = Hitters_new, subset = train, scale = TRUE, validation = \"CV\")\r\r\nvalidationplot(pls.fit, val.type = \"MSEP\")\r\r\n\r\r\n\r\r\n\r\r\n# Lowest  MSEP at 5 components\r\r\npls.pred_7<- predict(pls.fit, Hitters.test, ncomp = 5)\r\r\npaste(\"The MSE error using M as 5: \", mean((pls.pred_7- Hitters.response)^2))\r\r\n\r\r\n\r\r\n[1] \"The MSE error using M as 5:  0.617966162736438\"\r\r\n\r\r\npls_mse <- mean((pls.pred_7- Hitters.response)^2)\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"PLS\",pls_mse))\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n#MSE_dataframe %>% arrange(MSE)\r\r\nMSE_dataframe$MSE <- as.numeric(MSE_dataframe$MSE)\r\r\n\r\r\nMSE_dataframe %>% ggplot2::ggplot(aes(x = model, y = MSE)) + geom_col(fill = \"deepskyblue3\")+theme(axis.text.y = element_blank())+labs(title = \"MSE per Model\")+geom_text(aes(label = round(as.numeric(MSE),3)), vjust = 2)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nBoosting the model will give us the lowest MSE, with PCR in second place. Boosting is the model of choice by far compared to the other models.\r\r\nF\r\r\nWhich variables appear to be the most important predictors in the boosted model?\r\r\n\r\r\n\r\r\nsummary(boost.Hitters)\r\r\n\r\r\n\r\r\n\r\r\n                var    rel.inf\r\r\nCAtBat       CAtBat 21.3776399\r\r\nCRuns         CRuns 18.4554725\r\r\nCHits         CHits  9.4230585\r\r\nCRBI           CRBI  9.1617592\r\r\nCWalks       CWalks  7.1643822\r\r\nYears         Years  5.2568153\r\r\nCHmRun       CHmRun  4.2210637\r\r\nWalks         Walks  3.0531623\r\r\nPutOuts     PutOuts  3.0017692\r\r\nAtBat         AtBat  2.9358935\r\r\nHits           Hits  2.8943762\r\r\nErrors       Errors  2.7801941\r\r\nRBI             RBI  2.7298938\r\r\nHmRun         HmRun  2.5411422\r\r\nRuns           Runs  2.1241977\r\r\nAssists     Assists  1.5876161\r\r\nDivision   Division  0.4762580\r\r\nNewLeague NewLeague  0.4306447\r\r\nLeague       League  0.3846610\r\r\n\r\r\nAt bat and runs have the most influence.\r\r\nG\r\r\nNow apply bagging to the training set. What is the test set MSE for this approach?\r\r\n\r\r\n\r\r\nset.seed(1)\r\r\n\r\r\n\r\r\nbag_data <- tibble(\"predictors\" = c(),\"ntree\" = double(), \"MSE\" = double())\r\r\n\r\r\n\r\r\n\r\r\nfor(i in seq(1, 19,2)){\r\r\n  for(x in seq(1, 600,100)){\r\r\n  bag.hitters <- randomForest(Salary ~., data = Hitters.train, mtry = i,ntree = x)\r\r\nyhat.bag <- predict(bag.hitters, newdata = Hitters.test)\r\r\nmse_1 <- (mean((yhat.bag- Hitters.response)^2))\r\r\nbag_data <-bag_data %>% add_row(predictors = as.character(i),ntree = x, MSE = mse_1)\r\r\n  }\r\r\n}\r\r\n\r\r\nplot<- bag_data %>% ggplot()+geom_line(aes(x = ntree, y = MSE, group = predictors,color = predictors, linetype = predictors))\r\r\n\r\r\nplotly::ggplotly(plot)\r\r\n\r\r\n\r\r\n\r\r\n{\"x\":{\"data\":[{\"x\":[1,101,201,301,401,501],\"y\":[0.43171395670545,0.406888206779161,0.38835026613887,0.394957294443029,0.391269651648264,0.39293152140034],\"text\":[\"ntree:   1<br />MSE: 0.4317140<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 101<br />MSE: 0.4068882<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 201<br />MSE: 0.3883503<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 301<br />MSE: 0.3949573<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 401<br />MSE: 0.3912697<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 501<br />MSE: 0.3929315<br />predictors: 1<br />predictors: 1<br />predictors: 1\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"1\",\"legendgroup\":\"1\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.661173726164936,0.397066154375408,0.362048665909562,0.358341413030472,0.367815443344352,0.363345651573596],\"text\":[\"ntree:   1<br />MSE: 0.6611737<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 101<br />MSE: 0.3970662<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 201<br />MSE: 0.3620487<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 301<br />MSE: 0.3583414<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 401<br />MSE: 0.3678154<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 501<br />MSE: 0.3633457<br />predictors: 11<br />predictors: 11<br />predictors: 11\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(216,144,0,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"name\":\"11\",\"legendgroup\":\"11\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.658288916933541,0.347060609906192,0.364754790637257,0.359946094273967,0.361640653629599,0.364537790832377],\"text\":[\"ntree:   1<br />MSE: 0.6582889<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 101<br />MSE: 0.3470606<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 201<br />MSE: 0.3647548<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 301<br />MSE: 0.3599461<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 401<br />MSE: 0.3616407<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 501<br />MSE: 0.3645378<br />predictors: 13<br />predictors: 13<br />predictors: 13\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(163,165,0,1)\",\"dash\":\"dot\"},\"hoveron\":\"points\",\"name\":\"13\",\"legendgroup\":\"13\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.49514177324075,0.370200746381667,0.363540704420413,0.361785301393901,0.360282087622746,0.368491273097209],\"text\":[\"ntree:   1<br />MSE: 0.4951418<br />predictors: 15<br />predictors: 15<br />predictors: 15\",\"ntree: 101<br />MSE: 0.3702007<br />predictors: 15<br />predictors: 15<br />predictors: 15\",\"ntree: 201<br />MSE: 0.3635407<br />predictors: 15<br />predictors: 15<br />predictors: 15\",\"ntree: 301<br />MSE: 0.3617853<br />predictors: 15<br />predictors: 15<br />predictors: 15\",\"ntree: 401<br />MSE: 0.3602821<br />predictors: 15<br />predictors: 15<br />predictors: 15\",\"ntree: 501<br />MSE: 0.3684913<br />predictors: 15<br />predictors: 15<br />predictors: 15\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(57,182,0,1)\",\"dash\":\"dashdot\"},\"hoveron\":\"points\",\"name\":\"15\",\"legendgroup\":\"15\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.526273744475996,0.35950692757155,0.359311412713107,0.369429456885847,0.365785126558743,0.360859328139336],\"text\":[\"ntree:   1<br />MSE: 0.5262737<br />predictors: 17<br />predictors: 17<br />predictors: 17\",\"ntree: 101<br />MSE: 0.3595069<br />predictors: 17<br />predictors: 17<br />predictors: 17\",\"ntree: 201<br />MSE: 0.3593114<br />predictors: 17<br />predictors: 17<br />predictors: 17\",\"ntree: 301<br />MSE: 0.3694295<br />predictors: 17<br />predictors: 17<br />predictors: 17\",\"ntree: 401<br />MSE: 0.3657851<br />predictors: 17<br />predictors: 17<br />predictors: 17\",\"ntree: 501<br />MSE: 0.3608593<br />predictors: 17<br />predictors: 17<br />predictors: 17\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,125,1)\",\"dash\":\"longdash\"},\"hoveron\":\"points\",\"name\":\"17\",\"legendgroup\":\"17\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.481039970852711,0.379086606019541,0.368019988076372,0.359608427537105,0.372666813244621,0.370620367912294],\"text\":[\"ntree:   1<br />MSE: 0.4810400<br />predictors: 19<br />predictors: 19<br />predictors: 19\",\"ntree: 101<br />MSE: 0.3790866<br />predictors: 19<br />predictors: 19<br />predictors: 19\",\"ntree: 201<br />MSE: 0.3680200<br />predictors: 19<br />predictors: 19<br />predictors: 19\",\"ntree: 301<br />MSE: 0.3596084<br />predictors: 19<br />predictors: 19<br />predictors: 19\",\"ntree: 401<br />MSE: 0.3726668<br />predictors: 19<br />predictors: 19<br />predictors: 19\",\"ntree: 501<br />MSE: 0.3706204<br />predictors: 19<br />predictors: 19<br />predictors: 19\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,196,1)\",\"dash\":\"longdashdot\"},\"hoveron\":\"points\",\"name\":\"19\",\"legendgroup\":\"19\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.61417411004752,0.372236226979121,0.368948611943404,0.362922284804175,0.358775690435265,0.370532894636705],\"text\":[\"ntree:   1<br />MSE: 0.6141741<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 101<br />MSE: 0.3722362<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 201<br />MSE: 0.3689486<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 301<br />MSE: 0.3629223<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 401<br />MSE: 0.3587757<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 501<br />MSE: 0.3705329<br />predictors: 3<br />predictors: 3<br />predictors: 3\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,176,246,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"name\":\"3\",\"legendgroup\":\"3\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.868157355329765,0.367166134779675,0.36291527410717,0.367454466896753,0.362269143984216,0.354995554790265],\"text\":[\"ntree:   1<br />MSE: 0.8681574<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 101<br />MSE: 0.3671661<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 201<br />MSE: 0.3629153<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 301<br />MSE: 0.3674545<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 401<br />MSE: 0.3622691<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 501<br />MSE: 0.3549956<br />predictors: 5<br />predictors: 5<br />predictors: 5\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(149,144,255,1)\",\"dash\":\"dashdot\"},\"hoveron\":\"points\",\"name\":\"5\",\"legendgroup\":\"5\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.963598359013147,0.35257182218199,0.355642502239179,0.350693194525137,0.363054250887282,0.368009632771206],\"text\":[\"ntree:   1<br />MSE: 0.9635984<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 101<br />MSE: 0.3525718<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 201<br />MSE: 0.3556425<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 301<br />MSE: 0.3506932<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 401<br />MSE: 0.3630543<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 501<br />MSE: 0.3680096<br />predictors: 7<br />predictors: 7<br />predictors: 7\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(231,107,243,1)\",\"dash\":\"dashdot\"},\"hoveron\":\"points\",\"name\":\"7\",\"legendgroup\":\"7\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.507282568957107,0.359309921576958,0.361063111593749,0.359500087972263,0.360198260789674,0.354346776110049],\"text\":[\"ntree:   1<br />MSE: 0.5072826<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 101<br />MSE: 0.3593099<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 201<br />MSE: 0.3610631<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 301<br />MSE: 0.3595001<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 401<br />MSE: 0.3601983<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 501<br />MSE: 0.3543468<br />predictors: 9<br />predictors: 9<br />predictors: 9\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(255,98,188,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"name\":\"9\",\"legendgroup\":\"9\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":27.8721461187215,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-24,526],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"100\",\"200\",\"300\",\"400\",\"500\"],\"tickvals\":[3.5527136788005e-15,100,200,300,400,500],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"100\",\"200\",\"300\",\"400\",\"500\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"ntree\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.316233722450844,0.994425246468495],\"tickmode\":\"array\",\"ticktext\":[\"0.4\",\"0.6\",\"0.8\"],\"tickvals\":[0.4,0.6,0.8],\"categoryorder\":\"array\",\"categoryarray\":[\"0.4\",\"0.6\",\"0.8\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"MSE\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":0.959399606299213},\"annotations\":[{\"text\":\"predictors\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"667818d757ea\":{\"x\":{},\"y\":{},\"colour\":{},\"linetype\":{},\"type\":\"scatter\"}},\"cur_data\":\"667818d757ea\",\"visdat\":{\"667818d757ea\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\r\nBased on the plot, we should use 13 predictors and 100 trees.\r\r\n\r\r\n\r\r\n bag.hitters <- randomForest(Salary ~., data = Hitters.train, mtry = 13,ntree = 100)\r\r\nyhat.bag <- predict(bag.hitters, newdata = Hitters.test)\r\r\npaste(\"The MSE for bagging is : \",(mean((yhat.bag- Hitters.response)^2)))\r\r\n\r\r\n\r\r\n[1] \"The MSE for bagging is :  0.362244453542562\"\r\r\n\r\r\nThe .3622 is slightly lower than the boosting MSE of .38.\r\r\nFinal Project IDEAS\r\r\nIdentify a data set that you plan to use for your project/poster and your likely collaborators. What outcome of interest do will you attempt to predict? Why do you expect that the available features (variables), or some subset of them, should help predict this outcome?\r\r\nFor the project, I plan to work alone. If I do stumble on certain problems, I will reach out via Piazza or I will go in for office hours.\r\r\nFor my dataset I went to Kaggle.com. I’m not entirely sure what data set I will use. I found the following data sets that I could possibly use:\r\r\nFor a descrete analysis I could use the Amazon seller dataset from Kagle. This data set is trying to predict whether an amazon order will go through. https://www.kaggle.com/pranalibose/amazon-seller-order-status-prediction This data set was created with the purpose of predicting order sucesses.\r\r\nI’d also like to possibly use stock data and indicators to predict whether a stock will increase or decrease, and to predict future prices. I know that a lot of people use technical indicators to make buy and sell decisions. I’d like to give that an attempt as well.\r\r\nI also found another data set that was interesting from Kagle. This data set looks at wind power. This data set is trying to predict how much wind is generated by the windmill in the following 15 days. https://www.kaggle.com/theforcecoder/wind-power-forecasting\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": {},
    "last_modified": "2022-03-13T18:56:16-04:00",
    "input_file": {}
  },
  {
    "path": "posts/machine-learning-analysis/",
    "title": "Machine Learning Analysis",
    "description": "A closer look into regression and classification models",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-03-13",
    "categories": [],
    "contents": "\r\r\nISLR Ch. 4, Exercise 16\r\r\nUsing the Boston data set, fit classification models in order to predict whether a given census tract has a crime rate above or below the median. Explore logistic regression, LDA, naive Bayes, and KNN models using various subsets of the predictors. Describe your findings. Hint: You will have to create the response variable yourself, using the variables that are contained in the Boston data set.\r\r\n\r\r\n\r\r\nsummary(Boston$crim)\r\r\n\r\r\n\r\r\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \r\r\n 0.00632  0.08204  0.25651  3.61352  3.67708 88.97620 \r\r\n\r\r\nBoston$response <- as.factor(ifelse(Boston$crim > median(Boston$crim),\"Above\",\"Below\"))\r\r\nBoston %>% filter(response==\"Above\") %>% head()\r\r\n\r\r\n\r\r\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat\r\r\n1 0.62976  0  8.14    0 0.538 5.949 61.8 4.7075   4 307      21  8.26\r\r\n2 0.63796  0  8.14    0 0.538 6.096 84.5 4.4619   4 307      21 10.26\r\r\n3 0.62739  0  8.14    0 0.538 5.834 56.5 4.4986   4 307      21  8.47\r\r\n4 1.05393  0  8.14    0 0.538 5.935 29.3 4.4986   4 307      21  6.58\r\r\n5 0.78420  0  8.14    0 0.538 5.990 81.7 4.2579   4 307      21 14.67\r\r\n6 0.80271  0  8.14    0 0.538 5.456 36.6 3.7965   4 307      21 11.69\r\r\n  medv response\r\r\n1 20.4    Above\r\r\n2 18.2    Above\r\r\n3 19.9    Above\r\r\n4 23.1    Above\r\r\n5 17.5    Above\r\r\n6 20.2    Above\r\r\n\r\r\nSet up Training and Test Sets\r\r\n\r\r\n\r\r\nset.seed(2)\r\r\n\r\r\ntrain <- sample(1:nrow(Boston), round((nrow(Boston)/4)*3,0))\r\r\n\r\r\nBoston.test <- Boston[-train,]\r\r\nboston.response <- Boston$response[-train]\r\r\nboston.train <- Boston[train,]\r\r\n\r\r\n\r\r\n\r\r\nLogistic Regression\r\r\n\r\r\n\r\r\nglm.fits <- glm(response ~ zn + indus+chas+nox+rm+age+dis+rad+tax, family = binomial, data = Boston, subset = train)\r\r\nsummary(glm.fits)\r\r\n\r\r\n\r\r\n\r\r\nCall:\r\r\nglm(formula = response ~ zn + indus + chas + nox + rm + age + \r\r\n    dis + rad + tax, family = binomial, data = Boston, subset = train)\r\r\n\r\r\nDeviance Residuals: \r\r\n     Min        1Q    Median        3Q       Max  \r\r\n-3.05155  -0.00772   0.00850   0.28098   1.80684  \r\r\n\r\r\nCoefficients:\r\r\n              Estimate Std. Error z value Pr(>|z|)    \r\r\n(Intercept)  24.861723   4.752476   5.231 1.68e-07 ***\r\r\nzn            0.061220   0.031442   1.947 0.051524 .  \r\r\nindus         0.008342   0.047150   0.177 0.859563    \r\r\nchas         -0.125309   0.847579  -0.148 0.882467    \r\r\nnox         -34.478533   6.935623  -4.971 6.65e-07 ***\r\r\nrm           -0.515107   0.353771  -1.456 0.145381    \r\r\nage          -0.011939   0.009914  -1.204 0.228468    \r\r\ndis          -0.338121   0.202467  -1.670 0.094918 .  \r\r\nrad          -0.594197   0.153996  -3.859 0.000114 ***\r\r\ntax           0.005827   0.002710   2.151 0.031508 *  \r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\n(Dispersion parameter for binomial family taken to be 1)\r\r\n\r\r\n    Null deviance: 526.53  on 379  degrees of freedom\r\r\nResidual deviance: 175.75  on 370  degrees of freedom\r\r\nAIC: 195.75\r\r\n\r\r\nNumber of Fisher Scoring iterations: 9\r\r\n\r\r\n#coef(glm.fits)\r\r\n#summary(glm.fits)$coef\r\r\n\r\r\n\r\r\nglm.probs <-predict(glm.fits, type = \"response\", Boston.test)\r\r\n#glm.probs %>% head()\r\r\n\r\r\ncontrasts(Boston$response)\r\r\n\r\r\n\r\r\n      Below\r\r\nAbove     0\r\r\nBelow     1\r\r\n\r\r\n#length(glm.probs)\r\r\n#dim(Boston)\r\r\nglm.pred <- rep(\"Above\",126)\r\r\nglm.pred[glm.probs >.5 ] = \"Below\"\r\r\n#Boston$response %>% length()\r\r\ntable(glm.pred,boston.response)\r\r\n\r\r\n\r\r\n        boston.response\r\r\nglm.pred Above Below\r\r\n   Above    55     3\r\r\n   Below    13    55\r\r\n\r\r\nmean(glm.pred==boston.response)\r\r\n\r\r\n\r\r\n[1] 0.8730159\r\r\n\r\r\nacc <- mean(glm.pred==boston.response)\r\r\n\r\r\nPrediction_Accuracy <- tibble(\"Model\" = \"Logistic Regression\", \"Accuracy\" = acc)\r\r\n\r\r\n\r\r\n\r\r\nThis means we were correct 87% of the time using the logistic analysis in predicting whether the crime level would be above the median using the variables chosen.\r\r\nLDA (Linear Discriminant Analysis)\r\r\n\r\r\n\r\r\nlibrary(MASS)\r\r\n\r\r\nlda.fit <- lda(response ~ zn + indus+chas+nox+rm+age+dis+rad+tax, data = Boston)\r\r\n#summary(lda.fit)\r\r\nlda.fit\r\r\n\r\r\n\r\r\nCall:\r\r\nlda(response ~ zn + indus + chas + nox + rm + age + dis + rad + \r\r\n    tax, data = Boston)\r\r\n\r\r\nPrior probabilities of groups:\r\r\nAbove Below \r\r\n  0.5   0.5 \r\r\n\r\r\nGroup means:\r\r\n             zn     indus       chas       nox       rm      age\r\r\nAbove  1.201581 15.271265 0.08695652 0.6384190 6.174874 85.83953\r\r\nBelow 21.525692  7.002292 0.05138340 0.4709711 6.394395 51.31028\r\r\n           dis       rad      tax\r\r\nAbove 2.498489 14.940711 510.7312\r\r\nBelow 5.091596  4.158103 305.7431\r\r\n\r\r\nCoefficients of linear discriminants:\r\r\n               LD1\r\r\nzn     0.004312445\r\r\nindus -0.014204281\r\r\nchas  -0.029319347\r\r\nnox   -7.380732435\r\r\nrm    -0.253144266\r\r\nage   -0.010855144\r\r\ndis    0.010054637\r\r\nrad   -0.084063067\r\r\ntax    0.001263545\r\r\n\r\r\nlda.pred <- predict(lda.fit,Boston)\r\r\nlda.class <- lda.pred$class\r\r\n\r\r\ntable(lda.class,Boston$response)\r\r\n\r\r\n\r\r\n         \r\r\nlda.class Above Below\r\r\n    Above   194    14\r\r\n    Below    59   239\r\r\n\r\r\nmean(lda.class==Boston$response)\r\r\n\r\r\n\r\r\n[1] 0.8557312\r\r\n\r\r\nacc <- mean(lda.class==Boston$response)\r\r\n\r\r\nPrediction_Accuracy <- rbind(Prediction_Accuracy,  c(\"LDA\", acc))\r\r\n\r\r\n\r\r\n\r\r\nFor LDA, 86% of predictions were correct. \r\r\nNaive Bayes\r\r\n\r\r\n\r\r\nlibrary(e1071)\r\r\n\r\r\nnb.fit <- naiveBayes(response ~ zn + indus+chas+nox+rm+age+dis+rad+tax, family = binomial, data = Boston, subset = train)\r\r\n#nb.fit\r\r\n\r\r\nnb.class <- predict(nb.fit, Boston.test)\r\r\ntable(nb.class, boston.response)\r\r\n\r\r\n\r\r\n        boston.response\r\r\nnb.class Above Below\r\r\n   Above    52    11\r\r\n   Below    16    47\r\r\n\r\r\nmean(nb.class == boston.response)\r\r\n\r\r\n\r\r\n[1] 0.7857143\r\r\n\r\r\nacc <- mean(lda.class==Boston$response)\r\r\n\r\r\nPrediction_Accuracy <- rbind(Prediction_Accuracy, c(\"Naive Bayes\", acc))\r\r\n\r\r\n\r\r\n\r\r\nUsing Naive Bayes, we were able to get a prediction accuracy of 78%. \r\r\nK nearest Neighbors\r\r\n\r\r\n\r\r\nlibrary(class)\r\r\nattach(Boston)\r\r\nset.seed(2)\r\r\n#Boston %>% dim()\r\r\ntrain.x <- Boston[train,colnames(Boston) %in% c(\"zn\",\"indus\",\"chas\",\"nox\",\"rm\",\"age\",\"dis\",\"rad\",\"tax\")]\r\r\n\r\r\ntest.x <- Boston[-train,colnames(Boston) %in% c(\"zn\",\"indus\",\"chas\",\"nox\",\"rm\",\"age\",\"dis\",\"rad\",\"tax\")]\r\r\ntrain.response <- Boston$response[train]\r\r\ntest.response <- Boston$response[-train]\r\r\n#train.x %>% dim()\r\r\n#test.x %>% dim()\r\r\n#train.response %>% length()\r\r\n\r\r\nknn.pred <- knn(train.x,test.x, train.response, k =1)\r\r\n#knn.pred %>% length()\r\r\n#test.response %>% length()\r\r\n#knn.pred %>% length()\r\r\n#test.response %>% length()\r\r\n\r\r\ntable(knn.pred,test.response)\r\r\n\r\r\n\r\r\n        test.response\r\r\nknn.pred Above Below\r\r\n   Above    66     7\r\r\n   Below     2    51\r\r\n\r\r\npaste(\"Percent Accuracy with K = 1\",mean(knn.pred == test.response))\r\r\n\r\r\n\r\r\n[1] \"Percent Accuracy with K = 1 0.928571428571429\"\r\r\n\r\r\nknn.pred <- knn(train.x,test.x, train.response, k =3)\r\r\npaste(\"Percent Accuracy with K =3\",mean(knn.pred == test.response))\r\r\n\r\r\n\r\r\n[1] \"Percent Accuracy with K =3 0.968253968253968\"\r\r\n\r\r\nknn.pred <- knn(train.x,test.x, train.response, k =5)\r\r\npaste(\"Percent Accuracy with K = 5\",mean(knn.pred == test.response))\r\r\n\r\r\n\r\r\n[1] \"Percent Accuracy with K = 5 0.952380952380952\"\r\r\n\r\r\nx<- NULL\r\r\ny <- NULL\r\r\nfor(i in 1:100){\r\r\n  knn.pred <- knn(train.x,test.x, train.response, k =i)\r\r\n  x<- rbind(x,mean(knn.pred == test.response))\r\r\n  y <- rbind(y,paste(i))\r\r\n}\r\r\ndata.frame(\"Kvalue\" = y, \"Accuracy\" = x) %>% arrange(desc(x)) %>% head() %>% kable()\r\r\n\r\r\n\r\r\nKvalue\r\r\nAccuracy\r\r\n3\r\r\n0.9682540\r\r\n4\r\r\n0.9682540\r\r\n6\r\r\n0.9603175\r\r\n5\r\r\n0.9523810\r\r\n1\r\r\n0.9285714\r\r\n7\r\r\n0.9285714\r\r\n\r\r\nknn.pred <- knn(train.x,test.x, train.response, k =3)\r\r\nacc <- mean(knn.pred == test.response)\r\r\n\r\r\nPrediction_Accuracy <- rbind(Prediction_Accuracy, c(\"KNN\", acc))\r\r\n\r\r\n\r\r\n\r\r\nIn order to set up the KNN model, I needed to create 4 data sets. A training and Testing data set, and a test response, and training response. I then decided to run the model and use various values of K. The first bit of code is a bit inefficient in deciding the best value of K. This can be seen in the first 3 predictions for K = 1, 3, and 5.\r\r\nIn order to find the best value of K, I decided to create a loop that would try every K value from 1 to 100 and store the models % accuracy in a vector named x. I then created a dataframe to visualize the x values and the corresponding K values used.\r\r\nUsing KNN, I created 100 models with a k value form 1 to 100. The top 6 are listed above. The K value of 3 was the best value for K.\r\r\n\r\r\n\r\r\nPrediction_Accuracy %>% arrange(desc(Accuracy)) %>% kable()\r\r\n\r\r\n\r\r\nModel\r\r\nAccuracy\r\r\nKNN\r\r\n0.968253968253968\r\r\nLogistic Regression\r\r\n0.873015873015873\r\r\nLDA\r\r\n0.855731225296443\r\r\nNaive Bayes\r\r\n0.855731225296443\r\r\n\r\r\nI would recommend using the KNN to predict whether a crime level will be higher than the median for the Boston data set. It has the highest accuracy at 97%.\r\r\n\r\r\nISLR Ch. 5, Exercise 3 (or substitute advanced: Exercise 2)\r\r\n\r\r\nA.\r\r\nK fold is implemented by dividing the set of cases into a number of groups which is dictated by K. Each group will have an equal length.\r\r\nThe first group is the validation set. The model is trained on the other groups. So if we had a value of 10 for K. Group 1 would be the validation set and the 9 other groups would be the training groups. A Mean squared error is calculated for the group. This process will be carried out for each group. At the end we will have a MSE for each group. Finally, we take the average of all the MSE’s and that gives us the k fold CV estimate.\r\r\nB.\r\r\nThe Validation Set Approach:\r\r\nIf K is set to low, then it approaches the same as using the validation set approach. Test Error rate will most likely be higher in the validation approach when compared to kfold validation.\r\r\nValidation set approach will most likely have a high bias and cause over fitting when compared to the k fold approach.\r\r\nLOOCV:\r\r\nComputation advantage when using kfold versus LOOCV. K fold will require less computation/resources/time, then compared to the LOOCV.\r\r\nLOOCV has less bias, which means less overfitting when applied to other datasets, but will have more variance when compared to K fold and when compared to the validation set approach.\r\r\n\r\r\nISLR Ch. 5, Exercise 5\r\r\n\r\r\n\r\r\nset.seed(5)\r\r\n\r\r\nData <- Default\r\r\n#Data %>% head()\r\r\n#Data %>% names()\r\r\n\r\r\ntrain <-  sample(1:nrow(Data), round((nrow(Data)/4)*3,0))\r\r\n#train %>% length()\r\r\n#Data %>% dim()\r\r\n\r\r\n\r\r\n### Create training Sets\r\r\nData.test <- Data[-train,]\r\r\nData.response <- Data$default[-train]\r\r\n\r\r\n\r\r\n# Fit model\r\r\nglm.fits <- glm(default ~ income+balance, family = binomial, data = Data, subset = train)\r\r\n#summary(glm.fits)\r\r\n#coef(glm.fits)\r\r\n#summary(glm.fits)$coef\r\r\nglm.probs <-predict(glm.fits, type = \"response\", Data.test)\r\r\n\r\r\n\r\r\ncontrasts(Data$default)\r\r\n\r\r\n\r\r\n    Yes\r\r\nNo    0\r\r\nYes   1\r\r\n\r\r\n#length(glm.probs)\r\r\n#dim(Data.test)\r\r\nglm.pred <- rep(\"No\",2500)\r\r\nglm.pred[glm.probs >.5 ] = \"Yes\"\r\r\n\r\r\ntable(glm.pred,Data.response)\r\r\n\r\r\n\r\r\n        Data.response\r\r\nglm.pred   No  Yes\r\r\n     No  2417   48\r\r\n     Yes    7   28\r\r\n\r\r\nmean(glm.pred==Data.response)\r\r\n\r\r\n\r\r\n[1] 0.978\r\r\n\r\r\npaste0(\"The test Error Rate is : \",(1-(mean(glm.pred==Data.response)))*100,\"%\" )\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate is : 2.2%\"\r\r\n\r\r\n\r\r\nFor this problem, we will run the sample function 3 times to get a different training set. I will fit the model, and then I will calculate the test error rate again. For ease, I will create a loop. I create a new seed each time I run the loop.\r\r\n\r\r\n\r\r\nfor(i in 1:3){\r\r\nset.seed(i)\r\r\ntrain <-  sample(1:nrow(Data), round((nrow(Data)/4)*3,0))\r\r\n# Create training Sets\r\r\nData.test <- Data[-train,]\r\r\nData.response <- Data$default[-train]\r\r\n# Fit model\r\r\nglm.fits <- glm(default ~ income+balance, family = binomial, data = Data, subset = train)\r\r\nglm.probs <-predict(glm.fits, type = \"response\", Data.test)\r\r\nglm.pred <- rep(\"No\",2500)\r\r\nglm.pred[glm.probs >.5 ] = \"Yes\"\r\r\nmean(glm.pred==Data.response)\r\r\nprint(paste0(\"The test Error Rate is : \",(1-(mean(glm.pred==Data.response)))*100,\"%\" ))\r\r\n}\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate is : 2.6%\"\r\r\n[1] \"The test Error Rate is : 2.12%\"\r\r\n[1] \"The test Error Rate is : 2.6%\"\r\r\n\r\r\nWhen Running the model with 3 different seeds. I saw that the error rate fluctuated from 2.12 % up to 2.6%.\r\r\n\r\r\n\r\r\n\r\r\nglm.fits <- glm(default ~ ., family = binomial, data = Data, subset = train)\r\r\nglm.probs <-predict(glm.fits, type = \"response\", Data.test)\r\r\nglm.pred <- rep(\"No\",2500)\r\r\nglm.pred[glm.probs >.5 ] = \"Yes\"\r\r\n\r\r\ntable(glm.pred,Data.response)\r\r\n\r\r\n\r\r\n        Data.response\r\r\nglm.pred   No  Yes\r\r\n     No  2416   54\r\r\n     Yes   13   17\r\r\n\r\r\nmean(glm.pred==Data.response)\r\r\n\r\r\n\r\r\n[1] 0.9732\r\r\n\r\r\npaste0(\"The test Error Rate is : \",(1-(mean(glm.pred==Data.response)))*100,\"%\" )\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate is : 2.68%\"\r\r\n\r\r\nUsing the dummy variable is about the same as the last error rate calculated in part c (2.68). I’m comparing it to that result, since the current sample seed is the one used in that loop.\r\r\n\r\r\nISLR Ch. 5, Exercise 9\r\r\n\r\r\n\r\r\n\r\r\nu_ <- Boston$medv %>% mean()\r\r\npaste(\"The population mean of the medv:\",u_)\r\r\n\r\r\n\r\r\n[1] \"The population mean of the medv: 22.5328063241107\"\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nsd_error <- Boston$medv%>%sd()/(length(Boston)^(1/2))\r\r\n\r\r\npaste(\"The Standard error of the sample mean is:\", sd_error)\r\r\n\r\r\n\r\r\n[1] \"The Standard error of the sample mean is: 2.45802946039096\"\r\r\n\r\r\n\r\r\n\r\r\nlibrary(boot)\r\r\n\r\r\nalpha.fn <- function(data, index){\r\r\n  medv <-data$medv[index]\r\r\n  mean_1 <- mean(medv)\r\r\n \r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n#Boston$medv %>% length()\r\r\n\r\r\n\r\r\nboot(Boston, alpha.fn,10000)\r\r\n\r\r\n\r\r\n\r\r\nORDINARY NONPARAMETRIC BOOTSTRAP\r\r\n\r\r\n\r\r\nCall:\r\r\nboot(data = Boston, statistic = alpha.fn, R = 10000)\r\r\n\r\r\n\r\r\nBootstrap Statistics :\r\r\n    original       bias    std. error\r\r\nt1* 22.53281 0.0006395257    0.407591\r\r\n\r\r\nThe standard error is lower using the Bootstrap when compared to the answer from part c.\r\r\n\r\r\n\r\r\n\r\r\nt.test(Boston$medv)\r\r\n\r\r\n\r\r\n\r\r\n    One Sample t-test\r\r\n\r\r\ndata:  Boston$medv\r\r\nt = 55.111, df = 505, p-value < 2.2e-16\r\r\nalternative hypothesis: true mean is not equal to 0\r\r\n95 percent confidence interval:\r\r\n 21.72953 23.33608\r\r\nsample estimates:\r\r\nmean of x \r\r\n 22.53281 \r\r\n\r\r\nmean_boot <- 22.53281\r\r\nstd.boot <- .4137494\r\r\n\r\r\nupperbound <- mean_boot + 2*std.boot\r\r\nlowerbound <- mean_boot -2*std.boot\r\r\n\r\r\npaste0(\"The confidence interval using the boot: [\",lowerbound,\" , \",upperbound,\"]\")\r\r\n\r\r\n\r\r\n[1] \"The confidence interval using the boot: [21.7053112 , 23.3603088]\"\r\r\n\r\r\n\r\r\nThe boot and the t test confidence intervals are very close, almost identical.\r\r\n\r\r\n\r\r\n\r\r\npaste(\"The median value of medv in the population\", Boston$medv %>% median())\r\r\n\r\r\n\r\r\n[1] \"The median value of medv in the population 21.2\"\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nalpha.median <- function(data, index){\r\r\n  medv <-data$medv[index]\r\r\n  median_1 <- median(medv)\r\r\n\r\r\n}\r\r\n\r\r\nboot(Boston, alpha.median, 10000)\r\r\n\r\r\n\r\r\n\r\r\nORDINARY NONPARAMETRIC BOOTSTRAP\r\r\n\r\r\n\r\r\nCall:\r\r\nboot(data = Boston, statistic = alpha.median, R = 10000)\r\r\n\r\r\n\r\r\nBootstrap Statistics :\r\r\n    original    bias    std. error\r\r\nt1*     21.2 -0.013115   0.3808402\r\r\n\r\r\nThe standard error of the median is .3768. This standard error is very small for the original median of 21.2.\r\r\n\r\r\n\r\r\n\r\r\nquantile(Boston$medv, probs = c(.1,.2,.5,.75,.9))\r\r\n\r\r\n\r\r\n  10%   20%   50%   75%   90% \r\r\n12.75 15.30 21.20 25.00 34.80 \r\r\n\r\r\npaste(\"The 10th percentile is :\", quantile(Boston$medv, probs = .1),\"%\")\r\r\n\r\r\n\r\r\n[1] \"The 10th percentile is : 12.75 %\"\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nalpha.quantile <- function(data, index){\r\r\n  medv <-data$medv[index]\r\r\n  perc_10 <- quantile(medv, probs = .1)\r\r\n\r\r\n}\r\r\n\r\r\nboot(Boston, alpha.quantile, 10000)\r\r\n\r\r\n\r\r\n\r\r\nORDINARY NONPARAMETRIC BOOTSTRAP\r\r\n\r\r\n\r\r\nCall:\r\r\nboot(data = Boston, statistic = alpha.quantile, R = 10000)\r\r\n\r\r\n\r\r\nBootstrap Statistics :\r\r\n    original   bias    std. error\r\r\nt1*    12.75 0.007025   0.4981081\r\r\n\r\r\ndetach(Boston)\r\r\n\r\r\n\r\r\n\r\r\nThe standard error for the 10% quantile is .5 for a original value of 12.75.\r\r\n\r\r\nISLR Ch. 6, Exercise 2\r\r\nReferenced Article: https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\r\r\nI found this article helpful in understanding accuracy with reference to bias/variance tradeoff.\r\r\nFigure 2.7 in Textbook also helped with flexibility of different models.\r\r\n\r\r\nA.\r\r\nI\r\r\nMore flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\r\r\nIncorrect. Lasso is less flexible than Least squares.\r\r\nII\r\r\nMore flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\r\r\nIncorrect - less flexible than OLS\r\r\nIII\r\r\nLess flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\r\r\nCorrect - Low bias and high variance will cause overfitting, but will have a higher prediction accuracy than underfitting the data. since the variance is not increasing, we will have more coefficents that explain the response being measuared, and lamda will not go to zero for those predictors.\r\r\nIV\r\r\nLess flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\r\r\nIncorrect - Increase in variance < then decrease in bias would give a situation of high variance and high bias.\r\r\nAlso this is not how the lasso works, lambda will get rid of the variables that have high variance, by setting their cofficents to\r\r\nB\r\r\nRepeat (a) for ridge regression relative to least squares.\r\r\ni\r\r\nIncorrect - Less Flexible\r\r\nII\r\r\nIncorrect - Less Flexible\r\r\nIII\r\r\nCorrect - As lamda increases bias, flexibility decreases, which means we get a decreased variance and increased bias.\r\r\nSlight increase in bias with less variance increases prediction accuracy.\r\r\nIV\r\r\nIncorrect. Larger increase in varriance associated with smaller decrease in bias would not improve prediction accuracy.\r\r\nC\r\r\nRepeat (a) for non-linear methods relative to least squares.\r\r\ni\r\r\nCorrect - More flexible model, Slight increase in bias with less variance increases prediction accuracy.\r\r\nii-iV\r\r\nIncorrect.\r\r\n\r\r\nISLR Ch. 6, Exercise 9\r\r\n(omit e & f) (requires time and effort; please collaborate & use Piazza)\r\r\nIn this exercise, we will predict the number of applications received using the other variables in the College data set.\r\r\nA\r\r\nSplit the data set into a training set and a test set.\r\r\n\r\r\n\r\r\nlibrary(ISLR2)\r\r\n\r\r\nset.seed(1)\r\r\n\r\r\n#College %>% dim()\r\r\n#College %>% names()\r\r\n#College %>% head()\r\r\n#?College\r\r\n\r\r\ntrain <-  sample(1:nrow(College), round((nrow(College)/4)*3,0))\r\r\n#train %>% length()\r\r\n#train %>% head()\r\r\n\r\r\n\r\r\n\r\r\n# Create Test actuals and training Sets\r\r\n\r\r\nCollege.test <- College[-train,]\r\r\nCollege.response <- College$Apps[-train]\r\r\ncollege.train <- College[train,]\r\r\n\r\r\n\r\r\n\r\r\nAfter looking into College data set, I identified that our response variable will be the column Apps. The training set will be the college dataframe filtered on the train index.\r\r\nB\r\r\nFit a linear model using least squares on the training set, and report the test error obtained.\r\r\n\r\r\n\r\r\nattach(College)\r\r\nlm.fit <- lm(Apps ~., data = College, subset = train)\r\r\nlm.pred <- predict(lm.fit, College.test, type = \"response\")\r\r\nlm_mse <- mean((College.response -lm.pred )^2)\r\r\nlm_mse\r\r\n\r\r\n\r\r\n[1] 1389123\r\r\n\r\r\nMSE_dataframe <- data.frame(\"model\" = \"Least Square\", \"MSE\" = lm_mse)\r\r\n\r\r\n\r\r\n\r\r\nC\r\r\nFit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.\r\r\n\r\r\n\r\r\nlibrary(glmnet)\r\r\n\r\r\n\r\r\nx <- model.matrix(Apps~.,College)[,-1]\r\r\ny <- College$Apps\r\r\ngrid <- 10^seq(10,-2, length = 100)\r\r\n\r\r\nridge.mod <- glmnet(x[train,], y[train], alpha =0, lambda = grid )\r\r\ncv.out <- cv.glmnet(x[train,], y[train],alpha = 0)\r\r\nplot(cv.out)\r\r\n\r\r\n\r\r\n\r\r\nlamda_best <- cv.out$lambda.min\r\r\nlamda_best\r\r\n\r\r\n\r\r\n[1] 364.3384\r\r\n\r\r\nridge.pred <- predict(ridge.mod, s = lamda_best, newx = x[-train,])\r\r\nridge_mse <- mean((ridge.pred - y[-train])^2)\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"ridge\",ridge_mse))\r\r\n\r\r\npaste(\"The test Error Rate is : \", mean((ridge.pred - y[-train])^2))\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate is :  1204431.39092091\"\r\r\n\r\r\nD\r\r\nFit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\r\r\n\r\r\n\r\r\nlasso.mod <- glmnet(x[train,],y[train],alpha = 1, lambda = grid)\r\r\ncv.out <- cv.glmnet(x[train,], y[train], alpha = 1)\r\r\nplot(cv.out)\r\r\n\r\r\n\r\r\n\r\r\nlamda_best <- cv.out$lambda.min\r\r\nlamda_best\r\r\n\r\r\n\r\r\n[1] 2.133937\r\r\n\r\r\nlasso.pred <- predict(lasso.mod, s= lamda_best, newx = x[-train,])\r\r\npaste(\"The test error rate is \",mean((lasso.pred - y[-train])^2))\r\r\n\r\r\n\r\r\n[1] \"The test error rate is  1374692.34765744\"\r\r\n\r\r\nlasso_mse <- mean((lasso.pred - y[-train])^2)\r\r\n\r\r\nout <- glmnet(x,y, alpha = 1 ,lambda = grid)\r\r\nlamda.coef <- predict(out, type = \"coefficients\", s= lamda_best)[1:18,]\r\r\npaste(\"The number of non-zero coeficients:\",lamda.coef[lamda.coef !=0] %>% length())\r\r\n\r\r\n\r\r\n[1] \"The number of non-zero coeficients: 18\"\r\r\n\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"Lasso\",lasso_mse))\r\r\n\r\r\n\r\r\n\r\r\nNone of the coefficients are zero.\r\r\nE\r\r\nFit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.\r\r\n\r\r\n\r\r\nlibrary(pls)\r\r\nset.seed(10)\r\r\npcr.fit <- pcr(Apps ~ . , data = College, subset = train, scale = TRUE, validation = \"CV\")\r\r\nvalidationplot(pcr.fit, val.type = \"MSEP\")\r\r\n\r\r\n\r\r\n\r\r\npcr.pred <- predict(pcr.fit,College.test, ncomp = 17)\r\r\npcr_mse<- mean((pcr.pred - College.response)^2)\r\r\npaste(\"The MSE error is \", pcr_mse)\r\r\n\r\r\n\r\r\n[1] \"The MSE error is  1389123.27022729\"\r\r\n\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"PCR\",pcr_mse))\r\r\n\r\r\n\r\r\n\r\r\nFrom the validation plot, the lowest MSEP for the number of components is around 16,17, and 18.I decided to use 17 as my value of M.\r\r\nF\r\r\nFit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.\r\r\n\r\r\n\r\r\npls.fit <- plsr(Apps~., data = College, subset = train, scale = TRUE, validation = \"CV\")\r\r\n\r\r\nvalidationplot(pls.fit, val.type = \"MSEP\")\r\r\n\r\r\n\r\r\n\r\r\npls.pred_7<- predict(pls.fit, College.test, ncomp = 7)\r\r\npls.pred_8 <- predict(pls.fit, College.test, ncomp = 8)\r\r\n\r\r\npaste(\"The MSE error using M as 7: \", mean((pls.pred_7- College.response)^2))\r\r\n\r\r\n\r\r\n[1] \"The MSE error using M as 7:  1336641.93400001\"\r\r\n\r\r\npaste(\"The MSE error using M as 8: \", mean((pls.pred_8- College.response)^2))\r\r\n\r\r\n\r\r\n[1] \"The MSE error using M as 8:  1362535.73332994\"\r\r\n\r\r\npls_mse <- mean((pls.pred_8- College.response)^2)\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"PLS\",pls_mse))\r\r\n\r\r\n\r\r\n\r\r\nThe number of components ranges from 7 to 8. I will predict using both values of M.\r\r\nG\r\r\nComment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?\r\r\n\r\r\n\r\r\nMSE_dataframe %>% arrange(MSE) %>% kable()\r\r\n\r\r\n\r\r\nmodel\r\r\nMSE\r\r\nridge\r\r\n1204431.39092091\r\r\nPLS\r\r\n1362535.73332994\r\r\nLasso\r\r\n1374692.34765744\r\r\nLeast Square\r\r\n1389123.27022729\r\r\nPCR\r\r\n1389123.27022729\r\r\n\r\r\nMSE_dataframe$MSE <- as.numeric(MSE_dataframe$MSE)\r\r\n\r\r\nMSE_dataframe %>% ggplot2::ggplot(aes(x = model, y = MSE)) + geom_col(fill = \"deepskyblue3\")+ theme(axis.text.y = element_blank())+labs(title = \"MSE per Model\")+geom_text(aes(label = round(as.numeric(MSE),0)), vjust = 2)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nThe Ridge regression model had the lowest MSE, with PLS coming up the second lowest MSE, following Lasso and the PCR and Least square in last. There is not too much of a difference between the MSE’s, it looks larger due to the scale on the graph.\r\r\n\r\r\nISLR Ch. 8, Exercise 4\r\r\n\r\r\nA.\r\r\n\r\r\nB\r\r\n\r\r\n\r\r\nISLR Ch. 8, Exercise 7\r\r\n\r\r\n\r\r\n\r\r\nlibrary(randomForest); library(ggplot2)\r\r\nset.seed(1)\r\r\ntrain <- sample (1: nrow (Boston), (nrow (Boston) / 4)*3)\r\r\nboston.response_test <- Boston$medv[-train]\r\r\ndf <- tibble(\"predictors\" = c(),\"ntree\" = double(), \"MSE\" = double())\r\r\n\r\r\n\r\r\nfor(i in seq(1, 13,2)){\r\r\n  for(x in seq(1, 600,25)){\r\r\n  bag.boston <- randomForest(medv ~., data = Boston, subset = train, mtry = i,ntree = x)\r\r\nyhat.bag <- predict(bag.boston, newdata = Boston[-train,])\r\r\nmse_1 <- (mean((yhat.bag- boston.response_test)^2))\r\r\ndf <-df %>% add_row(predictors = as.character(i),ntree = x, MSE = mse_1)\r\r\n  }\r\r\n}\r\r\n\r\r\nplot<- df %>% ggplot()+geom_line(aes(x = ntree, y = MSE, group = predictors,color = predictors, linetype = predictors))\r\r\n\r\r\nplotly::ggplotly(plot)\r\r\n\r\r\n\r\r\n\r\r\n{\"x\":{\"data\":[{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[36.4030132360704,20.1672870413967,18.7087560009328,19.1354783627998,18.496417690562,19.1775878361241,19.2898228736319,18.4637276207247,18.2145217932484,19.3112908727648,18.2604710744005,18.8832646753751,18.3325016116797,18.5013258158174,18.5583502300008,18.0282923967408,18.8787594530618,18.7177851193332,19.1305364079644,18.3444224181792,18.4552723600292,18.0868986971722,18.4964681541742,17.8802581871862],\"text\":[\"ntree:   1<br />MSE: 36.40301<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree:  26<br />MSE: 20.16729<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree:  51<br />MSE: 18.70876<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree:  76<br />MSE: 19.13548<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 101<br />MSE: 18.49642<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 126<br />MSE: 19.17759<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 151<br />MSE: 19.28982<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 176<br />MSE: 18.46373<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 201<br />MSE: 18.21452<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 226<br />MSE: 19.31129<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 251<br />MSE: 18.26047<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 276<br />MSE: 18.88326<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 301<br />MSE: 18.33250<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 326<br />MSE: 18.50133<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 351<br />MSE: 18.55835<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 376<br />MSE: 18.02829<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 401<br />MSE: 18.87876<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 426<br />MSE: 18.71779<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 451<br />MSE: 19.13054<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 476<br />MSE: 18.34442<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 501<br />MSE: 18.45527<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 526<br />MSE: 18.08690<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 551<br />MSE: 18.49647<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 576<br />MSE: 17.88026<br />predictors: 1<br />predictors: 1<br />predictors: 1\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"1\",\"legendgroup\":\"1\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[27.8376998031496,16.381568998522,16.151450555939,15.1073078911924,15.6034328258243,15.5810081577421,15.3438551453429,14.683301877896,15.2702784168452,15.3317851316579,15.0863588551852,15.5714223302754,14.9814345841696,15.3187311977802,15.526801532571,15.007413021855,15.3519339589946,15.4099309083397,15.3871500250433,15.4012488824985,15.0536589769078,15.584226152949,15.0936646237656,15.1259037440271],\"text\":[\"ntree:   1<br />MSE: 27.83770<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree:  26<br />MSE: 16.38157<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree:  51<br />MSE: 16.15145<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree:  76<br />MSE: 15.10731<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 101<br />MSE: 15.60343<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 126<br />MSE: 15.58101<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 151<br />MSE: 15.34386<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 176<br />MSE: 14.68330<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 201<br />MSE: 15.27028<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 226<br />MSE: 15.33179<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 251<br />MSE: 15.08636<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 276<br />MSE: 15.57142<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 301<br />MSE: 14.98143<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 326<br />MSE: 15.31873<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 351<br />MSE: 15.52680<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 376<br />MSE: 15.00741<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 401<br />MSE: 15.35193<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 426<br />MSE: 15.40993<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 451<br />MSE: 15.38715<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 476<br />MSE: 15.40125<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 501<br />MSE: 15.05366<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 526<br />MSE: 15.58423<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 551<br />MSE: 15.09366<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 576<br />MSE: 15.12590<br />predictors: 11<br />predictors: 11<br />predictors: 11\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(196,154,0,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"name\":\"11\",\"legendgroup\":\"11\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[23.1885851487314,16.3271876644626,16.1047661877473,16.3791585132342,16.6291119647191,16.4217080410046,16.6730086205132,16.0405735001123,16.0220958864348,16.0493263788259,16.0666392771661,15.5211045158706,15.6571753152715,15.6984812880089,15.6301215460534,15.6347657250359,15.9207285686653,15.8682693914913,15.9122302907001,16.160446247243,15.7876944857743,16.1627848489863,15.9502883447923,16.185244746838],\"text\":[\"ntree:   1<br />MSE: 23.18859<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree:  26<br />MSE: 16.32719<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree:  51<br />MSE: 16.10477<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree:  76<br />MSE: 16.37916<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 101<br />MSE: 16.62911<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 126<br />MSE: 16.42171<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 151<br />MSE: 16.67301<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 176<br />MSE: 16.04057<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 201<br />MSE: 16.02210<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 226<br />MSE: 16.04933<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 251<br />MSE: 16.06664<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 276<br />MSE: 15.52110<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 301<br />MSE: 15.65718<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 326<br />MSE: 15.69848<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 351<br />MSE: 15.63012<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 376<br />MSE: 15.63477<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 401<br />MSE: 15.92073<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 426<br />MSE: 15.86827<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 451<br />MSE: 15.91223<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 476<br />MSE: 16.16045<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 501<br />MSE: 15.78769<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 526<br />MSE: 16.16278<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 551<br />MSE: 15.95029<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 576<br />MSE: 16.18524<br />predictors: 13<br />predictors: 13<br />predictors: 13\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(83,180,0,1)\",\"dash\":\"dot\"},\"hoveron\":\"points\",\"name\":\"13\",\"legendgroup\":\"13\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[24.2867815122725,12.3097981555951,13.2899636404797,12.5922919059138,11.901794350884,11.8469873720801,11.4910846625509,11.8795585953659,11.4774907434225,12.1788916340457,11.5700539828036,11.5894458676662,11.2340814879293,11.6815794427441,11.7074766939527,11.2447940285054,11.2516583730643,11.6852042911835,11.5307976987236,11.5382852856446,11.5206350639931,11.5700853879932,11.3575994037437,11.3967207815116],\"text\":[\"ntree:   1<br />MSE: 24.28678<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree:  26<br />MSE: 12.30980<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree:  51<br />MSE: 13.28996<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree:  76<br />MSE: 12.59229<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 101<br />MSE: 11.90179<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 126<br />MSE: 11.84699<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 151<br />MSE: 11.49108<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 176<br />MSE: 11.87956<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 201<br />MSE: 11.47749<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 226<br />MSE: 12.17889<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 251<br />MSE: 11.57005<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 276<br />MSE: 11.58945<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 301<br />MSE: 11.23408<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 326<br />MSE: 11.68158<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 351<br />MSE: 11.70748<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 376<br />MSE: 11.24479<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 401<br />MSE: 11.25166<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 426<br />MSE: 11.68520<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 451<br />MSE: 11.53080<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 476<br />MSE: 11.53829<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 501<br />MSE: 11.52064<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 526<br />MSE: 11.57009<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 551<br />MSE: 11.35760<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 576<br />MSE: 11.39672<br />predictors: 3<br />predictors: 3<br />predictors: 3\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,192,148,1)\",\"dash\":\"dashdot\"},\"hoveron\":\"points\",\"name\":\"3\",\"legendgroup\":\"3\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[31.3454156338068,14.2111065693747,12.2659125711638,11.6653790175755,11.9028340711658,11.9682225145091,11.8588202019782,11.7650163287732,11.5963491624945,11.7296883364314,12.3432801031288,11.7395155861203,11.9830501030999,11.9127830303761,12.1803461640039,12.6475849384718,12.0449384438374,12.4057677552547,12.2842212042761,12.3943910987525,12.0961226067077,11.8386920877585,11.8533340005994,12.2655681649624],\"text\":[\"ntree:   1<br />MSE: 31.34542<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree:  26<br />MSE: 14.21111<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree:  51<br />MSE: 12.26591<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree:  76<br />MSE: 11.66538<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 101<br />MSE: 11.90283<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 126<br />MSE: 11.96822<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 151<br />MSE: 11.85882<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 176<br />MSE: 11.76502<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 201<br />MSE: 11.59635<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 226<br />MSE: 11.72969<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 251<br />MSE: 12.34328<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 276<br />MSE: 11.73952<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 301<br />MSE: 11.98305<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 326<br />MSE: 11.91278<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 351<br />MSE: 12.18035<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 376<br />MSE: 12.64758<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 401<br />MSE: 12.04494<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 426<br />MSE: 12.40577<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 451<br />MSE: 12.28422<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 476<br />MSE: 12.39439<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 501<br />MSE: 12.09612<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 526<br />MSE: 11.83869<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 551<br />MSE: 11.85333<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 576<br />MSE: 12.26557<br />predictors: 5<br />predictors: 5<br />predictors: 5\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,182,235,1)\",\"dash\":\"longdash\"},\"hoveron\":\"points\",\"name\":\"5\",\"legendgroup\":\"5\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[23.7007902012248,13.1657455337221,14.0937209565992,12.6350737975176,14.1611095307225,13.4832536439717,12.7699077265545,13.2423457680251,12.9255826430514,13.0021587712479,12.6451985361649,13.5994099184761,12.7978397312921,12.9726545106415,12.7602862562019,13.0227192839547,12.900307636034,13.5060756503014,12.6563400858669,13.3638675355722,12.9117074954309,12.93785098855,13.0279022924108,12.8812600065364],\"text\":[\"ntree:   1<br />MSE: 23.70079<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree:  26<br />MSE: 13.16575<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree:  51<br />MSE: 14.09372<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree:  76<br />MSE: 12.63507<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 101<br />MSE: 14.16111<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 126<br />MSE: 13.48325<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 151<br />MSE: 12.76991<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 176<br />MSE: 13.24235<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 201<br />MSE: 12.92558<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 226<br />MSE: 13.00216<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 251<br />MSE: 12.64520<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 276<br />MSE: 13.59941<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 301<br />MSE: 12.79784<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 326<br />MSE: 12.97265<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 351<br />MSE: 12.76029<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 376<br />MSE: 13.02272<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 401<br />MSE: 12.90031<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 426<br />MSE: 13.50608<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 451<br />MSE: 12.65634<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 476<br />MSE: 13.36387<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 501<br />MSE: 12.91171<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 526<br />MSE: 12.93785<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 551<br />MSE: 13.02790<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 576<br />MSE: 12.88126<br />predictors: 7<br />predictors: 7<br />predictors: 7\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(165,138,255,1)\",\"dash\":\"longdashdot\"},\"hoveron\":\"points\",\"name\":\"7\",\"legendgroup\":\"7\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476,501,526,551,576],\"y\":[18.7287790026247,13.8109805731246,13.8730105994801,14.9131231682735,14.7543041460434,14.1643088771201,13.8650545046038,14.0235497026901,13.5187483572158,14.5877501213954,14.4669967470558,14.0755375096726,14.1877815448516,14.220098355586,14.4249112372478,14.1968511526347,14.0710383780131,14.1670186876829,13.957019898319,13.9256491173259,14.7120238348951,14.3069928578327,13.7578946432719,14.2319280108093],\"text\":[\"ntree:   1<br />MSE: 18.72878<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree:  26<br />MSE: 13.81098<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree:  51<br />MSE: 13.87301<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree:  76<br />MSE: 14.91312<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 101<br />MSE: 14.75430<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 126<br />MSE: 14.16431<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 151<br />MSE: 13.86505<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 176<br />MSE: 14.02355<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 201<br />MSE: 13.51875<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 226<br />MSE: 14.58775<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 251<br />MSE: 14.46700<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 276<br />MSE: 14.07554<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 301<br />MSE: 14.18778<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 326<br />MSE: 14.22010<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 351<br />MSE: 14.42491<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 376<br />MSE: 14.19685<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 401<br />MSE: 14.07104<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 426<br />MSE: 14.16702<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 451<br />MSE: 13.95702<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 476<br />MSE: 13.92565<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 501<br />MSE: 14.71202<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 526<br />MSE: 14.30699<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 551<br />MSE: 13.75789<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 576<br />MSE: 14.23193<br />predictors: 9<br />predictors: 9<br />predictors: 9\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(251,97,215,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"name\":\"9\",\"legendgroup\":\"9\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":27.8721461187215,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":37.2602739726027},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-27.75,604.75],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"200\",\"400\",\"600\"],\"tickvals\":[3.5527136788005e-15,200,400,600],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"200\",\"400\",\"600\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"ntree\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[9.97563490052224,37.6614598234775],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\"],\"tickvals\":[10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"MSE\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":0.959399606299213},\"annotations\":[{\"text\":\"predictors\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"5560672f4413\":{\"x\":{},\"y\":{},\"colour\":{},\"linetype\":{},\"type\":\"scatter\"}},\"cur_data\":\"5560672f4413\",\"visdat\":{\"5560672f4413\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\r\nWhen starting off with less trees, we see that the MSE’s for model is higher. As we increase trees, the MSE drops significantly for all Models. The model that only uses one predictor seems to have the highest MSE ranging from 30 - 25. The models with 3, 5 and 7 seem to have the lowest amount of MSE. Theres no need to have more than 100 Ntrees to get the lowest MSE.\r\r\n\r\r\nISLR Ch. 8, Exercise 9\r\r\nA\r\r\n\r\r\nCreate a training set containing a random sample of 800 observations, and a test set containing the remaining observations.\r\r\n\r\r\n\r\r\n## OJ dataset\r\r\nlibrary(ISLR2)\r\r\nset.seed(1)\r\r\n\r\r\ntrain <- sample(1:nrow(OJ), 800)\r\r\n#train %>% length()\r\r\nOJ.test <- OJ[-train,]\r\r\nOJ.response <- OJ$Purchase[-train]\r\r\nOJ.train <- OJ[train,]\r\r\nOJ.train.response <- OJ$Purchase[train]\r\r\n\r\r\n\r\r\n#OJ.test %>% dim()\r\r\n#OJ.train %>% dim()\r\r\n#OJ %>% dim()\r\r\n#OJ.response %>% length()\r\r\n#OJ.train.response %>% length()\r\r\n\r\r\n\r\r\n\r\r\nB\r\r\nFit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?\r\r\n\r\r\n\r\r\nlibrary(tree)\r\r\nattach(OJ)\r\r\ntree.oj <- tree(Purchase ~.,data = OJ, subset = train)\r\r\nsummary(tree.oj)\r\r\n\r\r\n\r\r\n\r\r\nClassification tree:\r\r\ntree(formula = Purchase ~ ., data = OJ, subset = train)\r\r\nVariables actually used in tree construction:\r\r\n[1] \"LoyalCH\"       \"PriceDiff\"     \"SpecialCH\"     \"ListPriceDiff\"\r\r\n[5] \"PctDiscMM\"    \r\r\nNumber of terminal nodes:  9 \r\r\nResidual mean deviance:  0.7432 = 587.8 / 791 \r\r\nMisclassification error rate: 0.1588 = 127 / 800 \r\r\n\r\r\nThe training error rate is 16% There are 8 terminal nodes for this tree. The residual mean deviance is 75%.\r\r\nC\r\r\nType in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.\r\r\n\r\r\n\r\r\ntree.oj\r\r\n\r\r\n\r\r\nnode), split, n, deviance, yval, (yprob)\r\r\n      * denotes terminal node\r\r\n\r\r\n 1) root 800 1073.00 CH ( 0.60625 0.39375 )  \r\r\n   2) LoyalCH < 0.5036 365  441.60 MM ( 0.29315 0.70685 )  \r\r\n     4) LoyalCH < 0.280875 177  140.50 MM ( 0.13559 0.86441 )  \r\r\n       8) LoyalCH < 0.0356415 59   10.14 MM ( 0.01695 0.98305 ) *\r\r\n       9) LoyalCH > 0.0356415 118  116.40 MM ( 0.19492 0.80508 ) *\r\r\n     5) LoyalCH > 0.280875 188  258.00 MM ( 0.44149 0.55851 )  \r\r\n      10) PriceDiff < 0.05 79   84.79 MM ( 0.22785 0.77215 )  \r\r\n        20) SpecialCH < 0.5 64   51.98 MM ( 0.14062 0.85938 ) *\r\r\n        21) SpecialCH > 0.5 15   20.19 CH ( 0.60000 0.40000 ) *\r\r\n      11) PriceDiff > 0.05 109  147.00 CH ( 0.59633 0.40367 ) *\r\r\n   3) LoyalCH > 0.5036 435  337.90 CH ( 0.86897 0.13103 )  \r\r\n     6) LoyalCH < 0.764572 174  201.00 CH ( 0.73563 0.26437 )  \r\r\n      12) ListPriceDiff < 0.235 72   99.81 MM ( 0.50000 0.50000 )  \r\r\n        24) PctDiscMM < 0.196197 55   73.14 CH ( 0.61818 0.38182 ) *\r\r\n        25) PctDiscMM > 0.196197 17   12.32 MM ( 0.11765 0.88235 ) *\r\r\n      13) ListPriceDiff > 0.235 102   65.43 CH ( 0.90196 0.09804 ) *\r\r\n     7) LoyalCH > 0.764572 261   91.20 CH ( 0.95785 0.04215 ) *\r\r\n\r\r\nThe terminal nodes are shown by the asterix at the end. Lets look at branch 7. This ends in a terminal node. The split is LoyaCH > .764572. There are 123.80 observations in this branch. The prediction for this branch is CH.\r\r\nD\r\r\nCreate a plot of the tree, and interpret the results.\r\r\n\r\r\n\r\r\nplot(tree.oj)\r\r\ntext(tree.oj, pretty = 0)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nThe criteria most used are loyal CH and price diff.\r\r\ne\r\r\nPredict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?\r\r\n\r\r\n\r\r\ntree.pred <- predict(tree.oj, OJ.test, type = \"class\")\r\r\nx <- table(tree.pred, OJ.response)\r\r\n\r\r\n\r\r\ntable(tree.pred,OJ.response) %>% kable()\r\r\n\r\r\n\r\r\n\r\r\nCH\r\r\nMM\r\r\nCH\r\r\n160\r\r\n38\r\r\nMM\r\r\n8\r\r\n64\r\r\n\r\r\npaste(\"The Test error rate is :\", round(1-(x[1,1]+x[2,2])/sum(x),4),\"%\")\r\r\n\r\r\n\r\r\n[1] \"The Test error rate is : 0.1704 %\"\r\r\n\r\r\nF, G, H\r\r\nApply the cv.tree() function to the training set in order to determine the optimal tree size.\r\r\nProduce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.\r\r\nWhich tree size corresponds to the lowest cross-validated classification error rate?\r\r\n\r\r\n\r\r\ncv.oj <- cv.tree(tree.oj, FUN = prune.misclass )\r\r\n\r\r\n\r\r\npar(mfrow = c(1,2))\r\r\nplot(cv.oj$size, cv.oj$dev, type = \"b\", xlab = \"Number of Trees\", ylab = \"Crossvalidation Error\")\r\r\nplot(cv.oj$k, cv.oj$dev, type = \"b\", xlab = \"Cross complexity Parameter\", ylab = \"Crossvalidation Error\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nAccording to the figure above, the Crossvalidation error was the lowest with the number of trees at 8.\r\r\nI\r\r\nProduce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.\r\r\n\r\r\n\r\r\nprune.oj <- prune.misclass(tree.oj, best = 8)\r\r\nplot(prune.oj)\r\r\ntext(prune.oj, pretty = 0)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nJ\r\r\nCompare the training error rates between the pruned and unpruned trees. Which is higher?\r\r\n\r\r\n\r\r\nprune.oj.pred <- predict(prune.oj, OJ.train, type = \"class\")\r\r\nprune.oj.pred %>% length()\r\r\n\r\r\n\r\r\n[1] 800\r\r\n\r\r\nOJ.train.response %>% length()\r\r\n\r\r\n\r\r\n[1] 800\r\r\n\r\r\nx<- table(prune.oj.pred,OJ.train.response)\r\r\n\r\r\n\r\r\n\r\r\n table(prune.oj.pred,OJ.train.response)%>% kable()\r\r\n\r\r\n\r\r\n\r\r\nCH\r\r\nMM\r\r\nCH\r\r\n450\r\r\n92\r\r\nMM\r\r\n35\r\r\n223\r\r\n\r\r\npaste(\"The Test error rate is :\", round(1-(x[1,1]+x[2,2])/sum(x),3),\"%\")\r\r\n\r\r\n\r\r\n[1] \"The Test error rate is : 0.159 %\"\r\r\n\r\r\nThe training error rate is slightly less at 15% compared to the 16 % shown in the original training set.\r\r\nK\r\r\nCompare the test error rates between the pruned and unpruned trees. Which is higher?\r\r\n\r\r\n\r\r\nprune.test.pred <- predict(prune.oj, OJ.test, type = \"class\")\r\r\n\r\r\nx<- table(prune.test.pred,OJ.response)\r\r\n\r\r\n\r\r\n\r\r\n table(prune.test.pred,OJ.response)%>% kable()\r\r\n\r\r\n\r\r\n\r\r\nCH\r\r\nMM\r\r\nCH\r\r\n160\r\r\n38\r\r\nMM\r\r\n8\r\r\n64\r\r\n\r\r\npaste(\"The Test error rate is :\", round(1-(x[1,1]+x[2,2])/sum(x),4),\"%\")\r\r\n\r\r\n\r\r\n[1] \"The Test error rate is : 0.1704 %\"\r\r\n\r\r\nThe test error from the pruned tree is the same as the unpruned tree. We only removed one branch from the original 9 and the CV error rates looked pretty similar for both.\r\r\n\r\r\nISLR Ch. 8, Exercise 10\r\r\nWe now use boosting to predict Salary in the Hitters data set.\r\r\nA\r\r\nRemove the observations for whom the salary information is unknown, and then log-transform the salaries.\r\r\n\r\r\n\r\r\nHitters %>% filter(is.na(Hitters$Salary)) %>% count()\r\r\n\r\r\n\r\r\n   n\r\r\n1 59\r\r\n\r\r\nHitters %>% dim()\r\r\n\r\r\n\r\r\n[1] 322  20\r\r\n\r\r\nHitters_new <- Hitters %>% filter(!is.na(Hitters$Salary))\r\r\nHitters_new %>% dim()\r\r\n\r\r\n\r\r\n[1] 263  20\r\r\n\r\r\nUsing !is.na(), I filtered the dataset to filter out all rows with an NA salary.\r\r\nNext we will log transform the Salaries\r\r\n\r\r\n\r\r\nHitters_new$Salary <- log(Hitters_new$Salary)\r\r\n\r\r\n\r\r\n\r\r\nB\r\r\nCreate a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.\r\r\n\r\r\n\r\r\nset.seed(10)\r\r\n\r\r\ntrain <- sample(1:nrow(Hitters_new), 200)\r\r\n#train %>% length()\r\r\n\r\r\nHitters.test <- Hitters_new[-train,]\r\r\nHitters.response <- Hitters_new$Salary[-train]\r\r\nHitters.train <- Hitters_new[train,]\r\r\nHitters.train.response <- Hitters_new$Salary[train]\r\r\n\r\r\n\r\r\n#Hitters.test %>% dim()\r\r\n#Hitters_new %>% dim()\r\r\n#Hitters.train.response%>% length()\r\r\n#Hitters.response %>% length()\r\r\n\r\r\n\r\r\n\r\r\nC\r\r\nPerform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter λ. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.\r\r\n\r\r\n\r\r\nlibrary(gbm)\r\r\n\r\r\n## Example Boost function, summary, and way to calculate MSE\r\r\nboost.Hitters <- gbm(Salary~.,data = Hitters_new[train,],\r\r\n                     distribution = \"gaussian\", n.trees = 1000,\r\r\n                     interaction.depth = 4, shrinkage = .001)\r\r\n\r\r\nsummary(boost.Hitters)\r\r\n\r\r\n\r\r\n\r\r\n                var     rel.inf\r\r\nCAtBat       CAtBat 28.48705371\r\r\nCRuns         CRuns 26.44960088\r\r\nCHits         CHits 11.53406819\r\r\nCRBI           CRBI  9.37181852\r\r\nCWalks       CWalks  6.10917041\r\r\nHits           Hits  3.05905465\r\r\nCHmRun       CHmRun  2.19862661\r\r\nWalks         Walks  2.17115968\r\r\nHmRun         HmRun  2.01444003\r\r\nAtBat         AtBat  1.94060681\r\r\nYears         Years  1.86440763\r\r\nRBI             RBI  1.73382350\r\r\nPutOuts     PutOuts  1.62680896\r\r\nRuns           Runs  0.89988120\r\r\nErrors       Errors  0.16759670\r\r\nAssists     Assists  0.12090202\r\r\nDivision   Division  0.11322110\r\r\nNewLeague NewLeague  0.08105077\r\r\nLeague       League  0.05670863\r\r\n\r\r\nyhat.boost <- predict(boost.Hitters, newdata = Hitters_new[-train,], n.trees = 1000)\r\r\nyhat.boost %>% length()\r\r\n\r\r\n\r\r\n[1] 63\r\r\n\r\r\npaste(\"The MSE test error rate is\",mean((yhat.boost- Hitters.response)^2))\r\r\n\r\r\n\r\r\n[1] \"The MSE test error rate is 0.420831510594534\"\r\r\n\r\r\nUsing the method above, I can now calculate the MSE using the boost model, yhat predictions and the actual results on the training set. Next I will create a loop that will run through multiple values of shrinage and add the calculated MSE’s to a dataframe. I will then plot those values.\r\r\n\r\r\n\r\r\nboost_df <- tibble(\"Shrinkage\" = double(), \"MSE\" = double())\r\r\n\r\r\n\r\r\nfor(i in seq(.0001, .25,.01)){\r\r\n  \r\r\n  boost.Hitters <- gbm(Salary~.,data = Hitters_new[train,],\r\r\n                     distribution = \"gaussian\", n.trees = 1000,\r\r\n                     interaction.depth = 4, shrinkage = i)\r\r\n  yhat.boost <- predict(boost.Hitters, newdata = Hitters_new[train,], n.trees = 1000)\r\r\nmse_1 <- mean((yhat.boost- Hitters.train.response)^2)\r\r\n\r\r\n\r\r\nboost_df <- boost_df %>% add_row(Shrinkage = i, MSE = mse_1)\r\r\n  \r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nplot<- boost_df %>% ggplot()+geom_line(aes(x = Shrinkage, y = MSE))+xlab(\"Shrinkage (lambda)\")\r\r\n\r\r\nplotly::ggplotly(plot)\r\r\n\r\r\n\r\r\n\r\r\n{\"x\":{\"data\":[{\"x\":[0.0001,0.0101,0.0201,0.0301,0.0401,0.0501,0.0601,0.0701,0.0801,0.0901,0.1001,0.1101,0.1201,0.1301,0.1401,0.1501,0.1601,0.1701,0.1801,0.1901,0.2001,0.2101,0.2201,0.2301,0.2401],\"y\":[0.665861327231243,0.0379234798261602,0.0144307205324894,0.0072596447854902,0.00344975971057255,0.00168453714329211,0.000835282591719583,0.000417665506786859,0.000285515859136994,0.000131718353511751,9.27714113673313e-05,4.16776791624866e-05,2.44783876823639e-05,1.1576696795739e-05,8.58353457180432e-06,3.28766728356092e-06,2.91449594811509e-06,1.37631946441906e-06,8.32728768275394e-07,5.78267535350603e-07,3.53992245551131e-07,1.19388320961587e-07,8.84088622687979e-08,7.50714592385369e-08,1.44906879843596e-08],\"text\":[\"Shrinkage: 0.0001<br />MSE: 6.658613e-01\",\"Shrinkage: 0.0101<br />MSE: 3.792348e-02\",\"Shrinkage: 0.0201<br />MSE: 1.443072e-02\",\"Shrinkage: 0.0301<br />MSE: 7.259645e-03\",\"Shrinkage: 0.0401<br />MSE: 3.449760e-03\",\"Shrinkage: 0.0501<br />MSE: 1.684537e-03\",\"Shrinkage: 0.0601<br />MSE: 8.352826e-04\",\"Shrinkage: 0.0701<br />MSE: 4.176655e-04\",\"Shrinkage: 0.0801<br />MSE: 2.855159e-04\",\"Shrinkage: 0.0901<br />MSE: 1.317184e-04\",\"Shrinkage: 0.1001<br />MSE: 9.277141e-05\",\"Shrinkage: 0.1101<br />MSE: 4.167768e-05\",\"Shrinkage: 0.1201<br />MSE: 2.447839e-05\",\"Shrinkage: 0.1301<br />MSE: 1.157670e-05\",\"Shrinkage: 0.1401<br />MSE: 8.583535e-06\",\"Shrinkage: 0.1501<br />MSE: 3.287667e-06\",\"Shrinkage: 0.1601<br />MSE: 2.914496e-06\",\"Shrinkage: 0.1701<br />MSE: 1.376319e-06\",\"Shrinkage: 0.1801<br />MSE: 8.327288e-07\",\"Shrinkage: 0.1901<br />MSE: 5.782675e-07\",\"Shrinkage: 0.2001<br />MSE: 3.539922e-07\",\"Shrinkage: 0.2101<br />MSE: 1.193883e-07\",\"Shrinkage: 0.2201<br />MSE: 8.840886e-08\",\"Shrinkage: 0.2301<br />MSE: 7.507146e-08\",\"Shrinkage: 0.2401<br />MSE: 1.449069e-08\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":27.8721461187215,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.0119,0.2521],\"tickmode\":\"array\",\"ticktext\":[\"0.00\",\"0.05\",\"0.10\",\"0.15\",\"0.20\",\"0.25\"],\"tickvals\":[0,0.05,0.1,0.15,0.2,0.25],\"categoryorder\":\"array\",\"categoryarray\":[\"0.00\",\"0.05\",\"0.10\",\"0.15\",\"0.20\",\"0.25\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Shrinkage (lambda)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.0332930511463398,0.699154392868271],\"tickmode\":\"array\",\"ticktext\":[\"0.0\",\"0.2\",\"0.4\",\"0.6\"],\"tickvals\":[0,0.2,0.4,0.6],\"categoryorder\":\"array\",\"categoryarray\":[\"0.0\",\"0.2\",\"0.4\",\"0.6\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"MSE\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"55606a0e591d\":{\"x\":{},\"y\":{},\"type\":\"scatter\"}},\"cur_data\":\"55606a0e591d\",\"visdat\":{\"55606a0e591d\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\r\nD\r\r\nProduce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.\r\r\nFor this, I will do the exact steps as done in part C, but I will use the test set instead of the training set when making predicitons.\r\r\n\r\r\n\r\r\nboost_df_test <- tibble(\"Shrinkage\" = double(), \"MSE\" = double())\r\r\n\r\r\n\r\r\nfor(i in seq(.0001, .25,.01)){\r\r\n  \r\r\n  boost.Hitters <- gbm(Salary~.,data = Hitters_new[train,],\r\r\n                     distribution = \"gaussian\", n.trees = 1000,\r\r\n                     interaction.depth = 4, shrinkage = i)\r\r\n  yhat.boost <- predict(boost.Hitters, newdata = Hitters_new[-train,], n.trees = 1000)\r\r\nmse_1 <- mean((yhat.boost- Hitters.test$Salary)^2)\r\r\n\r\r\nboost_df_test <- boost_df_test %>% add_row(Shrinkage = i, MSE = mse_1)\r\r\n  \r\r\n}\r\r\n\r\r\n\r\r\nplot<- boost_df_test %>% ggplot()+geom_line(aes(x = Shrinkage, y = MSE))+xlab(\"Shrinkage (lambda)\")\r\r\n\r\r\nplotly::ggplotly(plot)\r\r\n\r\r\n\r\r\n\r\r\n{\"x\":{\"data\":[{\"x\":[0.0001,0.0101,0.0201,0.0301,0.0401,0.0501,0.0601,0.0701,0.0801,0.0901,0.1001,0.1101,0.1201,0.1301,0.1401,0.1501,0.1601,0.1701,0.1801,0.1901,0.2001,0.2101,0.2201,0.2301,0.2401],\"y\":[0.776404949431054,0.377468213068746,0.389109787753448,0.397285920230495,0.398521711253916,0.392998980345278,0.411981165955552,0.411056091167511,0.40978317154764,0.424797677427592,0.424781638545013,0.403721699223818,0.442243230922348,0.411557203950517,0.434448046097555,0.436631619422199,0.415804628242507,0.442882398988851,0.442741487743061,0.441862464416142,0.398775869459989,0.447842056881034,0.424421198723408,0.409074194786413,0.448232315646583],\"text\":[\"Shrinkage: 0.0001<br />MSE: 0.7764049\",\"Shrinkage: 0.0101<br />MSE: 0.3774682\",\"Shrinkage: 0.0201<br />MSE: 0.3891098\",\"Shrinkage: 0.0301<br />MSE: 0.3972859\",\"Shrinkage: 0.0401<br />MSE: 0.3985217\",\"Shrinkage: 0.0501<br />MSE: 0.3929990\",\"Shrinkage: 0.0601<br />MSE: 0.4119812\",\"Shrinkage: 0.0701<br />MSE: 0.4110561\",\"Shrinkage: 0.0801<br />MSE: 0.4097832\",\"Shrinkage: 0.0901<br />MSE: 0.4247977\",\"Shrinkage: 0.1001<br />MSE: 0.4247816\",\"Shrinkage: 0.1101<br />MSE: 0.4037217\",\"Shrinkage: 0.1201<br />MSE: 0.4422432\",\"Shrinkage: 0.1301<br />MSE: 0.4115572\",\"Shrinkage: 0.1401<br />MSE: 0.4344480\",\"Shrinkage: 0.1501<br />MSE: 0.4366316\",\"Shrinkage: 0.1601<br />MSE: 0.4158046\",\"Shrinkage: 0.1701<br />MSE: 0.4428824\",\"Shrinkage: 0.1801<br />MSE: 0.4427415\",\"Shrinkage: 0.1901<br />MSE: 0.4418625\",\"Shrinkage: 0.2001<br />MSE: 0.3987759\",\"Shrinkage: 0.2101<br />MSE: 0.4478421\",\"Shrinkage: 0.2201<br />MSE: 0.4244212\",\"Shrinkage: 0.2301<br />MSE: 0.4090742\",\"Shrinkage: 0.2401<br />MSE: 0.4482323\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":27.8721461187215,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.0119,0.2521],\"tickmode\":\"array\",\"ticktext\":[\"0.00\",\"0.05\",\"0.10\",\"0.15\",\"0.20\",\"0.25\"],\"tickvals\":[0,0.05,0.1,0.15,0.2,0.25],\"categoryorder\":\"array\",\"categoryarray\":[\"0.00\",\"0.05\",\"0.10\",\"0.15\",\"0.20\",\"0.25\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Shrinkage (lambda)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.357521376250631,0.796351786249169],\"tickmode\":\"array\",\"ticktext\":[\"0.4\",\"0.5\",\"0.6\",\"0.7\"],\"tickvals\":[0.4,0.5,0.6,0.7],\"categoryorder\":\"array\",\"categoryarray\":[\"0.4\",\"0.5\",\"0.6\",\"0.7\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"MSE\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"55606cbe6c3b\":{\"x\":{},\"y\":{},\"type\":\"scatter\"}},\"cur_data\":\"55606cbe6c3b\",\"visdat\":{\"55606cbe6c3b\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\r\nboost_df_test %>% summarise(min(MSE))\r\r\n\r\r\n\r\r\n# A tibble: 1 x 1\r\r\n  `min(MSE)`\r\r\n       <dbl>\r\r\n1      0.377\r\r\n\r\r\nwhich.min(boost_df_test$MSE)\r\r\n\r\r\n\r\r\n[1] 2\r\r\n\r\r\nboost_df_test[which.min(boost_df_test$MSE),]\r\r\n\r\r\n\r\r\n# A tibble: 1 x 2\r\r\n  Shrinkage   MSE\r\r\n      <dbl> <dbl>\r\r\n1    0.0101 0.377\r\r\n\r\r\nMin MSE can be found at .0101 Lamda.\r\r\nE\r\r\nCompare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.\r\r\nFor this I decided to use multiple models from ch. 3 and ch. 6. See below:\r\r\n\r\r\n\r\r\nlibrary(ggplot2)\r\r\n\r\r\n## Linear Model\r\r\n\r\r\nlm.fit <- lm(Salary ~., data = Hitters_new, subset = train)\r\r\nlm.pred <- predict(lm.fit, Hitters_new[-train,], type = \"response\")\r\r\nlm_mse <- mean((lm.pred - Hitters.response)^2)\r\r\npaste(\"The Linear Model MSE is: \",lm_mse)\r\r\n\r\r\n\r\r\n[1] \"The Linear Model MSE is:  0.576163818320103\"\r\r\n\r\r\nMSE_dataframe <- data.frame(\"model\" = \"Least Square\", \"MSE\" = lm_mse)\r\r\n\r\r\n\r\r\n\r\r\n## Boost\r\r\n boost.Hitters <- gbm(Salary~.,data = Hitters_new[train,],\r\r\n                     distribution = \"gaussian\", n.trees = 1000,\r\r\n                     interaction.depth = 4, shrinkage = .0101)\r\r\n  yhat.boost <- predict(boost.Hitters, newdata = Hitters_new[-train,], n.trees = 1000)\r\r\nmse_1 <- mean((yhat.boost- Hitters.response)^2)\r\r\n\r\r\n\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"Boosting\",mse_1))\r\r\n\r\r\npaste(\"The test Error Rate for Boosting is : \", mean((yhat.boost - Hitters.response)^2))\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate for Boosting is :  0.382735329466136\"\r\r\n\r\r\n## Ridge\r\r\nset.seed(10)\r\r\nx <- model.matrix(Salary~.,Hitters_new)[,-1]\r\r\ny <- Hitters_new$Salary\r\r\ngrid <- 10^seq(10,-2, length = 100)\r\r\n\r\r\nridge.mod <- glmnet(x[train,], y[train], alpha =0, lambda = grid )\r\r\ncv.out <- cv.glmnet(x[train,], y[train],alpha = 0)\r\r\nlamda_best <- cv.out$lambda.min\r\r\nridge.pred <- predict(ridge.mod, s = lamda_best, newx = x[-train,])\r\r\nridge_mse <- mean((ridge.pred - y[-train])^2)\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"ridge\",ridge_mse))\r\r\n\r\r\npaste(\"The test Error Rate for Ridge is : \", mean((ridge.pred - y[-train])^2))\r\r\n\r\r\n\r\r\n[1] \"The test Error Rate for Ridge is :  0.581396338527786\"\r\r\n\r\r\n## Lasso\r\r\n\r\r\nlasso.mod <- glmnet(x[train,],y[train],alpha = 1, lambda = grid)\r\r\ncv.out <- cv.glmnet(x[train,], y[train], alpha = 1)\r\r\nlamda_best <- cv.out$lambda.min\r\r\nlasso.pred <- predict(lasso.mod, s= lamda_best, newx = x[-train,])\r\r\npaste(\"The test error rate for lasso \",mean((lasso.pred - y[-train])^2))\r\r\n\r\r\n\r\r\n[1] \"The test error rate for lasso  0.575921346257875\"\r\r\n\r\r\nlasso_mse <- mean((lasso.pred - y[-train])^2)\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"Lasso\",lasso_mse))\r\r\n\r\r\n\r\r\n\r\r\n## PCR\r\r\n\r\r\npcr.fit <- pcr(Salary ~ . , data = Hitters_new, subset = train, scale = TRUE, validation = \"CV\")\r\r\nvalidationplot(pcr.fit,val.type = \"MSEP\")\r\r\n\r\r\n\r\r\n\r\r\npcr.pred <- predict(pcr.fit,Hitters.test, ncomp = 10)\r\r\npcr_mse<- mean((pcr.pred - Hitters.response)^2)\r\r\n# lowest MSEP at 10 componenets.\r\r\npaste(\"The MSE error for PCR is \", pcr_mse)\r\r\n\r\r\n\r\r\n[1] \"The MSE error for PCR is  0.619877318874639\"\r\r\n\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"PCR\",pcr_mse))\r\r\n\r\r\n\r\r\n\r\r\n## PLS\r\r\n\r\r\npls.fit <- plsr(Salary~., data = Hitters_new, subset = train, scale = TRUE, validation = \"CV\")\r\r\nvalidationplot(pls.fit, val.type = \"MSEP\")\r\r\n\r\r\n\r\r\n\r\r\n# Lowest  MSEP at 5 components\r\r\npls.pred_7<- predict(pls.fit, Hitters.test, ncomp = 5)\r\r\npaste(\"The MSE error using M as 5: \", mean((pls.pred_7- Hitters.response)^2))\r\r\n\r\r\n\r\r\n[1] \"The MSE error using M as 5:  0.617966162736438\"\r\r\n\r\r\npls_mse <- mean((pls.pred_7- Hitters.response)^2)\r\r\nMSE_dataframe <- rbind(MSE_dataframe, c(\"PLS\",pls_mse))\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n#MSE_dataframe %>% arrange(MSE)\r\r\nMSE_dataframe$MSE <- as.numeric(MSE_dataframe$MSE)\r\r\n\r\r\nMSE_dataframe %>% ggplot2::ggplot(aes(x = model, y = MSE)) + geom_col(fill = \"deepskyblue3\")+theme(axis.text.y = element_blank())+labs(title = \"MSE per Model\")+geom_text(aes(label = round(as.numeric(MSE),3)), vjust = 2)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nBoosting the model will give us the lowest MSE, with PCR in second place. Boosting is the model of choice by far compared to the other models.\r\r\nF\r\r\nWhich variables appear to be the most important predictors in the boosted model?\r\r\n\r\r\n\r\r\nsummary(boost.Hitters)\r\r\n\r\r\n\r\r\n\r\r\n                var    rel.inf\r\r\nCAtBat       CAtBat 21.3776399\r\r\nCRuns         CRuns 18.4554725\r\r\nCHits         CHits  9.4230585\r\r\nCRBI           CRBI  9.1617592\r\r\nCWalks       CWalks  7.1643822\r\r\nYears         Years  5.2568153\r\r\nCHmRun       CHmRun  4.2210637\r\r\nWalks         Walks  3.0531623\r\r\nPutOuts     PutOuts  3.0017692\r\r\nAtBat         AtBat  2.9358935\r\r\nHits           Hits  2.8943762\r\r\nErrors       Errors  2.7801941\r\r\nRBI             RBI  2.7298938\r\r\nHmRun         HmRun  2.5411422\r\r\nRuns           Runs  2.1241977\r\r\nAssists     Assists  1.5876161\r\r\nDivision   Division  0.4762580\r\r\nNewLeague NewLeague  0.4306447\r\r\nLeague       League  0.3846610\r\r\n\r\r\nAt bat and runs have the most influence.\r\r\nG\r\r\nNow apply bagging to the training set. What is the test set MSE for this approach?\r\r\n\r\r\n\r\r\nset.seed(1)\r\r\n\r\r\n\r\r\nbag_data <- tibble(\"predictors\" = c(),\"ntree\" = double(), \"MSE\" = double())\r\r\n\r\r\n\r\r\n\r\r\nfor(i in seq(1, 19,2)){\r\r\n  for(x in seq(1, 600,100)){\r\r\n  bag.hitters <- randomForest(Salary ~., data = Hitters.train, mtry = i,ntree = x)\r\r\nyhat.bag <- predict(bag.hitters, newdata = Hitters.test)\r\r\nmse_1 <- (mean((yhat.bag- Hitters.response)^2))\r\r\nbag_data <-bag_data %>% add_row(predictors = as.character(i),ntree = x, MSE = mse_1)\r\r\n  }\r\r\n}\r\r\n\r\r\nplot<- bag_data %>% ggplot()+geom_line(aes(x = ntree, y = MSE, group = predictors,color = predictors, linetype = predictors))\r\r\n\r\r\nplotly::ggplotly(plot)\r\r\n\r\r\n\r\r\n\r\r\n{\"x\":{\"data\":[{\"x\":[1,101,201,301,401,501],\"y\":[0.43171395670545,0.406888206779161,0.38835026613887,0.394957294443029,0.391269651648264,0.39293152140034],\"text\":[\"ntree:   1<br />MSE: 0.4317140<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 101<br />MSE: 0.4068882<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 201<br />MSE: 0.3883503<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 301<br />MSE: 0.3949573<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 401<br />MSE: 0.3912697<br />predictors: 1<br />predictors: 1<br />predictors: 1\",\"ntree: 501<br />MSE: 0.3929315<br />predictors: 1<br />predictors: 1<br />predictors: 1\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"1\",\"legendgroup\":\"1\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.661173726164936,0.397066154375408,0.362048665909562,0.358341413030472,0.367815443344352,0.363345651573596],\"text\":[\"ntree:   1<br />MSE: 0.6611737<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 101<br />MSE: 0.3970662<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 201<br />MSE: 0.3620487<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 301<br />MSE: 0.3583414<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 401<br />MSE: 0.3678154<br />predictors: 11<br />predictors: 11<br />predictors: 11\",\"ntree: 501<br />MSE: 0.3633457<br />predictors: 11<br />predictors: 11<br />predictors: 11\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(216,144,0,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"name\":\"11\",\"legendgroup\":\"11\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.658288916933541,0.347060609906192,0.364754790637257,0.359946094273967,0.361640653629599,0.364537790832377],\"text\":[\"ntree:   1<br />MSE: 0.6582889<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 101<br />MSE: 0.3470606<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 201<br />MSE: 0.3647548<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 301<br />MSE: 0.3599461<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 401<br />MSE: 0.3616407<br />predictors: 13<br />predictors: 13<br />predictors: 13\",\"ntree: 501<br />MSE: 0.3645378<br />predictors: 13<br />predictors: 13<br />predictors: 13\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(163,165,0,1)\",\"dash\":\"dot\"},\"hoveron\":\"points\",\"name\":\"13\",\"legendgroup\":\"13\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.49514177324075,0.370200746381667,0.363540704420413,0.361785301393901,0.360282087622746,0.368491273097209],\"text\":[\"ntree:   1<br />MSE: 0.4951418<br />predictors: 15<br />predictors: 15<br />predictors: 15\",\"ntree: 101<br />MSE: 0.3702007<br />predictors: 15<br />predictors: 15<br />predictors: 15\",\"ntree: 201<br />MSE: 0.3635407<br />predictors: 15<br />predictors: 15<br />predictors: 15\",\"ntree: 301<br />MSE: 0.3617853<br />predictors: 15<br />predictors: 15<br />predictors: 15\",\"ntree: 401<br />MSE: 0.3602821<br />predictors: 15<br />predictors: 15<br />predictors: 15\",\"ntree: 501<br />MSE: 0.3684913<br />predictors: 15<br />predictors: 15<br />predictors: 15\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(57,182,0,1)\",\"dash\":\"dashdot\"},\"hoveron\":\"points\",\"name\":\"15\",\"legendgroup\":\"15\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.526273744475996,0.35950692757155,0.359311412713107,0.369429456885847,0.365785126558743,0.360859328139336],\"text\":[\"ntree:   1<br />MSE: 0.5262737<br />predictors: 17<br />predictors: 17<br />predictors: 17\",\"ntree: 101<br />MSE: 0.3595069<br />predictors: 17<br />predictors: 17<br />predictors: 17\",\"ntree: 201<br />MSE: 0.3593114<br />predictors: 17<br />predictors: 17<br />predictors: 17\",\"ntree: 301<br />MSE: 0.3694295<br />predictors: 17<br />predictors: 17<br />predictors: 17\",\"ntree: 401<br />MSE: 0.3657851<br />predictors: 17<br />predictors: 17<br />predictors: 17\",\"ntree: 501<br />MSE: 0.3608593<br />predictors: 17<br />predictors: 17<br />predictors: 17\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,125,1)\",\"dash\":\"longdash\"},\"hoveron\":\"points\",\"name\":\"17\",\"legendgroup\":\"17\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.481039970852711,0.379086606019541,0.368019988076372,0.359608427537105,0.372666813244621,0.370620367912294],\"text\":[\"ntree:   1<br />MSE: 0.4810400<br />predictors: 19<br />predictors: 19<br />predictors: 19\",\"ntree: 101<br />MSE: 0.3790866<br />predictors: 19<br />predictors: 19<br />predictors: 19\",\"ntree: 201<br />MSE: 0.3680200<br />predictors: 19<br />predictors: 19<br />predictors: 19\",\"ntree: 301<br />MSE: 0.3596084<br />predictors: 19<br />predictors: 19<br />predictors: 19\",\"ntree: 401<br />MSE: 0.3726668<br />predictors: 19<br />predictors: 19<br />predictors: 19\",\"ntree: 501<br />MSE: 0.3706204<br />predictors: 19<br />predictors: 19<br />predictors: 19\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,196,1)\",\"dash\":\"longdashdot\"},\"hoveron\":\"points\",\"name\":\"19\",\"legendgroup\":\"19\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.61417411004752,0.372236226979121,0.368948611943404,0.362922284804175,0.358775690435265,0.370532894636705],\"text\":[\"ntree:   1<br />MSE: 0.6141741<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 101<br />MSE: 0.3722362<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 201<br />MSE: 0.3689486<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 301<br />MSE: 0.3629223<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 401<br />MSE: 0.3587757<br />predictors: 3<br />predictors: 3<br />predictors: 3\",\"ntree: 501<br />MSE: 0.3705329<br />predictors: 3<br />predictors: 3<br />predictors: 3\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,176,246,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"name\":\"3\",\"legendgroup\":\"3\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.868157355329765,0.367166134779675,0.36291527410717,0.367454466896753,0.362269143984216,0.354995554790265],\"text\":[\"ntree:   1<br />MSE: 0.8681574<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 101<br />MSE: 0.3671661<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 201<br />MSE: 0.3629153<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 301<br />MSE: 0.3674545<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 401<br />MSE: 0.3622691<br />predictors: 5<br />predictors: 5<br />predictors: 5\",\"ntree: 501<br />MSE: 0.3549956<br />predictors: 5<br />predictors: 5<br />predictors: 5\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(149,144,255,1)\",\"dash\":\"dashdot\"},\"hoveron\":\"points\",\"name\":\"5\",\"legendgroup\":\"5\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.963598359013147,0.35257182218199,0.355642502239179,0.350693194525137,0.363054250887282,0.368009632771206],\"text\":[\"ntree:   1<br />MSE: 0.9635984<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 101<br />MSE: 0.3525718<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 201<br />MSE: 0.3556425<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 301<br />MSE: 0.3506932<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 401<br />MSE: 0.3630543<br />predictors: 7<br />predictors: 7<br />predictors: 7\",\"ntree: 501<br />MSE: 0.3680096<br />predictors: 7<br />predictors: 7<br />predictors: 7\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(231,107,243,1)\",\"dash\":\"dashdot\"},\"hoveron\":\"points\",\"name\":\"7\",\"legendgroup\":\"7\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,101,201,301,401,501],\"y\":[0.507282568957107,0.359309921576958,0.361063111593749,0.359500087972263,0.360198260789674,0.354346776110049],\"text\":[\"ntree:   1<br />MSE: 0.5072826<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 101<br />MSE: 0.3593099<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 201<br />MSE: 0.3610631<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 301<br />MSE: 0.3595001<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 401<br />MSE: 0.3601983<br />predictors: 9<br />predictors: 9<br />predictors: 9\",\"ntree: 501<br />MSE: 0.3543468<br />predictors: 9<br />predictors: 9<br />predictors: 9\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(255,98,188,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"name\":\"9\",\"legendgroup\":\"9\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":27.8721461187215,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-24,526],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"100\",\"200\",\"300\",\"400\",\"500\"],\"tickvals\":[3.5527136788005e-15,100,200,300,400,500],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"100\",\"200\",\"300\",\"400\",\"500\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"ntree\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.316233722450844,0.994425246468495],\"tickmode\":\"array\",\"ticktext\":[\"0.4\",\"0.6\",\"0.8\"],\"tickvals\":[0.4,0.6,0.8],\"categoryorder\":\"array\",\"categoryarray\":[\"0.4\",\"0.6\",\"0.8\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"MSE\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":0.959399606299213},\"annotations\":[{\"text\":\"predictors\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"55603f4359ff\":{\"x\":{},\"y\":{},\"colour\":{},\"linetype\":{},\"type\":\"scatter\"}},\"cur_data\":\"55603f4359ff\",\"visdat\":{\"55603f4359ff\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\r\nBased on the plot, we should use 13 predictors and 100 trees.\r\r\n\r\r\n\r\r\n bag.hitters <- randomForest(Salary ~., data = Hitters.train, mtry = 13,ntree = 100)\r\r\nyhat.bag <- predict(bag.hitters, newdata = Hitters.test)\r\r\npaste(\"The MSE for bagging is : \",(mean((yhat.bag- Hitters.response)^2)))\r\r\n\r\r\n\r\r\n[1] \"The MSE for bagging is :  0.362244453542562\"\r\r\n\r\r\nThe .3622 is slightly lower than the boosting MSE of .38.\r\r\nFinal Project IDEAS\r\r\nIdentify a data set that you plan to use for your project/poster and your likely collaborators. What outcome of interest do will you attempt to predict? Why do you expect that the available features (variables), or some subset of them, should help predict this outcome?\r\r\nFor the project, I plan to work alone. If I do stumble on certain problems, I will reach out via Piazza or I will go in for office hours.\r\r\nFor my dataset I went to Kaggle.com. I’m not entirely sure what data set I will use. I found the following data sets that I could possibly use:\r\r\nFor a descrete analysis I could use the Amazon seller dataset from Kagle. This data set is trying to predict whether an amazon order will go through. https://www.kaggle.com/pranalibose/amazon-seller-order-status-prediction This data set was created with the purpose of predicting order sucesses.\r\r\nI’d also like to possibly use stock data and indicators to predict whether a stock will increase or decrease, and to predict future prices. I know that a lot of people use technical indicators to make buy and sell decisions. I’d like to give that an attempt as well.\r\r\nI also found another data set that was interesting from Kagle. This data set looks at wind power. This data set is trying to predict how much wind is generated by the windmill in the following 15 days. https://www.kaggle.com/theforcecoder/wind-power-forecasting\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": {},
    "last_modified": "2022-03-13T19:10:33-04:00",
    "input_file": {}
  },
  {
    "path": "posts/monk-monastery-network/",
    "title": "Monk Monastery Network",
    "description": "A closer look into Samson dataset",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-17",
    "categories": [],
    "contents": "\r\r\nIntroduction to Dataset\r\r\nThe following data set looks at social interactions amoung monks. This data was recorded in 1969. During Sampsons stay at a monestary, 4 monks were expelled from the monestary. Sampson interviewed the monks about positive relations between monks. Each monk ranked their top three choices. A tie is represented by monks liking another monk. The data was gathered three times to see if group sentiment changed over time.\r\r\nDescribe the Network\r\r\n\r\r\n\r\r\nls()\r\r\n\r\r\n\r\r\n[1] \"network_igraph\"  \"network_nodes\"   \"network_statnet\"\r\r\n\r\r\nprint(network_statnet)\r\r\n\r\r\n\r\r\n Network attributes:\r\r\n  vertices = 18 \r\r\n  directed = TRUE \r\r\n  hyper = FALSE \r\r\n  loops = FALSE \r\r\n  multiple = FALSE \r\r\n  total edges= 88 \r\r\n    missing edges= 0 \r\r\n    non-missing edges= 88 \r\r\n\r\r\n Vertex attribute names: \r\r\n    cloisterville group vertex.names \r\r\n\r\r\n Edge attribute names: \r\r\n    nominations \r\r\n\r\r\nvcount(network_igraph)\r\r\n\r\r\n\r\r\n[1] 18\r\r\n\r\r\necount(network_igraph)\r\r\n\r\r\n\r\r\n[1] 88\r\r\n\r\r\nBoth the igraph and statnet datasets are showing the same number of nodes and edges. 18 nodes and 88 edges.\r\r\n\r\r\n\r\r\nis_bipartite(network_igraph)\r\r\n\r\r\n\r\r\n[1] FALSE\r\r\n\r\r\nis_directed(network_igraph)\r\r\n\r\r\n\r\r\n[1] TRUE\r\r\n\r\r\nis_weighted(network_igraph)\r\r\n\r\r\n\r\r\n[1] FALSE\r\r\n\r\r\nV(network_igraph)$color = \"yellow\"\r\r\n\r\r\n\r\r\nplot(\r\r\n  network_igraph,\r\r\n  edge.arrow.mode = .9,\r\r\n  vertex.label.color = \"black\",\r\r\n  vertex.shape = \"square\",\r\r\n  vertex.label.font = 3,\r\r\n  main = \"Monastery Monks\"\r\r\n  \r\r\n)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nWe have a single mode, directed, and unweighted dataset.\r\r\nLooking at the Attributes\r\r\n\r\r\n\r\r\nigraph::vertex_attr_names(network_igraph)\r\r\n\r\r\n\r\r\n[1] \"cloisterville\" \"group\"         \"na\"            \"vertex.names\" \r\r\n[5] \"color\"        \r\r\n\r\r\nnetwork::list.vertex.attributes(network_statnet)\r\r\n\r\r\n\r\r\n[1] \"cloisterville\" \"group\"         \"na\"            \"vertex.names\" \r\r\n\r\r\nigraph::edge_attr_names(network_igraph)\r\r\n\r\r\n\r\r\n[1] \"na\"          \"nominations\"\r\r\n\r\r\nnetwork::list.edge.attributes(network_statnet)\r\r\n\r\r\n\r\r\n[1] \"na\"          \"nominations\"\r\r\n\r\r\nBoth statnet and igraph are showing the same results. The vertex attribute names are cloisterville, group and vertex.names. There are three types of groups, Loyal, Outcasts and Turks. Cloisterville is indicated by true or false and it represents whether the monk was in the seminary of Cloisterville before coming the the present monestary.\r\r\nLooking into the attribute data\r\r\n\r\r\n\r\r\nV(network_igraph)$vertex.names %>% head()\r\r\n\r\r\n\r\r\n[1] \"John Bosco\"  \"Gregory\"     \"Basil\"       \"Peter\"      \r\r\n[5] \"Bonaventure\" \"Berthold\"   \r\r\n\r\r\nnetwork_statnet %v% \"vertex.names\" %>% head()\r\r\n\r\r\n\r\r\n[1] \"John Bosco\"  \"Gregory\"     \"Basil\"       \"Peter\"      \r\r\n[5] \"Bonaventure\" \"Berthold\"   \r\r\n\r\r\nV(network_igraph)$cloisterville %>% head()\r\r\n\r\r\n\r\r\n[1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\r\r\n\r\r\nV(network_igraph)$group %>% head()\r\r\n\r\r\n\r\r\n[1] \"Turks\"    \"Turks\"    \"Outcasts\" \"Loyal\"    \"Loyal\"    \"Loyal\"   \r\r\n\r\r\nV(network_igraph)$vertex.names %>% length()\r\r\n\r\r\n\r\r\n[1] 18\r\r\n\r\r\nWe are able to retrieve all 18 of the monks names.\r\r\n\r\r\n\r\r\nE(network_igraph)$nominations %>% head()\r\r\n\r\r\n\r\r\n[1] 1 1 1 1 3 3\r\r\n\r\r\nnetwork_statnet %e% \"nominations\" %>% head()\r\r\n\r\r\n\r\r\n[1] 1 1 1 1 3 3\r\r\n\r\r\nE(network_igraph)$nominations %>% unique() %>% sort()\r\r\n\r\r\n\r\r\n[1] 1 2 3\r\r\n\r\r\nThe edge attributes seem to be numbers ranging form 1 to 3.\r\r\nSummarizing Network Attriubtes\r\r\n\r\r\n\r\r\nsummary(E(network_igraph)$nominations)\r\r\n\r\r\n\r\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\r\n  1.000   1.000   2.000   1.909   3.000   3.000 \r\r\n\r\r\nDyad Census\r\r\n\r\r\n\r\r\nigraph::dyad.census(network_igraph)\r\r\n\r\r\n\r\r\n$mut\r\r\n[1] 28\r\r\n\r\r\n$asym\r\r\n[1] 32\r\r\n\r\r\n$null\r\r\n[1] 93\r\r\n\r\r\nsna::dyad.census(network_statnet)\r\r\n\r\r\n\r\r\n     Mut Asym Null\r\r\n[1,]  28   32   93\r\r\n\r\r\nBoth network and igraph are getting the same results. Since each monk was each recommending the top three of each, I’m surprised by how many mutual ties there are.\r\r\nTriad Census\r\r\n\r\r\n\r\r\nigraph::triad_census(network_igraph)\r\r\n\r\r\n\r\r\n [1] 167 205 190  12  24  24  68  34   5   0  35  15   6   5  18   8\r\r\n\r\r\nsna::triad.census(network_statnet)\r\r\n\r\r\n\r\r\n     003 012 102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U\r\r\n[1,] 167 205 190   12   24   24   68   34    5    0  35   15    6\r\r\n     120C 210 300\r\r\n[1,]    5  18   8\r\r\n\r\r\nTransitivity\r\r\n\r\r\n\r\r\ntransitivity(network_igraph, type = \"global\")\r\r\n\r\r\n\r\r\n[1] 0.4646739\r\r\n\r\r\ntransitivity(network_igraph, type = \"average\")\r\r\n\r\r\n\r\r\n[1] 0.4925926\r\r\n\r\r\nThe global transivity is much lower than the average transivity. This means that we have multiple small groups of well connected monks with in their respective groups.\r\r\nGeodesic Paths\r\r\n\r\r\n\r\r\naverage.path.length(network_igraph)\r\r\n\r\r\n\r\r\n[1] 1.996732\r\r\n\r\r\nThis is to be expected with such a small network.\r\r\nComponent Structure\r\r\n\r\r\n\r\r\nnames(igraph::components(network_igraph))\r\r\n\r\r\n\r\r\n[1] \"membership\" \"csize\"      \"no\"        \r\r\n\r\r\nigraph::components(network_igraph)$no\r\r\n\r\r\n\r\r\n[1] 1\r\r\n\r\r\nigraph::components(network_igraph)$csize\r\r\n\r\r\n\r\r\n[1] 18\r\r\n\r\r\nigraph::components(network_igraph)$membership\r\r\n\r\r\n\r\r\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n\r\r\nIsolates\r\r\n\r\r\n\r\r\nisolates(network_statnet)\r\r\n\r\r\n\r\r\ninteger(0)\r\r\n\r\r\nThere are no isolates. This means there are no monks that don’t like any monks or are liked by other monks.\r\r\nDensity\r\r\n\r\r\n\r\r\ngraph.density(network_igraph)\r\r\n\r\r\n\r\r\n[1] 0.2875817\r\r\n\r\r\nnetwork.density(network_statnet)\r\r\n\r\r\n\r\r\n[1] 0.2875817\r\r\n\r\r\nBoth Network densitys are the same for both igraph and statnet. The denisty is low, but this is to be expected, since each monk could only pick 3 other monks.\r\r\nDegrees\r\r\n\r\r\n\r\r\nx <- data.frame(\r\r\n  Name = network_statnet %v% \"vertex.names\",\r\r\n  Total_Degrees = sna::degree(network_statnet),\r\r\n  Out_Degrees = sna::degree(network_statnet, cmode = \"indegree\"),\r\r\n  in_Degrees = sna::degree(network_statnet, cmode = \"outdegree\")\r\r\n  \r\r\n)\r\r\n\r\r\nx %>% arrange(desc(Total_Degrees)) %>% head()\r\r\n\r\r\n\r\r\n         Name Total_Degrees Out_Degrees in_Degrees\r\r\n1  John Bosco            17          11          6\r\r\n2     Gregory            15          10          5\r\r\n3 Bonaventure            13           8          5\r\r\n4        Mark            11           6          5\r\r\n5      Victor            11           5          6\r\r\n6     Winfrid            11           7          4\r\r\n\r\r\nx %>% arrange(Total_Degrees) %>% head()\r\r\n\r\r\n\r\r\n      Name Total_Degrees Out_Degrees in_Degrees\r\r\n1 Berthold             6           2          4\r\r\n2    Elias             6           2          4\r\r\n3    Basil             8           3          5\r\r\n4  Ambrose             8           4          4\r\r\n5  Romauld             8           2          6\r\r\n6    Louis             8           3          5\r\r\n\r\r\nx %>% arrange(in_Degrees)\r\r\n\r\r\n\r\r\n          Name Total_Degrees Out_Degrees in_Degrees\r\r\n1        Amand             8           5          3\r\r\n2        Peter             9           5          4\r\r\n3     Berthold             6           2          4\r\r\n4      Ambrose             8           4          4\r\r\n5      Winfrid            11           7          4\r\r\n6        Elias             6           2          4\r\r\n7      Gregory            15          10          5\r\r\n8        Basil             8           3          5\r\r\n9  Bonaventure            13           8          5\r\r\n10        Mark            11           6          5\r\r\n11       Louis             8           3          5\r\r\n12        Hugh            11           6          5\r\r\n13      Albert             8           3          5\r\r\n14  John Bosco            17          11          6\r\r\n15      Victor            11           5          6\r\r\n16     Romauld             8           2          6\r\r\n17    Boniface             9           3          6\r\r\n18  Simplicius             9           3          6\r\r\n\r\r\nx %>% arrange(Out_Degrees)\r\r\n\r\r\n\r\r\n          Name Total_Degrees Out_Degrees in_Degrees\r\r\n1     Berthold             6           2          4\r\r\n2      Romauld             8           2          6\r\r\n3        Elias             6           2          4\r\r\n4        Basil             8           3          5\r\r\n5        Louis             8           3          5\r\r\n6     Boniface             9           3          6\r\r\n7       Albert             8           3          5\r\r\n8   Simplicius             9           3          6\r\r\n9      Ambrose             8           4          4\r\r\n10       Peter             9           5          4\r\r\n11      Victor            11           5          6\r\r\n12       Amand             8           5          3\r\r\n13        Mark            11           6          5\r\r\n14        Hugh            11           6          5\r\r\n15     Winfrid            11           7          4\r\r\n16 Bonaventure            13           8          5\r\r\n17     Gregory            15          10          5\r\r\n18  John Bosco            17          11          6\r\r\n\r\r\nEverybody must love John Bosco. Berthold and Elias were very much disliked. Berthold, Elias and Basil were apart of the 4 monks that were expelled from the monestary. It seems that those with the smallest out degrees were the ones to expelled.\r\r\n\r\r\n\r\r\nsummary(x %>% select(-Name))\r\r\n\r\r\n\r\r\n Total_Degrees     Out_Degrees       in_Degrees   \r\r\n Min.   : 6.000   Min.   : 2.000   Min.   :3.000  \r\r\n 1st Qu.: 8.000   1st Qu.: 3.000   1st Qu.:4.000  \r\r\n Median : 9.000   Median : 4.500   Median :5.000  \r\r\n Mean   : 9.778   Mean   : 4.889   Mean   :4.889  \r\r\n 3rd Qu.:11.000   3rd Qu.: 6.000   3rd Qu.:5.750  \r\r\n Max.   :17.000   Max.   :11.000   Max.   :6.000  \r\r\n\r\r\nThe lowest degrees were 6, while on average, each monk had ~5 out and indegrees.\r\r\nDegree Distribution\r\r\n\r\r\n\r\r\nhist(x$Total_Degrees, main = \"Total Degree Distribution\",\r\r\n     xlab = \"Number of Likes\")\r\r\n\r\r\n\r\r\n\r\r\nhist(x$Out_Degrees, main = \"Out Degree Distribution\",\r\r\n     xlab = \"Number of Likes\")\r\r\n\r\r\n\r\r\n\r\r\nhist(x$in_Degrees, main = \"In Degree Distribution\",\r\r\n     xlab = \"Number of Likes\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nDegree Centralization\r\r\n\r\r\n\r\r\ncentr_degree(network_igraph, loops = FALSE, mode =\"in\")\r\r\n\r\r\n\r\r\n$res\r\r\n [1] 11 10  3  5  8  2  6  5  4  2  3  7  5  6  3  3  2  3\r\r\n\r\r\n$centralization\r\r\n[1] 0.3806228\r\r\n\r\r\n$theoretical_max\r\r\n[1] 289\r\r\n\r\r\ncentr_degree(network_igraph, loops = FALSE, mode = \"out\")\r\r\n\r\r\n\r\r\n$res\r\r\n [1] 6 5 5 4 5 4 5 6 4 6 5 4 3 5 6 5 4 6\r\r\n\r\r\n$centralization\r\r\n[1] 0.06920415\r\r\n\r\r\n$theoretical_max\r\r\n[1] 289\r\r\n\r\r\nThe indegree centralization is at 38 % while the out degree is at 7%.\r\r\nEigenvector Centrality\r\r\n\r\r\n\r\r\nx$eigen <- sna::evcent(network_statnet)\r\r\n\r\r\nx %>% arrange(eigen)\r\r\n\r\r\n\r\r\n          Name Total_Degrees Out_Degrees in_Degrees     eigen\r\r\n1        Amand             8           5          3 0.1479037\r\r\n2     Berthold             6           2          4 0.1711191\r\r\n3        Peter             9           5          4 0.1717189\r\r\n4        Elias             6           2          4 0.1833911\r\r\n5      Ambrose             8           4          4 0.1900849\r\r\n6  Bonaventure            13           8          5 0.2061513\r\r\n7      Winfrid            11           7          4 0.2138861\r\r\n8        Basil             8           3          5 0.2313738\r\r\n9        Louis             8           3          5 0.2334813\r\r\n10     Romauld             8           2          6 0.2481996\r\r\n11        Mark            11           6          5 0.2591275\r\r\n12        Hugh            11           6          5 0.2592391\r\r\n13     Gregory            15          10          5 0.2635120\r\r\n14      Albert             8           3          5 0.2643663\r\r\n15      Victor            11           5          6 0.2664155\r\r\n16  Simplicius             9           3          6 0.2745468\r\r\n17  John Bosco            17          11          6 0.2879960\r\r\n18    Boniface             9           3          6 0.2978587\r\r\n\r\r\nBonacich Power Centrality\r\r\n\r\r\n\r\r\nlibrary(DT)\r\r\nx$Bonach_Power <- power_centrality(network_igraph)\r\r\nx$Bon_pow <- sna::bonpow(network_statnet)\r\r\n\r\r\nx %>% DT::datatable()\r\r\n\r\r\n\r\r\n\r\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\"],[\"John Bosco\",\"Gregory\",\"Basil\",\"Peter\",\"Bonaventure\",\"Berthold\",\"Mark\",\"Victor\",\"Ambrose\",\"Romauld\",\"Louis\",\"Winfrid\",\"Amand\",\"Hugh\",\"Boniface\",\"Albert\",\"Elias\",\"Simplicius\"],[17,15,8,9,13,6,11,11,8,8,8,11,8,11,9,8,6,9],[11,10,3,5,8,2,6,5,4,2,3,7,5,6,3,3,2,3],[6,5,5,4,5,4,5,6,4,6,5,4,3,5,6,5,4,6],[0.287995960973052,0.263512041683265,0.23137382298106,0.171718943308207,0.206151324404231,0.171119148648865,0.259127539555084,0.266415461646183,0.19008488030372,0.248199602384464,0.233481252428839,0.213886100223566,0.147903702675302,0.259239139423174,0.297858707718366,0.264366267335889,0.183391103899862,0.274546803242429],[-1.42155092108412,-0.684824226688595,-1.37829886466168,-1.26007657710702,-1.07553446970461,-1.05679191192156,-0.596878378629638,-1.21682452068458,-0.321506952740119,-1.00488944421463,-1.14473775998051,-0.640130435052077,-0.705008519685733,-0.958753917364034,-0.844856835451614,-0.410894536013156,-1.04525803020891,-1.29900342788721],[-1.42155092108411,-0.684824226688599,-1.37829886466168,-1.26007657710701,-1.07553446970461,-1.05679191192156,-0.596878378629644,-1.21682452068458,-0.321506952740126,-1.00488944421464,-1.14473775998051,-0.640130435052078,-0.705008519685735,-0.958753917364033,-0.844856835451616,-0.410894536013161,-1.04525803020891,-1.29900342788721]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Name<\\/th>\\n      <th>Total_Degrees<\\/th>\\n      <th>Out_Degrees<\\/th>\\n      <th>in_Degrees<\\/th>\\n      <th>eigen<\\/th>\\n      <th>Bonach_Power<\\/th>\\n      <th>Bon_pow<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5,6,7]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\r\nCreating Derived and Centrality Scores\r\r\n\r\r\n\r\r\nMatSamson <- as.matrix(as_adjacency_matrix(network_igraph, attr = \"nominations\"))\r\r\n\r\r\nMatSamsonsq<- t(MatSamson) %*% MatSamson\r\r\nx$rc <- diag(MatSamsonsq)/rowSums(MatSamsonsq)\r\r\nx$rc <- ifelse(is.nan(x$rc),0,x$rc)\r\r\nx$eigen.rc <- x$eigen*x$rc\r\r\n\r\r\nx$dc <- 1 - diag(MatSamsonsq)/rowSums(MatSamsonsq)\r\r\nx$dc <- ifelse(is.nan(x$dc),1,x$dc)\r\r\nx$eigen.dc <- x$eigen*x$dc\r\r\n\r\r\n\r\r\nx%>% arrange(Total_Degrees)\r\r\n\r\r\n\r\r\n          Name Total_Degrees Out_Degrees in_Degrees     eigen\r\r\n1     Berthold             6           2          4 0.1711191\r\r\n2        Elias             6           2          4 0.1833911\r\r\n3        Basil             8           3          5 0.2313738\r\r\n4      Ambrose             8           4          4 0.1900849\r\r\n5      Romauld             8           2          6 0.2481996\r\r\n6        Louis             8           3          5 0.2334813\r\r\n7        Amand             8           5          3 0.1479037\r\r\n8       Albert             8           3          5 0.2643663\r\r\n9        Peter             9           5          4 0.1717189\r\r\n10    Boniface             9           3          6 0.2978587\r\r\n11  Simplicius             9           3          6 0.2745468\r\r\n12        Mark            11           6          5 0.2591275\r\r\n13      Victor            11           5          6 0.2664155\r\r\n14     Winfrid            11           7          4 0.2138861\r\r\n15        Hugh            11           6          5 0.2592391\r\r\n16 Bonaventure            13           8          5 0.2061513\r\r\n17     Gregory            15          10          5 0.2635120\r\r\n18  John Bosco            17          11          6 0.2879960\r\r\n   Bonach_Power    Bon_pow        rc   eigen.rc        dc  eigen.dc\r\r\n1    -1.0567919 -1.0567919 0.2888889 0.04943442 0.7111111 0.1216847\r\r\n2    -1.0452580 -1.0452580 0.2708333 0.04966842 0.7291667 0.1337227\r\r\n3    -1.3782989 -1.3782989 0.2592593 0.05998581 0.7407407 0.1713880\r\r\n4    -0.3215070 -0.3215070 0.2343750 0.04455114 0.7656250 0.1455337\r\r\n5    -1.0048894 -1.0048894 0.1111111 0.02757773 0.8888889 0.2206219\r\r\n6    -1.1447378 -1.1447378 0.2500000 0.05837031 0.7500000 0.1751109\r\r\n7    -0.7050085 -0.7050085 0.1403509 0.02075841 0.8596491 0.1271453\r\r\n8    -0.4108945 -0.4108945 0.2444444 0.06462287 0.7555556 0.1997434\r\r\n9    -1.2600766 -1.2600766 0.2882883 0.04950456 0.7117117 0.1222144\r\r\n10   -0.8448568 -0.8448568 0.2115385 0.06300857 0.7884615 0.2348501\r\r\n11   -1.2990034 -1.2990034 0.2972973 0.08162202 0.7027027 0.1929248\r\r\n12   -0.5968784 -0.5968784 0.2678571 0.06940916 0.7321429 0.1897184\r\r\n13   -1.2168245 -1.2168245 0.2191781 0.05839243 0.7808219 0.2080230\r\r\n14   -0.6401304 -0.6401304 0.1944444 0.04158896 0.8055556 0.1722971\r\r\n15   -0.9587539 -0.9587539 0.1914894 0.04964154 0.8085106 0.2095976\r\r\n16   -1.0755345 -1.0755345 0.2787879 0.05747249 0.7212121 0.1486788\r\r\n17   -0.6848242 -0.6848242 0.2717949 0.07162122 0.7282051 0.1918908\r\r\n18   -1.4215509 -1.4215509 0.2413793 0.06951627 0.7586207 0.2184797\r\r\n\r\r\nCentrality Score Distributions\r\r\n\r\r\n\r\r\nlibrary(ggplot2)\r\r\nx %>% select(-Name)%>%\r\r\n  gather()%>% ggplot(aes(value))+\r\r\n  geom_histogram()+\r\r\n  facet_wrap(~key, scales = \"free\")+\r\r\n  ggtitle(\"Centrality Distributions\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nCentrality Measure Correlations\r\r\n\r\r\n\r\r\nlibrary(corrplot)\r\r\ntemp <- x %>% select(Total_Degrees,in_Degrees,Out_Degrees, eigen, eigen.rc, eigen.dc, Bonach_Power) %>% cor() \r\r\n\r\r\ntemp\r\r\n\r\r\n\r\r\n              Total_Degrees  in_Degrees  Out_Degrees      eigen\r\r\nTotal_Degrees     1.0000000  0.39262202  0.951760560  0.4898144\r\r\nin_Degrees        0.3926220  1.00000000  0.091479857  0.8976743\r\r\nOut_Degrees       0.9517606  0.09147986  1.000000000  0.2308543\r\r\neigen             0.4898144  0.89767426  0.230854298  1.0000000\r\r\neigen.rc          0.4138993  0.57257367  0.257121131  0.6628988\r\r\neigen.dc          0.4276075  0.86168397  0.175507168  0.9493956\r\r\nBonach_Power     -0.1269956 -0.38933451 -0.007610095 -0.1159809\r\r\n                eigen.rc    eigen.dc Bonach_Power\r\r\nTotal_Degrees  0.4138993  0.42760750 -0.126995572\r\r\nin_Degrees     0.5725737  0.86168397 -0.389334506\r\r\nOut_Degrees    0.2571211  0.17550717 -0.007610095\r\r\neigen          0.6628988  0.94939557 -0.115980915\r\r\neigen.rc       1.0000000  0.39419648 -0.228543837\r\r\neigen.dc       0.3941965  1.00000000 -0.046490413\r\r\nBonach_Power  -0.2285438 -0.04649041  1.000000000\r\r\n\r\r\ncorrplot(temp)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/monk-monastery-network/distill-preview.png",
    "last_modified": "2022-02-17T08:28:55-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/machine-learning-hw1/",
    "title": "Machine Learning Hw1",
    "description": "A look into Linear and Logistic Regression, and Clustering.",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-13",
    "categories": [],
    "contents": "\r\r\nQuestion 2. Exercise 1.\r\r\nFor each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\r\r\n(a)\r\r\nThe sample size n is extremely large, and the number of predictors p is small.\r\r\nI would expect it to be better, as flexible models require lots of more data. So a large n would be beneficial for a flexible model.\r\r\n(b)\r\r\nThe number of predictors p is extremely large, and the number of observations n is small.\r\r\nFor this I would recommend a more rigid model with the small n. The large number of p is also worrying for a flexible model. As shown in the text book in figure 2.6, large amount of predictors can cause the model to overfit.\r\r\n(c)\r\r\nThe relationship between the predictors and response is highly non-linear.\r\r\nI would expect a better performance form a flexible model. Lasso and least squares run better on linear data, which means the more flexible models will perform better.\r\r\n(d)\r\r\nThe variance of the error terms, i.e. σ2 = Var(ϵ), is extremely high.\r\r\nHigh Variance means we will have a lot of points above and below the line of best fit. If we were to use a more flexible model, we would see something similar to the high predictors p, overfitting. So a more flexible model would perform worse.\r\r\n\r\r\nISLR Ch. 2, exercise 2\r\r\nExplain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.\r\r\n(a)\r\r\nWe collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\r\r\nn = # of firms, 500 p = profit, # of employees, industry Regression Inference\r\r\nThis is a regression problem. We are not trying to classify the CEO salary into bands, or group the company salarys to similar ones. We are trying to understand how these predictors correlate to the dependent variable, CEO salary. We are most interested in inference.\r\r\n(b)\r\r\nWe are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\r\r\nn = # of products, 20 p = price, marketing budget, competition price, ten other variables Classification Prediction\r\r\nThis is a classification problem. We are trying to learn from the predictors to label a product as a success or failure. If we were trying to predict the probalitlity of success and failure, then I believe that would be a logit regression. We are most interested in prediction.\r\r\n(c)\r\r\nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\r\r\nn = # of weeks recorded, 1 year, 52 p = % change of us, british and german market\r\r\nThis is a regression problem. We are trying to predict a number based on predictors. This is a prediciton problem.\r\r\n\r\r\nISLR Ch. 2, exercise 7\r\r\nThe table below provides a training data set containing six observations, three predictors, and one qualitative response variable. \r\r\nSuppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors.\r\r\n(a)\r\r\n\r\r\nCompute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0.\r\r\n\r\r\n\r\r\ndist<- function(x,y,z){\r\r\n  distance <- (x^2+y^2+z^2)^(1/2)\r\r\n \r\r\n}\r\r\n\r\r\no1<- dist(0,3,0)\r\r\no2 <- dist(2,0,0)\r\r\no3<- dist(0,1,3)\r\r\no4<- dist(0,1,2)\r\r\no5 <- dist(-1,0,1)\r\r\no6 <- dist(1,1,1)\r\r\n\r\r\n\r\r\ncbind(o1,o2, o3,o4,o5,o6)\r\r\n\r\r\n\r\r\n     o1 o2       o3       o4       o5       o6\r\r\n[1,]  3  2 3.162278 2.236068 1.414214 1.732051\r\r\n\r\r\n(b)\r\r\nWhat is our prediction with K = 1? Why?\r\r\nIf we use K =1, then the nearest observation from 0,0,0 is the green observation at -1,0,1.\r\r\nPredict Green.\r\r\n(c)\r\r\nWhat is our prediction with K = 3? Why?\r\r\nWe would use observations 2, 5, and 6 as they are the closest to the center point. These are red, green, and red. So, we would predict red.\r\r\nIf the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?\r\r\nAs the K value becomes large, we expect the model to become more linear. So a smaller K would work better for a highly nonlinear model.\r\r\n\r\r\nISLR Ch. 2, exercise 10\r\r\n\r\r\nThis exercise involves the Boston housing data set. ##### (a) To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library.\r\r\n\r\r\n\r\r\nlibrary(ISLR2)\r\r\nhead(Boston)\r\r\n\r\r\n\r\r\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat\r\r\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98\r\r\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14\r\r\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03\r\r\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94\r\r\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33\r\r\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21\r\r\n  medv\r\r\n1 24.0\r\r\n2 21.6\r\r\n3 34.7\r\r\n4 33.4\r\r\n5 36.2\r\r\n6 28.7\r\r\n\r\r\n#?Boston\r\r\n\r\r\n\r\r\n\r\r\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\r\r\n\r\r\n\r\r\ndim(Boston)\r\r\n\r\r\n\r\r\n[1] 506  13\r\r\n\r\r\ncolnames(Boston)\r\r\n\r\r\n\r\r\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"     \r\r\n [7] \"age\"     \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"lstat\"  \r\r\n[13] \"medv\"   \r\r\n\r\r\n#head(Boston)\r\r\n\r\r\n\r\r\n\r\r\n506 Rows of suburbs in Boston. 14 Predictors (columns)\r\r\nEach row is a case.\r\r\n(b)\r\r\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.\r\r\n\r\r\n\r\r\npairs(Boston)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nInitial Plot is hard to interpret. Lets trim down to these variables.\r\r\n\r\r\n\r\r\ncolumns <- colnames(Boston)\r\r\n\r\r\n\r\r\nx = 0\r\r\ngroup = NULL\r\r\nfor(i in columns){\r\r\n  #print(i)\r\r\n  x <- x+1\r\r\n  #print(x)\r\r\n  group <- cbind(group,i)\r\r\n  \r\r\n  if(x%%3 == 0){\r\r\n    #print(x)\r\r\n    #pairs(group)\r\r\n    #print(group)\r\r\n    df <- Boston[,colnames(Boston) %in% group]\r\r\n    pairs(df)\r\r\n    group <- NULL\r\r\n    \r\r\n    \r\r\n  }\r\r\n  \r\r\n  \r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nPossible Correlation between Age and dis, Zn and indus and possibly nox an rm.\r\r\n(c)\r\r\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship. Lets try using the cor function.\r\r\n\r\r\n\r\r\nhead(cor(Boston),1)\r\r\n\r\r\n\r\r\n     crim         zn     indus        chas       nox         rm\r\r\ncrim    1 -0.2004692 0.4065834 -0.05589158 0.4209717 -0.2192467\r\r\n           age        dis       rad       tax   ptratio     lstat\r\r\ncrim 0.3527343 -0.3796701 0.6255051 0.5827643 0.2899456 0.4556215\r\r\n           medv\r\r\ncrim -0.3883046\r\r\n\r\r\nCrim has the highest correlations with indus at .41, nox at .42, rad at .625, tax at .58, and lstat at .455. The two highes are lstat, and rad.\r\r\n(d)\r\r\nDo any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\r\r\n\r\r\n\r\r\nsummary(Boston)\r\r\n\r\r\n\r\r\n      crim                zn             indus      \r\r\n Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46  \r\r\n 1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19  \r\r\n Median : 0.25651   Median :  0.00   Median : 9.69  \r\r\n Mean   : 3.61352   Mean   : 11.36   Mean   :11.14  \r\r\n 3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10  \r\r\n Max.   :88.97620   Max.   :100.00   Max.   :27.74  \r\r\n      chas              nox               rm             age        \r\r\n Min.   :0.00000   Min.   :0.3850   Min.   :3.561   Min.   :  2.90  \r\r\n 1st Qu.:0.00000   1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02  \r\r\n Median :0.00000   Median :0.5380   Median :6.208   Median : 77.50  \r\r\n Mean   :0.06917   Mean   :0.5547   Mean   :6.285   Mean   : 68.57  \r\r\n 3rd Qu.:0.00000   3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08  \r\r\n Max.   :1.00000   Max.   :0.8710   Max.   :8.780   Max.   :100.00  \r\r\n      dis              rad              tax           ptratio     \r\r\n Min.   : 1.130   Min.   : 1.000   Min.   :187.0   Min.   :12.60  \r\r\n 1st Qu.: 2.100   1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40  \r\r\n Median : 3.207   Median : 5.000   Median :330.0   Median :19.05  \r\r\n Mean   : 3.795   Mean   : 9.549   Mean   :408.2   Mean   :18.46  \r\r\n 3rd Qu.: 5.188   3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20  \r\r\n Max.   :12.127   Max.   :24.000   Max.   :711.0   Max.   :22.00  \r\r\n     lstat            medv      \r\r\n Min.   : 1.73   Min.   : 5.00  \r\r\n 1st Qu.: 6.95   1st Qu.:17.02  \r\r\n Median :11.36   Median :21.20  \r\r\n Mean   :12.65   Mean   :22.53  \r\r\n 3rd Qu.:16.95   3rd Qu.:25.00  \r\r\n Max.   :37.97   Max.   :50.00  \r\r\n\r\r\nThe max crime rate is at 88% with a min at .006%. The crime rate data is heavily skewed by outliers, this can be seen by the very low median and a very high mean. The range of tax rates are from 187 per $10000 to 711. The pupil teacher ratio has a mean of 18.46 and a min of 12.6 and a max of 22.\r\r\n(e)\r\r\nHow many of the census tracts in this data set bound the Charles river?\r\r\n\r\r\n\r\r\nhead(Boston)\r\r\n\r\r\n\r\r\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat\r\r\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98\r\r\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14\r\r\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03\r\r\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94\r\r\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33\r\r\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21\r\r\n  medv\r\r\n1 24.0\r\r\n2 21.6\r\r\n3 34.7\r\r\n4 33.4\r\r\n5 36.2\r\r\n6 28.7\r\r\n\r\r\nBoston %>% filter(chas==1) %>% dim()\r\r\n\r\r\n\r\r\n[1] 35 13\r\r\n\r\r\nThere are 35 census tracts bound by the charles river.\r\r\n(f)\r\r\nWhat is the median pupil-teacher ratio among the towns in this data set?\r\r\n\r\r\n\r\r\nmedian(Boston$ptratio)\r\r\n\r\r\n\r\r\n[1] 19.05\r\r\n\r\r\nsummary(Boston$ptratio)\r\r\n\r\r\n\r\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\r\n  12.60   17.40   19.05   18.46   20.20   22.00 \r\r\n\r\r\nThe Median pratio is 19.05.\r\r\n(g)\r\r\nWhich census tract of Boston has lowest median value of owneroccupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.\r\r\n\r\r\n\r\r\n# 2 census tracts with lowest median value of owner occupied homes\r\r\nBoston %>% arrange(medv) %>% slice(1:2)\r\r\n\r\r\n\r\r\n     crim zn indus chas   nox    rm age    dis rad tax ptratio lstat\r\r\n1 38.3518  0  18.1    0 0.693 5.453 100 1.4896  24 666    20.2 30.59\r\r\n2 67.9208  0  18.1    0 0.693 5.683 100 1.4254  24 666    20.2 22.98\r\r\n  medv\r\r\n1    5\r\r\n2    5\r\r\n\r\r\nI found 2 census tracts with the lowest median value of owner occupied homes at 5. The crime rates were relatively high at 38 and 67. The tax rates were also very high in this area, which is surprising with a very low median home value. Age also seems to be maxed out for these at the 100 value. So these are highly taxed, very old houses, with a high crime rate.\r\r\n(h)\r\r\nIn this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.\r\r\n\r\r\n\r\r\n# higher than 7 per dwelling\r\r\n\r\r\nBoston %>% filter(rm >= 7) %>%  dim()\r\r\n\r\r\n\r\r\n[1] 64 13\r\r\n\r\r\n# higher then 8 per dwelling\r\r\n\r\r\n\r\r\nBoston %>% filter(rm >= 8) %>% dim()\r\r\n\r\r\n\r\r\n[1] 13 13\r\r\n\r\r\nBoston%>% filter(rm >=8) %>% summary()\r\r\n\r\r\n\r\r\n      crim               zn            indus             chas       \r\r\n Min.   :0.02009   Min.   : 0.00   Min.   : 2.680   Min.   :0.0000  \r\r\n 1st Qu.:0.33147   1st Qu.: 0.00   1st Qu.: 3.970   1st Qu.:0.0000  \r\r\n Median :0.52014   Median : 0.00   Median : 6.200   Median :0.0000  \r\r\n Mean   :0.71879   Mean   :13.62   Mean   : 7.078   Mean   :0.1538  \r\r\n 3rd Qu.:0.57834   3rd Qu.:20.00   3rd Qu.: 6.200   3rd Qu.:0.0000  \r\r\n Max.   :3.47428   Max.   :95.00   Max.   :19.580   Max.   :1.0000  \r\r\n      nox               rm             age             dis       \r\r\n Min.   :0.4161   Min.   :8.034   Min.   : 8.40   Min.   :1.801  \r\r\n 1st Qu.:0.5040   1st Qu.:8.247   1st Qu.:70.40   1st Qu.:2.288  \r\r\n Median :0.5070   Median :8.297   Median :78.30   Median :2.894  \r\r\n Mean   :0.5392   Mean   :8.349   Mean   :71.54   Mean   :3.430  \r\r\n 3rd Qu.:0.6050   3rd Qu.:8.398   3rd Qu.:86.50   3rd Qu.:3.652  \r\r\n Max.   :0.7180   Max.   :8.780   Max.   :93.90   Max.   :8.907  \r\r\n      rad              tax           ptratio          lstat     \r\r\n Min.   : 2.000   Min.   :224.0   Min.   :13.00   Min.   :2.47  \r\r\n 1st Qu.: 5.000   1st Qu.:264.0   1st Qu.:14.70   1st Qu.:3.32  \r\r\n Median : 7.000   Median :307.0   Median :17.40   Median :4.14  \r\r\n Mean   : 7.462   Mean   :325.1   Mean   :16.36   Mean   :4.31  \r\r\n 3rd Qu.: 8.000   3rd Qu.:307.0   3rd Qu.:17.40   3rd Qu.:5.12  \r\r\n Max.   :24.000   Max.   :666.0   Max.   :20.20   Max.   :7.44  \r\r\n      medv     \r\r\n Min.   :21.9  \r\r\n 1st Qu.:41.7  \r\r\n Median :48.3  \r\r\n Mean   :44.2  \r\r\n 3rd Qu.:50.0  \r\r\n Max.   :50.0  \r\r\n\r\r\nThere are 64 census tracts that are 7 or more rooms per dwelling, but only 13 with 8 or more rooms per dwelling. For the 13 dwellings with 8 or mor rooms per dweling I saw the following statistics: - max crime rate of 3.4, mean of .718 -max lstat of 7.44, mean of 4.3 - max mdv value of 50, average of 44\r\r\nThese seem to be a safer neighborhood, with a higher status and more expensive homes.\r\r\n\r\r\nISLR Ch. 3, exercise 1\r\r\nDescribe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.\r\r\n\r\r\nAll independent variables =0. Alternative hypothesis - at least one independent variable is non-zero.\r\r\nTV and radio are both statistically significant, which means the null hypothesis can be rejected. We can assume there is some correlation between TV and Radio. Newspaper’s pvalue is above .05 which means the independent variable is not statistically significant. Which means we accept the null hypothesis, and there is no correlation between newspaper and sales.\r\r\n\r\r\nISLR Ch. 3, exercise 3\r\r\nSuppose we have a data set with five predictors, X1 = GPA, X2 =IQ, X3 = Level (1 for College and 0 for High School), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get βˆ0 = 50, βˆ1 = 20, βˆ2 = 0.07, βˆ3 = 35, βˆ4 = 0.01, βˆ5 = −10.\r\r\nSalary = 50 + GPA * 20 + IQ +.07 + Level * 35 + .01 * GPA:IQ -10 * GPA:Level ##### (a) Which answer is correct, and why?\r\r\ni.\r\r\nFor a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\r\r\nfixed iq and gpa iq, gpa = 1 S = 70.08 + level* 35 -10 * level\r\r\nCollege = 1 highschool = 0\r\r\ncollege - S = 70.08 +35 - 10 Highschool - S = 70\r\r\nThis one is not true.\r\r\nii.\r\r\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\r\r\nThis one is true based on the logic from above. On average, assuming that iq and GPA = 1. College grads will earch 20 thousand dollars more.\r\r\niii.\r\r\nFor a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\r\r\nThis one is wrong.\r\r\niv.\r\r\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.\r\r\nThis one is wrong.\r\r\n(b)\r\r\nPredict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\r\r\n\r\r\n\r\r\nGPA = 4\r\r\nIQ =110\r\r\nLevel = 1\r\r\n\r\r\nSalary = 50 + GPA * 20  + IQ +.07 + Level * 35 + .01 * GPA*IQ  -10 * GPA*Level\r\r\n\r\r\nSalary\r\r\n\r\r\n\r\r\n[1] 239.47\r\r\n\r\r\nSalary = 239K\r\r\n(c)\r\r\nTrue or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\r\r\nThis is false. .01 is a relative term. for the problem above we had an IQ of 110. GPA ranging from 1 - 4 would give the range of 1.1K to 4.4 K.\r\r\n\r\r\nISLR Ch. 3, exercise 15\r\r\nThis problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.\r\r\n(a)\r\r\nFor each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.\r\r\nPredict Crime Rate\r\r\n\r\r\n\r\r\n#Boston\r\r\n#colnames(Boston)\r\r\nDependent_variable <- Boston %>% dplyr::select(crim)\r\r\npredictors <- Boston %>% dplyr::select(-crim)\r\r\n#Dependent_variable %>% head()\r\r\n#predictors %>% head()\r\r\n\r\r\n\r\r\npredictors_varname <- colnames(predictors)\r\r\n\r\r\n\r\r\n\r\r\noutcome <- \"crim\"\r\r\nmodels <- lapply(paste(outcome,\" ~\", predictors_varname), as.formula)\r\r\ny <- NULL\r\r\n\r\r\n\r\r\nfor (model in models){\r\r\n  linearmodel <- lm(model, data = Boston)\r\r\n  x <- summary(linearmodel)\r\r\n  print(x$coefficients)\r\r\n  #print(paste(format(model), \"R^2 value: \", round(x$r.squared,3)*100, \"%\"))\r\r\n  y <- rbind(y, data.frame(variable = as.character(model[3]),\r\r\n              \"Rvalue_Percent\" = round(x$r.squared,3)*100))\r\r\n}\r\r\n\r\r\n\r\r\n               Estimate Std. Error   t value     Pr(>|t|)\r\r\n(Intercept)  4.45369376  0.4172178 10.674746 4.037668e-24\r\r\nzn          -0.07393498  0.0160946 -4.593776 5.506472e-06\r\r\n              Estimate Std. Error   t value     Pr(>|t|)\r\r\n(Intercept) -2.0637426 0.66722830 -3.093008 2.091266e-03\r\r\nindus        0.5097763 0.05102433  9.990848 1.450349e-21\r\r\n             Estimate Std. Error   t value     Pr(>|t|)\r\r\n(Intercept)  3.744447  0.3961111  9.453021 1.239505e-19\r\r\nchas        -1.892777  1.5061155 -1.256727 2.094345e-01\r\r\n             Estimate Std. Error   t value     Pr(>|t|)\r\r\n(Intercept) -13.71988   1.699479 -8.072992 5.076814e-15\r\r\nnox          31.24853   2.999190 10.418989 3.751739e-23\r\r\n             Estimate Std. Error   t value     Pr(>|t|)\r\r\n(Intercept) 20.481804  3.3644742  6.087669 2.272000e-09\r\r\nrm          -2.684051  0.5320411 -5.044819 6.346703e-07\r\r\n              Estimate Std. Error   t value     Pr(>|t|)\r\r\n(Intercept) -3.7779063 0.94398472 -4.002084 7.221718e-05\r\r\nage          0.1077862 0.01273644  8.462825 2.854869e-16\r\r\n             Estimate Std. Error   t value     Pr(>|t|)\r\r\n(Intercept)  9.499262  0.7303972 13.005611 1.502748e-33\r\r\ndis         -1.550902  0.1683300 -9.213458 8.519949e-19\r\r\n              Estimate Std. Error   t value     Pr(>|t|)\r\r\n(Intercept) -2.2871594 0.44347583 -5.157349 3.605846e-07\r\r\nrad          0.6179109 0.03433182 17.998199 2.693844e-56\r\r\n               Estimate  Std. Error   t value     Pr(>|t|)\r\r\n(Intercept) -8.52836909 0.815809392 -10.45387 2.773600e-23\r\r\ntax          0.02974225 0.001847415  16.09939 2.357127e-47\r\r\n              Estimate Std. Error   t value     Pr(>|t|)\r\r\n(Intercept) -17.646933  3.1472718 -5.607057 3.395255e-08\r\r\nptratio       1.151983  0.1693736  6.801430 2.942922e-11\r\r\n              Estimate Std. Error   t value     Pr(>|t|)\r\r\n(Intercept) -3.3305381 0.69375829 -4.800718 2.087022e-06\r\r\nlstat        0.5488048 0.04776097 11.490654 2.654277e-27\r\r\n              Estimate Std. Error  t value     Pr(>|t|)\r\r\n(Intercept) 11.7965358 0.93418916 12.62757 5.934119e-32\r\r\nmedv        -0.3631599 0.03839017 -9.45971 1.173987e-19\r\r\n\r\r\ny %>% arrange(desc(Rvalue_Percent))\r\r\n\r\r\n\r\r\n   variable Rvalue_Percent\r\r\n1       rad           39.1\r\r\n2       tax           34.0\r\r\n3     lstat           20.8\r\r\n4       nox           17.7\r\r\n5     indus           16.5\r\r\n6      medv           15.1\r\r\n7       dis           14.4\r\r\n8       age           12.4\r\r\n9   ptratio            8.4\r\r\n10       rm            4.8\r\r\n11       zn            4.0\r\r\n12     chas            0.3\r\r\n\r\r\nWhen predicting crime I found the following predictors statistically significant: - indus - zn - medv - lstat - pratio - tax - rad - dis - age - rm -nox\r\r\nThe only one not statistically significant is nox.\r\r\n(b)\r\r\nFit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0?\r\r\n\r\r\n\r\r\nlm.fit <- lm(data = Boston, crim ~ .)\r\r\nsummary(lm.fit)\r\r\n\r\r\n\r\r\n\r\r\nCall:\r\r\nlm(formula = crim ~ ., data = Boston)\r\r\n\r\r\nResiduals:\r\r\n   Min     1Q Median     3Q    Max \r\r\n-8.534 -2.248 -0.348  1.087 73.923 \r\r\n\r\r\nCoefficients:\r\r\n              Estimate Std. Error t value Pr(>|t|)    \r\r\n(Intercept) 13.7783938  7.0818258   1.946 0.052271 .  \r\r\nzn           0.0457100  0.0187903   2.433 0.015344 *  \r\r\nindus       -0.0583501  0.0836351  -0.698 0.485709    \r\r\nchas        -0.8253776  1.1833963  -0.697 0.485841    \r\r\nnox         -9.9575865  5.2898242  -1.882 0.060370 .  \r\r\nrm           0.6289107  0.6070924   1.036 0.300738    \r\r\nage         -0.0008483  0.0179482  -0.047 0.962323    \r\r\ndis         -1.0122467  0.2824676  -3.584 0.000373 ***\r\r\nrad          0.6124653  0.0875358   6.997 8.59e-12 ***\r\r\ntax         -0.0037756  0.0051723  -0.730 0.465757    \r\r\nptratio     -0.3040728  0.1863598  -1.632 0.103393    \r\r\nlstat        0.1388006  0.0757213   1.833 0.067398 .  \r\r\nmedv        -0.2200564  0.0598240  -3.678 0.000261 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nResidual standard error: 6.46 on 493 degrees of freedom\r\r\nMultiple R-squared:  0.4493,    Adjusted R-squared:  0.4359 \r\r\nF-statistic: 33.52 on 12 and 493 DF,  p-value: < 2.2e-16\r\r\n\r\r\nFor zn, dis, rad, and medv we can reject the null hypothesis that Beta = 0, as there is statistically significance and correlation between those variables and crime rate.\r\r\n(c)\r\r\nHow do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.\r\r\nFirst step is gathering the coefficients from each model.\r\r\n\r\r\n\r\r\nmultiple <- lm(data = Boston, crim ~ .)\r\r\nx <- summary(multiple)\r\r\n\r\r\n\r\r\n\r\r\nmulti_coef <- data.frame(multi_coef = x$coefficients[2:13,1])\r\r\n#multi_coef\r\r\n\r\r\n#now need coefficients from previous models\r\r\n\r\r\noutcome <- \"crim\"\r\r\nmodels <- lapply(paste(outcome,\" ~\", predictors_varname), as.formula)\r\r\ncoef_matrix <- NULL\r\r\n\r\r\n\r\r\nfor (model in models){\r\r\n  linearmodel <- lm(model, data = Boston)\r\r\n  x <- summary(linearmodel)\r\r\n  #print(x$coefficients[2,4])\r\r\n  \r\r\n  coef_matrix <- rbind(coef_matrix, data.frame(predictor = as.character(model[3]),\r\r\n              coef = x$coefficients[2,1]))\r\r\n}\r\r\n\r\r\n#multi_coef\r\r\n#coef_matrix\r\r\n\r\r\nplot(coef_matrix$coef,multi_coef$multi_coef, xlab = \"Univariate Coefficients\", ylab = \"multiple regression coefficients\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n(d)\r\r\nIs there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form Y = β0 + β1X + β2X2 + β3X3 + ϵ.\r\r\n\r\r\n\r\r\nlibrary(car)\r\r\nlm.fit <- lm(data = Boston, crim ~.)\r\r\nvif(lm.fit)\r\r\n\r\r\n\r\r\n      zn    indus     chas      nox       rm      age      dis \r\r\n2.323944 3.983627 1.093242 4.546642 2.201688 3.088678 4.280979 \r\r\n     rad      tax  ptratio    lstat     medv \r\r\n7.029796 9.195493 1.969732 3.538098 3.663205 \r\r\n\r\r\nPossible non-linearity due to tax, and may rad.\r\r\nAnother way to look into this is using the ploy function for each coefficient.\r\r\n\r\r\n\r\r\n#practice\r\r\n\r\r\nlm.poly <- lm(data = Boston, crim ~ poly(lstat,5))\r\r\nz <- summary(lm.poly)\r\r\n#data.frame(z$coefficients)\r\r\n\r\r\n#Boston %>% colnames()\r\r\n\r\r\n\r\r\n\r\r\n(lm.poly <- lm(data = Boston, crim ~ poly(lstat,5))) %>% summary()\r\r\n\r\r\n\r\r\n\r\r\nCall:\r\r\nlm(formula = crim ~ poly(lstat, 5), data = Boston)\r\r\n\r\r\nResiduals:\r\r\n    Min      1Q  Median      3Q     Max \r\r\n-13.682  -1.848  -0.547  -0.049  82.707 \r\r\n\r\r\nCoefficients:\r\r\n                Estimate Std. Error t value Pr(>|t|)    \r\r\n(Intercept)       3.6135     0.3368  10.729   <2e-16 ***\r\r\npoly(lstat, 5)1  88.0697     7.5758  11.625   <2e-16 ***\r\r\npoly(lstat, 5)2  15.8882     7.5758   2.097   0.0365 *  \r\r\npoly(lstat, 5)3 -11.5740     7.5758  -1.528   0.1272    \r\r\npoly(lstat, 5)4  15.5158     7.5758   2.048   0.0411 *  \r\r\npoly(lstat, 5)5  16.8403     7.5758   2.223   0.0267 *  \r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nResidual standard error: 7.576 on 500 degrees of freedom\r\r\nMultiple R-squared:  0.232, Adjusted R-squared:  0.2243 \r\r\nF-statistic:  30.2 on 5 and 500 DF,  p-value: < 2.2e-16\r\r\n\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(zn,5))) %>% summary()\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(indus,5))) %>% summary()\r\r\n#lm.poly <- lm(data = Boston, crim ~ poly(chas,2)) Won't let me run poly\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(nox,5))) %>% summary()\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(rm,5))) %>% summary()\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(age,5)))%>% summary()\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(dis,5))) %>% summary()\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(rad,5))) %>% summary()\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(tax,5)))%>% summary()\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(ptratio,5)))%>% summary()\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(black,5))) %>% summary()\r\r\n#(lm.poly <- lm(data = Boston, crim ~ poly(medv,5))) %>% summary()\r\r\n\r\r\n\r\r\n\r\r\nI originally ran the summar on each model using the polystat. I then commented the code after, since there is no need to have 14 summarys in a row on the html document.\r\r\nUsing Polystat, and observing the pvalues, here are the results : - lstat is linear - zn is linear, but could be quadratic - indus is statistically significant for 1,2,and 3 degrees -nox is nonlinear, with the 3 degree being the most significant -rm is linear -medv may be non linear\r\r\n\r\r\nISLR Ch. 4, exercise 4\r\r\nWhen the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that curse of dimensionality non-parametric approaches often perform poorly when p is large. We will now investigate this curse.\r\r\n(a) Instructors answer via Piazza helped answer this problem.\r\r\nSuppose that we have a set of observations, each with measurements on p = 1 feature, X. We assume that X is uniformly (evenly) distributed on [0, 1]. Associated with each observationis a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10 % of the range of X closest to that test observation. For instance, inorder to predict the response for a test observation with X = 0.6, we will use observations in the range [0.55, 0.65]. On average, what fraction of the available observations will we use to make the prediction?\r\r\n– p =1, x uniformly distributed Observations are in the range of .55 to .65. This means that observations .45 to .55 is just as likely. .65 -.55 = .1. 1/10 of the available observations we will use to make the predcition.\r\r\nB - Aanchal Setia from Piazza discussion\r\r\nNow suppose that we have a set of observations, each with measurements on p = 2 features, X1 and X2. We assume that (X1, X2) are uniformly distributed on [0, 1] × [0, 1]. We wish to predict a test observation’s response using only observations that are within 10 % of the range of X1 and within 10 % of the range of X2 closest to that test observation. For instance, in order to predict the response for a test observation with X1 = 0.6 and X2 = 0.35, we will use observations in the range [0.55, 0.65] for X1 and in the range [0.3, 0.4] for X2. On average, what fraction of the available observations will we use to make the prediction?\r\r\nFor x1 we will use 10% range, and for x2 we will also use a 10% range.Therefore, on average we will have 10% of 10% of observations.\r\r\n\r\r\n\r\r\n.1*.1\r\r\n\r\r\n\r\r\n[1] 0.01\r\r\n\r\r\nC\r\r\nNow suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10 % of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?\r\r\nP = 100 , range 0 to 1. Assuming that each response is using the 10% range p = 1 — .1 p =2 —- .1.1 p = 3 —- .1 .1 *.1 p = n —- .1^n\r\r\n\r\r\n\r\r\np = 100\r\r\n\r\r\n.1^p\r\r\n\r\r\n\r\r\n[1] 1e-100\r\r\n\r\r\nThere 1 E-100 of the available observations will be used to make the prediction.\r\r\nD\r\r\nUsing your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.\r\r\nBased on the above results, if we have 100 features, we would need an extremely large data set to get point that are with in the 10% range of each other.\r\r\nISLR Ch. 4, exercise 6\r\r\nSuppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, βˆ0 = −6, βˆ1 = 0.05, βˆ2 = 1.\r\r\n(a)\r\r\nEstimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.\r\r\nLogistic Funciton\r\r\n\r\r\nb_0 = -6\r\r\nb_1 = .05\r\r\nb_2 = 1\r\r\nx1 = 40\r\r\nx2 = 3.5\r\r\n\r\r\nprob = exp(b_0+b_1*x1+b_2*x2)/(1+exp(b_0+b_1*x1*1+b_2*x2))\r\r\nprob\r\r\n\r\r\n\r\r\n[1] 0.3775407\r\r\n\r\r\nThere is a 37% probablity of receiving an A in this class assuming a 3.5 gpa and 40 hours of studying.\r\r\n(b)\r\r\nHow many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?\r\r\nManipulate the above equaiton to solve for x1.\r\r\nLog Odds\r\r\n\r\r\nb_0 = -6\r\r\nb_1 = .05\r\r\nb_2 = 1\r\r\nprob = .5\r\r\nx2 = 3.5\r\r\n\r\r\nx1 = (log(prob/(1-prob))- b_0 - b_2*x2)/b_1\r\r\nx1\r\r\n\r\r\n\r\r\n[1] 50\r\r\n\r\r\nAssuming the 3.5 gpa, we will need to study 50 hours to have a probability of getting an A at 50%.\r\r\nISLR Ch. 4, exercise 9\r\r\nThis problem has to do with odds.\r\r\n(a)\r\r\nOn average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\r\r\nOdds = P(X)/1-P(X)  odds* (1-P(X)) = P(x)  odds - odds*p(x) = P(x) odds = p(x)(1+odds) p(x) = odds/(1+odds)\r\r\n\r\r\n\r\r\nodds = .37\r\r\nx  = odds/(1+odds)\r\r\nx\r\r\n\r\r\n\r\r\n[1] 0.270073\r\r\n\r\r\n.27 of people with an odds of .37 will default\r\r\n(b)\r\r\nSuppose that an individual has a 16 % chance of defaulting on her credit card payment. What are the odds that she will default? P(x) = .16 odds = P(x)/(1-P(x))\r\r\n\r\r\n\r\r\nx = .16\r\r\nodds = x/(1-x)\r\r\nodds\r\r\n\r\r\n\r\r\n[1] 0.1904762\r\r\n\r\r\n16 % of the people with an odds of .19 will default.\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": {},
    "last_modified": "2022-02-13T23:36:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/australian-weather-data/",
    "title": "Australian Weather Data",
    "description": "A glimpse into 10 years of Austrailian Weather Data",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-12",
    "categories": [],
    "contents": "\r\r\nData Description\r\r\nThis data set was found on Kagle.com. This data has 23 variables and 145,000 measurements. This data set has 10 years of daily weather observations from various locations in Australia. The data was published with the intent of using various weather indicators to predict whether it is raining or will rain the next day.Two columns, rain today and rain tomorrow are used as the target columns. If there was 1mm or more of rain, then that will count as “Yes”, meaning there was rain that day.\r\r\nBefore I start plotting, lets take a look at the variables we are working with. To do this, I will use the str function, sapply with sum is.na, and length with unique to count the locations.\r\r\nIt looks like we have a date column, a categorical column for locations, and continuous number type columns. It seems that the spread is pretty even for per location, With the largest amount of observations Canberra at 3436 and the lowest in Katherine, Nhil and Uluru at 1578. There are 49 cites in this data set. Also there are plenty of NA’s in this data set, so I’ll keep that in mind while thinking about which variables will help us predict rainfall in Australia. Due to the large amount of continuous variables, I will use histograms later on in the report.\r\r\nBefore we try the histograms, lets plot the top 10 cities by number of observations in a bar chart. \r\r\nVisuals\r\r\nHere are the top 10 cities based on rainfall: This time I will try to change the data set while piping directly into the plot.\r\r\n\r\r\n\r\r\nWeather_Data %>%\r\r\n  count(Location, sort = TRUE)%>%\r\r\n  slice(1:10)%>%\r\r\n  rename(Total_count = n)%>%\r\r\n  ggplot()+\r\r\n  geom_col(aes(x = Location, y =Total_count, fill = Location), show.legend = FALSE )+\r\r\n  geom_text(aes(x = Location, y =Total_count, label = Total_count), vjust = 3)+\r\r\n  theme_classic()+\r\r\n  labs(title = \"Rainfall Observations\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nWell that wasn’t very helpful. Lets get the total rainfall for the top 10 citys! This can be done by using a groupby and arrange by decending. Also it should be noted that when plotting with geo_col, the columns will be alphabetically plotted on the x axis. \r\r\n\r\r\n\r\r\nWeather_Data %>%\r\r\n group_by(Location)%>%\r\r\n  summarise( total_rain = sum(Rainfall, na.rm = TRUE))%>%\r\r\n  arrange(desc(total_rain))%>%\r\r\n  slice(1:10)%>%\r\r\n  ggplot()+\r\r\n  geom_col(aes(x = Location, y =total_rain, fill = Location), show.legend = FALSE )+\r\r\n  geom_text(aes(x = Location, y =total_rain, label = total_rain), hjust = 1.1)+\r\r\n  theme_classic()+\r\r\n  coord_flip()+\r\r\n  labs(title = \"Total Amount of Rain\")+\r\r\n  ylab(\"Total Rain fall (mm)\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nIT looks like Caims had the largest amount of Rainfall at 17,157.2 mm. Due to the overlapping x labels, I used coordflip to flip the x and y axis’s. I also wanted to rename the new x axis, and to do that I changed the ylab name. Even though the axis’s are flipped, Location is still seen as the x axis in ggplot.\r\r\nExploring\r\r\nI have a lot of continuous variables. To get the idea of their layout, lets try using some histograms. To get the most bang for buck, I also want to compare similar variables. Lets look at min and max temp recorded per observation. To make this data a little simpler, we are only going to look at the top 10 cities based on amount of rainfall recorded.To have two variables on one axis, I used the density function as the y variable in the histogram.\r\r\n\r\r\n\r\r\nrainfall_countries <- Weather_Data %>%\r\r\n group_by(Location)%>%\r\r\n  summarise(total_rain = sum(Rainfall, na.rm = TRUE))%>%\r\r\n  arrange(desc(total_rain))%>%\r\r\n  slice(1:10)\r\r\n\r\r\n \r\r\ncountryList <- unique(rainfall_countries$Location)\r\r\n\r\r\n\r\r\n\r\r\nTop_10 <-Weather_Data %>%\r\r\n  filter(Location %in% countryList)\r\r\n\r\r\n\r\r\n\r\r\nTop_10 %>% \r\r\n  ggplot()+\r\r\n  geom_histogram(aes(x = MinTemp, y= ..density.., color = Location) )+\r\r\n  annotate(\"text\", x = 3, y =.4, label = \"Min\")+\r\r\n  geom_histogram(aes(x = MaxTemp, y = -..density.., fill = Location))+\r\r\n  annotate(\"text\", x = 3, y =-.4, label = \"Max\")+\r\r\n  xlab(\"Temperature\")+\r\r\n  labs(title = \"Max vs Min Temperature\")\r\r\n\r\r\n\r\r\n\r\r\nTop_10 %>%\r\r\n  ggplot(aes(x = Humidity9am, y =..density..))+\r\r\n  geom_freqpoly(aes(color = Location))+\r\r\n  annotate(\"text\", x = 15, y = .025, label = \"9AM\")+\r\r\n  geom_freqpoly(aes(x = Humidity3pm, y = -..density.., color = Location))+\r\r\n  annotate(\"text\", x = 15, y = -.025, label = \"3PM\")+\r\r\n  xlab(\"Humidity\")+\r\r\n  labs(title = \"9AM vs 3PM Humidity\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nThe min vs max temps seem to have a normal distribution. It seems the most seen temperature for recorded mins is roughly 22 degrees celsius and 28 degrees celsius for max temp. The colors represent the top 10 cities based on their rainfall. I used two different ways to visualize the cities, it looks like the bottom histogram (Max temp) is a bit easier to visualize the cities. I also used annotate instead of geom_label, since geom_label was taking very long to run.\r\r\nThe bottom figure is a histogram using geom_freqpoly. Same idea for this one, I filtered by cities. It’s a nice visualization, though, a bit tough to tell the difference between each city. Mount Ginini has the highest recorded 9AM and 3PM humidity. This could be something to look into.\r\r\nBelow I’ve started to play around with Histograms and facet wraps:\r\r\n\r\r\n\r\r\nTop_10 %>% \r\r\n  ggplot()+\r\r\n  geom_histogram(aes(x = MinTemp))+\r\r\n  facet_wrap(~Location)\r\r\n\r\r\n\r\r\n\r\r\nTop_10 %>% \r\r\n  ggplot()+\r\r\n  geom_histogram(aes(x = MaxTemp))+\r\r\n  facet_wrap(~Location)\r\r\n\r\r\n\r\r\n\r\r\nTop_10 %>% \r\r\n  ggplot()+\r\r\n  geom_histogram(aes(x = MaxTemp))+\r\r\n  facet_wrap(~RainToday)+\r\r\n  labs(title = \"Max Temp vs Rain Today\")\r\r\n\r\r\n\r\r\n\r\r\nTop_10 %>% \r\r\n  ggplot()+\r\r\n  geom_histogram(aes(x = MaxTemp))+\r\r\n  facet_wrap(~RainTomorrow)+\r\r\n  labs(title = \"Max Temp vs Rain Tomorrow\")\r\r\n\r\r\n\r\r\n\r\r\nTop_10 %>% \r\r\n  ggplot()+\r\r\n  geom_histogram(aes(x = Humidity9am))+\r\r\n  facet_wrap(~RainToday)+\r\r\n  labs(title = \"9AM Humidity vs Rain Today\")\r\r\n\r\r\n\r\r\n\r\r\nTop_10 %>% \r\r\n  ggplot()+\r\r\n  geom_histogram(aes(x = Humidity9am))+\r\r\n  facet_wrap(~RainTomorrow)+\r\r\n  labs(title = \"9AM Humidity vs Rain Tomorrow\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nThe first two facet wraps (Min and Max Temp vs Location) just give us a little more insight into how the histograms above are compiled. I didn’t find these two charts too helpful. For Max Temp vs Rain Today/Tomorrow figures, I also found these not too helpful in whether predicting there will be rain or no rain.\r\r\nThe final two figures, 9AM humidity vs Rain Today/Tomorrow figures did seem to offer a bit of insight. It seems that if we are to expect rain that day or the next day, there needs to be a higher humidity. Days will low humidity in the morning have a higher probability of not raining today or tomorrow.\r\r\nFinal Figures\r\r\nI kind of feel like I’ve been going deeper and deeper down the rabbit hole. Each chart and figure leads me to a new idea, and is pushing me towards plotting and comparing other variables under different circumstances. I’ve only begun to scratch the surface of what this data set is offering.\r\r\nI know believe that humidity is a major factor in predicting rain. So know I will compare temp, humidity and whether it rained today or tomorrow using a facet grid.\r\r\nI will also take a closer look into the humidity of 9AM using a violin plot to confirm that Mount Ginini had the highest distribution of high humidity based on the other top 10 cities. \r\r\n\r\r\n\r\r\nTop_10 %>%\r\r\nggplot() + \r\r\n    geom_smooth(aes(x = Temp3pm, y = Humidity3pm, method = \"lm\", fill = \"red\")) + \r\r\n    facet_grid(RainToday ~ RainTomorrow, labeller = label_both)+\r\r\n  theme_get()+\r\r\n  labs(title = \"Temp and Humidity at 3 PM\")+\r\r\n  theme(legend.position = \"none\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nTop_10 %>%\r\r\n  ggplot(aes(x = Location, y = Humidity9am, fill = Location))+\r\r\n  coord_flip()+\r\r\n  geom_violin()+\r\r\n  ylab(\"9AM Humidity\")+\r\r\n  labs(title = \"Humidity Distribution by City\")+\r\r\n  xlab(\"\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nClosing Remarks\r\r\nIt looks like Temp is the lowest when the humidity is the highest. This seems kinda odd to me. There is definitely a higher temperature and humidity when there is rain today or rain tomorrow.\r\r\nThe violin plot did confirm that Mount Ginini had an unusual high distribution of humidity compared to the other cities. The violin plot is much easier to understand compared to the histograms and geom_freqpoly figures.\r\r\nI’m a bit disappointed by how little I was able to understand this data set. Even with out really understanding this data set, I had a great time learning how to use histograms, freqpoly, facet wraps, facet grid, and violin plots. Compared to other languages I studied, R is definitely the best I used when trying to visualize data! knitr::opts_chunk$set(echo = FALSE)\r\r\n\r\r\nDistill is a publication format for scientific and technical writing, native to the web. \r\r\n\r\r\nLearn more about using Distill for R Markdown at <https://rstudio.github.io/distill>.\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n```{.r .distill-force-highlighting-css}\r\r\n\r\r\n\r\r\n",
    "preview": "posts/australian-weather-data/distill-preview.png",
    "last_modified": "2022-02-12T17:28:54-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/hw-1-network-analysis/",
    "title": "HW 1 Network Analysis",
    "description": "A closer look into Airport data networks in the United States.",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-12",
    "categories": [],
    "contents": "\r\r\n\r\r\nLoad in DATA. This is the Airport DATA from the Google drive.\r\r\n\r\r\n\r\r\nload(file = \"USAirports.rda\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nLooking at Nodes and Edges:\r\r\n\r\r\n\r\r\nls()\r\r\n\r\r\n\r\r\n[1] \"network_edgelist\" \"network_igraph\"   \"network_nodes\"   \r\r\n[4] \"network_statnet\" \r\r\n\r\r\nvcount(network_igraph)\r\r\n\r\r\n\r\r\n[1] 755\r\r\n\r\r\necount(network_igraph)\r\r\n\r\r\n\r\r\n[1] 23473\r\r\n\r\r\nprint(network_statnet)\r\r\n\r\r\n\r\r\n Network attributes:\r\r\n  vertices = 755 \r\r\n  directed = TRUE \r\r\n  hyper = FALSE \r\r\n  loops = FALSE \r\r\n  multiple = FALSE \r\r\n  bipartite = FALSE \r\r\n  total edges= 8228 \r\r\n    missing edges= 0 \r\r\n    non-missing edges= 8228 \r\r\n\r\r\n Vertex attribute names: \r\r\n    City Distance vertex.names \r\r\n\r\r\n Edge attribute names not shown \r\r\n\r\r\n#print(network_igraph)\r\r\nplot(network_statnet)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nRight off the bat, it looks like the igraph and statnet variables are showing different edges. The network igraph is showing 755 nodes and 23473 edges. The network statnet is showing 755 nodes, and 8228 edges. \r\r\nWeighted, Directed, Single Mode Network?\r\r\n\r\r\n\r\r\nis_bipartite(network_igraph)\r\r\n\r\r\n\r\r\n[1] FALSE\r\r\n\r\r\nis_directed(network_igraph)\r\r\n\r\r\n\r\r\n[1] TRUE\r\r\n\r\r\nis_weighted(network_igraph)\r\r\n\r\r\n\r\r\n[1] FALSE\r\r\n\r\r\nUsing the Network Igraph set, we have a single mode network, which is directed, and is not weighted.\r\r\n\r\r\nLooking at Vertex and Edge Attributes:\r\r\n\r\r\n\r\r\nvertex_attr_names(network_igraph)\r\r\n\r\r\n\r\r\n[1] \"name\"     \"City\"     \"Position\"\r\r\n\r\r\nnetwork::list.vertex.attributes(network_statnet)\r\r\n\r\r\n\r\r\n[1] \"City\"         \"Distance\"     \"na\"           \"vertex.names\"\r\r\n\r\r\nedge_attr_names(network_igraph)\r\r\n\r\r\n\r\r\n[1] \"Carrier\"    \"Departures\" \"Seats\"      \"Passengers\" \"Aircraft\"  \r\r\n[6] \"Distance\"  \r\r\n\r\r\nnetwork::list.edge.attributes(network_statnet)\r\r\n\r\r\n\r\r\n[1] \"Aircraft\"   \"Carrier\"    \"Departures\" \"Distance\"   \"na\"        \r\r\n[6] \"Passangers\" \"Seats\"      \"weight\"    \r\r\n\r\r\nIgraph Attribute Names: name, City, Position\r\r\nIgraph edge names: Carrier, Departures, Seats, Passengers, Aircraft, Distance\r\r\nStatnet attribute names: City, Distance, na, vertex.names\r\r\nstatnet edge names: Aircraft, Carrier, Departures, Distance, na, Passangers, Seats, weight\r\r\n\r\r\nAccessing Attribute DATA:\r\r\n\r\r\n\r\r\nV(network_igraph)$name %>% head()\r\r\n\r\r\n\r\r\n[1] \"BGR\" \"BOS\" \"ANC\" \"JFK\" \"LAS\" \"MIA\"\r\r\n\r\r\nV(network_igraph)$City %>% head()\r\r\n\r\r\n\r\r\n[1] \"Bangor, ME\"    \"Boston, MA\"    \"Anchorage, AK\" \"New York, NY\" \r\r\n[5] \"Las Vegas, NV\" \"Miami, FL\"    \r\r\n\r\r\nV(network_igraph)$Position %>% head()\r\r\n\r\r\n\r\r\n[1] \"N444827 W0684941\" \"N422152 W0710019\" \"N611028 W1495947\"\r\r\n[4] \"N403823 W0734644\" \"N360449 W1150908\" \"N254736 W0801726\"\r\r\n\r\r\n(network_igraph)$Carrier %>% head()\r\r\n\r\r\n\r\r\nNULL\r\r\n\r\r\nhead(network_statnet %v% \"vertex.names\")\r\r\n\r\r\n\r\r\n[1] \"1G4\" \"A23\" \"A27\" \"A29\" \"ABE\" \"ABI\"\r\r\n\r\r\nhead(network_statnet %v% \"City\")\r\r\n\r\r\n\r\r\n[1] \"Bangor, ME\"    \"Boston, MA\"    \"Anchorage, AK\" \"New York, NY\" \r\r\n[5] \"Las Vegas, NV\" \"Miami, FL\"    \r\r\n\r\r\nhead(network_statnet %e% \"weight\")\r\r\n\r\r\n\r\r\n[1] \"193\"  \"253\"  \"141\"  \"3135\" \"4097\" \"1353\"\r\r\n\r\r\n\r\r\nSummarizing Attribute DATA\r\r\n\r\r\n\r\r\nsummary(E(network_igraph)$Distance)\r\r\n\r\r\n\r\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\r\n      0     223     496     639     903    6089 \r\r\n\r\r\nsummary(network_statnet %e% \"Distance\")\r\r\n\r\r\n\r\r\n   Length     Class      Mode \r\r\n     8228 character character \r\r\n\r\r\nThe way the summary function worked on the statnet set makes me think the statnet dataset is incorrectly set up at the moment.  #### Dyad Census\r\r\n\r\r\n\r\r\ndyad.census(network_igraph)\r\r\n\r\r\n\r\r\n$mut\r\r\n[1] 10449\r\r\n\r\r\n$asym\r\r\n[1] 2574\r\r\n\r\r\n$null\r\r\n[1] 271612\r\r\n\r\r\nsna::dyad.census(network_statnet)\r\r\n\r\r\n\r\r\n      Mut Asym   Null\r\r\n[1,] 3605 1018 280012\r\r\n\r\r\n\r\r\nTriad Census\r\r\n\r\r\n\r\r\ntriad.census(network_igraph)\r\r\n\r\r\n\r\r\n [1] 68169544   665870  2427052     1445     1289     2465    15322\r\r\n [8]    19171       91       39   114868      202      376      558\r\r\n[15]     6422    18671\r\r\n\r\r\nsna::triad.census(network_statnet)\r\r\n\r\r\n\r\r\n          003    012     102 021D 021U 021C  111D  111U 030T 030C\r\r\n[1,] 68169544 712579 2380343 1445 1289 2465 15322 19171   91   39\r\r\n        201 120D 120U 120C  210   300\r\r\n[1,] 114868  202  376  558 6422 18671\r\r\n\r\r\n\r\r\nTransivity\r\r\n\r\r\n\r\r\ntransitivity(network_igraph)\r\r\n\r\r\n\r\r\n[1] 0.3384609\r\r\n\r\r\ngtrans(network_statnet)\r\r\n\r\r\n\r\r\n[1] 0.3266617\r\r\n\r\r\nThe transitivity for igraph and statnet data sets were pretty close. \r\r\nLocal Transivity\r\r\n\r\r\n\r\r\nfirst_five_names <- V(network_igraph)$name %>% head(5)\r\r\nfirst_five_names\r\r\n\r\r\n\r\r\n[1] \"BGR\" \"BOS\" \"ANC\" \"JFK\" \"LAS\"\r\r\n\r\r\nfirst_five_transivity <- transitivity(network_igraph, type = \"local\", vids = V(network_igraph)[first_five_names])\r\r\n\r\r\ncbind(first_five_names,first_five_transivity)\r\r\n\r\r\n\r\r\n     first_five_names first_five_transivity\r\r\n[1,] \"BGR\"            \"0.581818181818182\"  \r\r\n[2,] \"BOS\"            \"0.35292389068469\"   \r\r\n[3,] \"ANC\"            \"0.0824960338445267\" \r\r\n[4,] \"JFK\"            \"0.385964912280702\"  \r\r\n[5,] \"LAS\"            \"0.223852116875373\"  \r\r\n\r\r\ntransitivity(network_igraph, type = \"global\")\r\r\n\r\r\n\r\r\n[1] 0.3384609\r\r\n\r\r\ntransitivity(network_igraph, type = \"average\")\r\r\n\r\r\n\r\r\n[1] 0.6452844\r\r\n\r\r\nLA seems to have low transivity while BGR has the highest at .58.\r\r\n\r\r\nDistances in the Network\r\r\n\r\r\n\r\r\ndistances(network_igraph, \"BGR\",\"BOS\")\r\r\n\r\r\n\r\r\n    BOS\r\r\nBGR   1\r\r\n\r\r\ndistances(network_igraph,\"BOS\", \"ANC\")\r\r\n\r\r\n\r\r\n    ANC\r\r\nBOS   2\r\r\n\r\r\naverage.path.length(network_igraph)\r\r\n\r\r\n\r\r\n[1] 3.52743\r\r\n\r\r\naverage.path.length(network_igraph, directed = F)\r\r\n\r\r\n\r\r\n[1] 3.447169\r\r\n\r\r\n\r\r\nIdentifying Isolates\r\r\n\r\r\n\r\r\nnames(igraph::components(network_igraph))\r\r\n\r\r\n\r\r\n[1] \"membership\" \"csize\"      \"no\"        \r\r\n\r\r\ncomponents(network_igraph)$no\r\r\n\r\r\n\r\r\n[1] 6\r\r\n\r\r\ncomponents(network_igraph)$csize\r\r\n\r\r\n\r\r\n[1] 745   2   2   3   2   1\r\r\n\r\r\ncomponents(network_igraph)$membership %>% head()\r\r\n\r\r\n\r\r\nBGR BOS ANC JFK LAS MIA \r\r\n  1   1   1   1   1   1 \r\r\n\r\r\n#Isolates\r\r\nisolates(network_statnet)\r\r\n\r\r\n\r\r\n[1] 166\r\r\n\r\r\nas.vector(network_statnet %v% \"vertex.names\")[c(isolates(network_statnet))]\r\r\n\r\r\n\r\r\n[1] \"DET\"\r\r\n\r\r\nDetroit seems to be the only isolate.\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/hw-1-network-analysis/distill-preview.png",
    "last_modified": "2022-02-12T11:30:07-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/hw-5-ufo-search/",
    "title": "HW 5: UFO Search!",
    "description": "This document focuses on the UFO sightings in the United states over the last 70 years.",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-12",
    "categories": [],
    "contents": "\r\nData Description:\r\nThis Data is published by the NUFORC, the National UFO Reporting Center. This data set was published roughly one year ago. When looking at the UFO data set, there are 88875 observations, and 11 variables. This data set was posted on kagle.com.\r\nHere are the column names: datetime, city, state, country, shape, duration (seconds), duration (hours/min), comments, date posted, latitude, longitude\r\nLets look into the data. When I read in the data set using read.csv, I used na.strings = \"\" to replace any blanks with NA. The first step thing I want to konw is how many blanks for each variable. To find this out I used the sapply, function, sum, and is.na to sum the number of NA’s in the data. I also plan to str() function, to see the data types of the variables. Letosttes see our results:\r\nMethod\r\n\r\n\r\nsapply(UFO_data, function(Count) sum(is.na(Count)))\r\n\r\n\r\n            datetime                 city                state \r\n                   0                  196                 7519 \r\n             country                shape   duration (seconds) \r\n               12561                 3118                    5 \r\nduration (hours/min)             comments          date posted \r\n                3101                  126                    0 \r\n            latitude            longitude \r\n                   0                    0 \r\n\r\n\r\n\r\n\r\nUS_data <- UFO_data %>%\r\n  filter(country == \"us\") \r\n\r\n\r\nUS_data %>% \r\n  select(state) %>%\r\ntable() %>%\r\n  sort(decreasing =TRUE)\r\n\r\n\r\n.\r\n  ca   wa   fl   tx   ny   il   az   pa   oh   mi   or   nc   mo   co \r\n9575 4292 4155 3742 3234 2698 2617 2520 2464 1980 1882 1863 1569 1521 \r\n  in   va   ga   nj   ma   wi   tn   mn   sc   ct   ky   md   nv   nm \r\n1404 1381 1359 1352 1341 1309 1228 1096 1078  971  931  890  858  794 \r\n  ok   ia   al   ut   ks   ar   me   la   id   mt   nh   wv   ne   ms \r\n 779  735  706  659  652  642  599  597  566  531  517  493  415  414 \r\n  ak   vt   hi   ri   sd   wy   de   nd   pr   dc \r\n 341  283  276  244  196  191  180  140   26    7 \r\n\r\nstr(US_data)\r\n\r\n\r\nspec_tbl_df [70,293 x 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ datetime            : chr [1:70293] \"10/10/1949 20:30\" \"10/10/1956 21:00\" \"10/10/1960 20:00\" \"10/10/1961 19:00\" ...\r\n $ city                : chr [1:70293] \"san marcos\" \"edna\" \"kaneohe\" \"bristol\" ...\r\n $ state               : chr [1:70293] \"tx\" \"tx\" \"hi\" \"tn\" ...\r\n $ country             : chr [1:70293] \"us\" \"us\" \"us\" \"us\" ...\r\n $ shape               : chr [1:70293] \"cylinder\" \"circle\" \"light\" \"sphere\" ...\r\n $ duration (seconds)  : num [1:70293] 2700 20 900 300 1200 180 120 300 180 1800 ...\r\n $ duration (hours/min): chr [1:70293] \"45 minutes\" \"1/2 hour\" \"15 minutes\" \"5 minutes\" ...\r\n $ comments            : chr [1:70293] \"This event took place in early fall around 1949-50. It occurred after a Boy Scout meeting in the Baptist Church\"| __truncated__ \"My older brother and twin sister were leaving the only Edna theater at about 9 PM&#44...we had our bikes and I \"| __truncated__ \"AS a Marine 1st Lt. flying an FJ4B fighter/attack aircraft on a solo night exercise&#44 I was at 50&#44000&#39 \"| __truncated__ \"My father is now 89 my brother 52 the girl with us now 51 myself 49 and the other fellow which worked with my f\"| __truncated__ ...\r\n $ date posted         : chr [1:70293] \"4/27/2004\" \"1/17/2004\" \"1/22/2004\" \"4/27/2007\" ...\r\n $ latitude            : chr [1:70293] \"29.8830556\" \"28.9783333\" \"21.4180556\" \"36.5950000\" ...\r\n $ longitude           : num [1:70293] -97.9 -96.6 -157.8 -82.2 -73.4 ...\r\n - attr(*, \"problems\")= tibble [199 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ row     : int [1:199] 877 1712 1814 2857 3733 4755 5388 5422 5613 5848 ...\r\n  ..$ col     : chr [1:199] NA NA NA NA ...\r\n  ..$ expected: chr [1:199] \"11 columns\" \"11 columns\" \"11 columns\" \"11 columns\" ...\r\n  ..$ actual  : chr [1:199] \"12 columns\" \"12 columns\" \"12 columns\" \"12 columns\" ...\r\n  ..$ file    : chr [1:199] \"'complete.csv'\" \"'complete.csv'\" \"'complete.csv'\" \"'complete.csv'\" ...\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   datetime = col_character(),\r\n  ..   city = col_character(),\r\n  ..   state = col_character(),\r\n  ..   country = col_character(),\r\n  ..   shape = col_character(),\r\n  ..   `duration (seconds)` = col_double(),\r\n  ..   `duration (hours/min)` = col_character(),\r\n  ..   comments = col_character(),\r\n  ..   `date posted` = col_character(),\r\n  ..   latitude = col_character(),\r\n  ..   longitude = col_double()\r\n  .. )\r\n\r\nThere is a lot to look into. First, It seems like the country column has the most amount of NA’s. 12,757 out of the 89,000 data points. DateTime is consistent with 0 NA’s. I filtered the data down to US DATA, and created a table showing the count per state of UFO sighting. From the table, California has the largest amount of sightings (9575), so I am going to filter further into California.\r\nWhen looking at the result of the str() function, I’ve noticed the the latitude column is a character type. This is going to cause trouble when trying to plot the location of each sighting. I’m going to use mutate to create a new column that is numeric. After I filter down to California, I will also create a table of the count per shape per sighting in California. \r\n\r\n\r\nlibrary(knitr)\r\nUS_data <- US_data %>%\r\n  mutate(new_latitude = as.numeric(latitude))\r\n\r\n\r\nCalifornia_data <- US_data %>%\r\n  filter(state == \"ca\")\r\n\r\n\r\n\r\nCalifornia_data %>%\r\n  select(shape)%>%\r\n  table()%>%\r\n  sort(decreasing = TRUE)\r\n\r\n\r\n.\r\n    light    circle  triangle  fireball      disk     other    sphere \r\n     1937       915       863       732       685       663       642 \r\n  unknown      oval formation  changing     cigar   chevron rectangle \r\n      613       416       312       294       208       158       155 \r\n    flash   diamond  cylinder       egg  teardrop     cross      cone \r\n      153       143       130        96        77        34        32 \r\n    flare     round \r\n        1         1 \r\n\r\nIt Looks like in California, The shape most seen is the light and circle, with triangle fire ball and disk. \r\nVisuals\r\nNow, lets create some visuals. Since there are so many shapes, let’s use Count and Slice to grab and plot the top 5 shapes in California.\r\n\r\n\r\ntop_5_shapes <- California_data %>%\r\n  count(shape, sort = TRUE) %>%\r\n  slice(1:5)\r\ntop_5_shapes <- rename(top_5_shapes, Total_Count = n)\r\n\r\n\r\n\r\ntop_5_shapes %>%\r\nggplot()+\r\ngeom_col(aes(x = shape, y =Total_Count, fill = shape), show.legend = FALSE )+\r\n  geom_text(aes(x = shape, y =Total_Count, label = Total_Count), vjust = 3)+\r\n  theme_classic()+\r\n  labs(title = \"Sighting by Shape in California\")\r\n\r\n\r\n\r\n\r\nSince the data was already counted using the count function, I had to use geom_col instead of geom_bar to plot the visual.\r\n\r\nMapping\r\nNow lets look into the positional data, specifically the latitude and longitude columns. As I mentioned before, I had to create a new column named “New_latitude” that was numeric instead of character type that was provided.\r\nTo plot this locational data, I plan to use ggmpa. To use ggmap, I had to create a free account with google cloud. I then had to request a key to the static map api. This allowed me to get the map that I would be plotting the location data onto.\r\n\r\n\r\n#register_google(key = \"Your API KEY\")\r\n#map <- get_googlemap(center = c(lon = -119.4179, lat = 36.7783),\r\n             #       zoom = 6, scale = 2,\r\n             #       maptype ='roadmap',\r\n              #      color = \"color\")\r\n\r\n#ggmap(map) + geom_point(data = US_data, aes(x = longitude, y = new_latitude ), size = .2) \r\n\r\n#map(\"state\", \".*california\", myborder = 0)  For one state.\r\ncalifornia_map <- map(\"state\", region = c(\"California\", \"Oregon\", \"Arizona\", \"Nevada\"), col = \"gray90\", fill = TRUE)\r\n\r\n\r\n\r\nhead(US_data)\r\n\r\n\r\n# A tibble: 6 x 12\r\n  datetime city  state country shape `duration (seco~ `duration (hour~\r\n  <chr>    <chr> <chr> <chr>   <chr>            <dbl> <chr>           \r\n1 10/10/1~ san ~ tx    us      cyli~             2700 45 minutes      \r\n2 10/10/1~ edna  tx    us      circ~               20 1/2 hour        \r\n3 10/10/1~ kane~ hi    us      light              900 15 minutes      \r\n4 10/10/1~ bris~ tn    us      sphe~              300 5 minutes       \r\n5 10/10/1~ norw~ ct    us      disk              1200 20 minutes      \r\n6 10/10/1~ pell~ al    us      disk               180 3  minutes      \r\n# ... with 5 more variables: comments <chr>, date posted <chr>,\r\n#   latitude <chr>, longitude <dbl>, new_latitude <dbl>\r\n\r\nplot(california_map)\r\n\r\n\r\n\r\n#ggplot(california_map, aes(long, lat))+\r\n  geom_point(aes(US_data$longitude,US_data$new_latitude))\r\n\r\n\r\nmapping: x = ~US_data$longitude, y = ~US_data$new_latitude \r\ngeom_point: na.rm = FALSE\r\nstat_identity: na.rm = FALSE\r\nposition_identity \r\n\r\n  geom_polygon(data = california_map)\r\n\r\n\r\ngeom_polygon: na.rm = FALSE, rule = evenodd\r\nstat_identity: na.rm = FALSE\r\nposition_identity \r\n\r\n#US_data %>%\r\n  #points(longitude, new_latitude, col = \"red\")\r\n\r\n\r\n\r\nFrom a Quick glimpse, most of the sightings are from San Franciso, LA, and San Diego. Lets see exactly how many sightings there are from those specific cities. To acomplish this, I will do the same method as above, create a dataframe of the top 5 citys by count.\r\n\r\n\r\n\r\ntop_5_citys <- California_data %>%\r\n  count(city, sort = TRUE) %>%\r\n  slice(1:5)\r\ntop_5_citys <- rename(top_5_citys, Total_Count = n)\r\n\r\n\r\n\r\ntop_5_citys %>%\r\nggplot()+\r\ngeom_col(aes(x = city, y =Total_Count, fill = city), show.legend = FALSE)+\r\n  geom_text(aes(x = city, y = Total_Count, label = Total_Count), vjust = 3)+\r\n  theme_get()+\r\n  labs(title = \"Sighting by City in California\")\r\n\r\n\r\n\r\n\r\nSurprisingly It looks like Sacramento actually has more sightings from San Francisco. LA and San Diego Still have the most sightings by far.\r\n\r\nTimeline\r\nThe last item I would like to look into is the datetime column. I’m curious to see if I can find out if there is a specific time of the year when more people report UFO sightings. Perhaps there’s a season where Californians will see more UFO’s than others.\r\nFrom a quick glance, it looks like the datetime column is a char type, so I’ll first have to change it to a datetime. I then plan to add new columns for for month, day and year.\r\n\r\n\r\nCalifornia_data <- California_data %>%\r\n  mutate(new_datetime = as.Date(datetime,format = \"%m/%d/%Y\"))\r\n\r\nhead(select(California_data,new_datetime, datetime))\r\n\r\n\r\n# A tibble: 6 x 2\r\n  new_datetime datetime        \r\n  <date>       <chr>           \r\n1 1968-10-10   10/10/1968 13:00\r\n2 1979-10-10   10/10/1979 22:00\r\n3 1989-10-10   10/10/1989 00:00\r\n4 1995-10-10   10/10/1995 22:40\r\n5 1998-10-10   10/10/1998 02:30\r\n6 1999-10-10   10/10/1999 00:01\r\n\r\nThe New datetime looks like it worked based off the first six rows. Lets just check the tail as well just in-case.\r\n\r\n\r\ntail(select(California_data,new_datetime, datetime))\r\n\r\n\r\n# A tibble: 6 x 2\r\n  new_datetime datetime      \r\n  <date>       <chr>         \r\n1 2012-09-09   9/9/2012 13:00\r\n2 2012-09-09   9/9/2012 20:00\r\n3 2012-09-09   9/9/2012 20:30\r\n4 2012-09-09   9/9/2012 21:00\r\n5 2013-09-09   9/9/2013 09:51\r\n6 2013-09-09   9/9/2013 22:00\r\n\r\nOK. Everything looks fine. Now I am going to create a month column from the new_datetime column.\r\n\r\n\r\nCalifornia_data <- California_data %>%\r\n  mutate(month_column = month(new_datetime),\r\n         year_column = year(new_datetime),\r\n         day_column = day(new_datetime))\r\nhead(select(California_data,datetime, new_datetime, month_column, year_column, day_column))\r\n\r\n\r\n# A tibble: 6 x 5\r\n  datetime         new_datetime month_column year_column day_column\r\n  <chr>            <date>              <dbl>       <dbl>      <int>\r\n1 10/10/1968 13:00 1968-10-10             10        1968         10\r\n2 10/10/1979 22:00 1979-10-10             10        1979         10\r\n3 10/10/1989 00:00 1989-10-10             10        1989         10\r\n4 10/10/1995 22:40 1995-10-10             10        1995         10\r\n5 10/10/1998 02:30 1998-10-10             10        1998         10\r\n6 10/10/1999 00:01 1999-10-10             10        1999         10\r\n\r\nCalifornia_data <- California_data %>%\r\n  mutate(month = case_when(\r\n    month_column == 1 ~ \"Jan\",\r\n    month_column == 2 ~ \"Feb\",\r\n    month_column == 3 ~ \"Mar\",\r\n    month_column == 4 ~ \"Apr\",\r\n    month_column == 5 ~ \"May\",\r\n    month_column == 6 ~ \"Jun\",\r\n    month_column == 7 ~ \"Jul\",\r\n    month_column == 8 ~ \"Aug\",\r\n    month_column == 9 ~ \"Sep\",\r\n    month_column == 10 ~ \"Oct\",\r\n    month_column == 11 ~ \"Nov\",\r\n    month_column == 12 ~ \"Dec\"\r\n  ))\r\n\r\n\r\nMonth_data <- California_data %>%\r\n  count(month) \r\n\r\nMonth_data <- rename(Month_data, Sightings = n)\r\nMonth_data\r\n\r\n\r\n# A tibble: 12 x 2\r\n   month Sightings\r\n * <chr>     <int>\r\n 1 Apr         697\r\n 2 Aug         962\r\n 3 Dec         708\r\n 4 Feb         639\r\n 5 Jan         844\r\n 6 Jul         943\r\n 7 Jun         910\r\n 8 Mar         724\r\n 9 May         647\r\n10 Nov         875\r\n11 Oct         804\r\n12 Sep         822\r\n\r\nMonth_data$month <- factor(Month_data$month, levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\",\"May\",\r\n                                                        \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\r\n                                                        \"Nov\",\"Dec\"))\r\n\r\n\r\n\r\nMonth_data%>%\r\n  ggplot()+\r\n  geom_col(aes(x = month, y = Sightings, fill = month), show.legend = FALSE)+  \r\n  geom_text(aes(x = month, y =Sightings, label = Sightings), vjust = 3)+\r\n  theme_classic()+\r\n  labs(title = \"Sighting by Month in California\")\r\n\r\n\r\n\r\n\r\nResults\r\nAlright. There were a few steps that I had to go through in order to create the Sightings by Month visual. I first used mutate to create month, day, and year variables from the new_datetime column. Then I used the case_when and mutate to change the months from a number to a string (Aug, Sep, etc…). I then created an object using count, called month data. I did this because as of now, I am unaware how to use geom text with geom_bar. Geom_text with geom_col works fine. Since the months are now characters, when I plot them, they will be out of order. To solve this problem, I used factor to set up levels in the month column. After that, the data was ready to plot.\r\nBased on this data set, if one was looking to see a UFO, I would recommend heading to San Diego, LA, or Sacramento during the Summer! If I was going to look deeper into this data set, I would look closer into the time of when these UFO’s are spotted.\r\n\r\n\r\n\r\n",
    "preview": "posts/hw-5-ufo-search/distill-preview.png",
    "last_modified": "2022-02-12T13:08:09-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/stock-price-predictions/",
    "title": "Stock Price Predictions",
    "description": "Predicting Stock Prices using linear Regression.",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-12",
    "categories": [],
    "contents": "\r\r\n\r\r\n\r\r\nStock = \"AAPL\"\r\r\n\r\r\nstart_date <- Sys.Date()\r\r\nstart_date\r\r\n\r\r\n\r\r\n[1] \"2022-02-12\"\r\r\n\r\r\nretrieval_date <- start_date - years(4)\r\r\n\r\r\nStock_data <- tq_get(Stock, get = \"stock.prices\", from = retrieval_date, to = start_date)\r\r\n\r\r\nStock_data %>% datatable()\r\r\n\r\r\n\r\r\n\r\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"261\",\"262\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"272\",\"273\",\"274\",\"275\",\"276\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"286\",\"287\",\"288\",\"289\",\"290\",\"291\",\"292\",\"293\",\"294\",\"295\",\"296\",\"297\",\"298\",\"299\",\"300\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"313\",\"314\",\"315\",\"316\",\"317\",\"318\",\"319\",\"320\",\"321\",\"322\",\"323\",\"324\",\"325\",\"326\",\"327\",\"328\",\"329\",\"330\",\"331\",\"332\",\"333\",\"334\",\"335\",\"336\",\"337\",\"338\",\"339\",\"340\",\"341\",\"342\",\"343\",\"344\",\"345\",\"346\",\"347\",\"348\",\"349\",\"350\",\"351\",\"352\",\"353\",\"354\",\"355\",\"356\",\"357\",\"358\",\"359\",\"360\",\"361\",\"362\",\"363\",\"364\",\"365\",\"366\",\"367\",\"368\",\"369\",\"370\",\"371\",\"372\",\"373\",\"374\",\"375\",\"376\",\"377\",\"378\",\"379\",\"380\",\"381\",\"382\",\"383\",\"384\",\"385\",\"386\",\"387\",\"388\",\"389\",\"390\",\"391\",\"392\",\"393\",\"394\",\"395\",\"396\",\"397\",\"398\",\"399\",\"400\",\"401\",\"402\",\"403\",\"404\",\"405\",\"406\",\"407\",\"408\",\"409\",\"410\",\"411\",\"412\",\"413\",\"414\",\"415\",\"416\",\"417\",\"418\",\"419\",\"420\",\"421\",\"422\",\"423\",\"424\",\"425\",\"426\",\"427\",\"428\",\"429\",\"430\",\"431\",\"432\",\"433\",\"434\",\"435\",\"436\",\"437\",\"438\",\"439\",\"440\",\"441\",\"442\",\"443\",\"444\",\"445\",\"446\",\"447\",\"448\",\"449\",\"450\",\"451\",\"452\",\"453\",\"454\",\"455\",\"456\",\"457\",\"458\",\"459\",\"460\",\"461\",\"462\",\"463\",\"464\",\"465\",\"466\",\"467\",\"468\",\"469\",\"470\",\"471\",\"472\",\"473\",\"474\",\"475\",\"476\",\"477\",\"478\",\"479\",\"480\",\"481\",\"482\",\"483\",\"484\",\"485\",\"486\",\"487\",\"488\",\"489\",\"490\",\"491\",\"492\",\"493\",\"494\",\"495\",\"496\",\"497\",\"498\",\"499\",\"500\",\"501\",\"502\",\"503\",\"504\",\"505\",\"506\",\"507\",\"508\",\"509\",\"510\",\"511\",\"512\",\"513\",\"514\",\"515\",\"516\",\"517\",\"518\",\"519\",\"520\",\"521\",\"522\",\"523\",\"524\",\"525\",\"526\",\"527\",\"528\",\"529\",\"530\",\"531\",\"532\",\"533\",\"534\",\"535\",\"536\",\"537\",\"538\",\"539\",\"540\",\"541\",\"542\",\"543\",\"544\",\"545\",\"546\",\"547\",\"548\",\"549\",\"550\",\"551\",\"552\",\"553\",\"554\",\"555\",\"556\",\"557\",\"558\",\"559\",\"560\",\"561\",\"562\",\"563\",\"564\",\"565\",\"566\",\"567\",\"568\",\"569\",\"570\",\"571\",\"572\",\"573\",\"574\",\"575\",\"576\",\"577\",\"578\",\"579\",\"580\",\"581\",\"582\",\"583\",\"584\",\"585\",\"586\",\"587\",\"588\",\"589\",\"590\",\"591\",\"592\",\"593\",\"594\",\"595\",\"596\",\"597\",\"598\",\"599\",\"600\",\"601\",\"602\",\"603\",\"604\",\"605\",\"606\",\"607\",\"608\",\"609\",\"610\",\"611\",\"612\",\"613\",\"614\",\"615\",\"616\",\"617\",\"618\",\"619\",\"620\",\"621\",\"622\",\"623\",\"624\",\"625\",\"626\",\"627\",\"628\",\"629\",\"630\",\"631\",\"632\",\"633\",\"634\",\"635\",\"636\",\"637\",\"638\",\"639\",\"640\",\"641\",\"642\",\"643\",\"644\",\"645\",\"646\",\"647\",\"648\",\"649\",\"650\",\"651\",\"652\",\"653\",\"654\",\"655\",\"656\",\"657\",\"658\",\"659\",\"660\",\"661\",\"662\",\"663\",\"664\",\"665\",\"666\",\"667\",\"668\",\"669\",\"670\",\"671\",\"672\",\"673\",\"674\",\"675\",\"676\",\"677\",\"678\",\"679\",\"680\",\"681\",\"682\",\"683\",\"684\",\"685\",\"686\",\"687\",\"688\",\"689\",\"690\",\"691\",\"692\",\"693\",\"694\",\"695\",\"696\",\"697\",\"698\",\"699\",\"700\",\"701\",\"702\",\"703\",\"704\",\"705\",\"706\",\"707\",\"708\",\"709\",\"710\",\"711\",\"712\",\"713\",\"714\",\"715\",\"716\",\"717\",\"718\",\"719\",\"720\",\"721\",\"722\",\"723\",\"724\",\"725\",\"726\",\"727\",\"728\",\"729\",\"730\",\"731\",\"732\",\"733\",\"734\",\"735\",\"736\",\"737\",\"738\",\"739\",\"740\",\"741\",\"742\",\"743\",\"744\",\"745\",\"746\",\"747\",\"748\",\"749\",\"750\",\"751\",\"752\",\"753\",\"754\",\"755\",\"756\",\"757\",\"758\",\"759\",\"760\",\"761\",\"762\",\"763\",\"764\",\"765\",\"766\",\"767\",\"768\",\"769\",\"770\",\"771\",\"772\",\"773\",\"774\",\"775\",\"776\",\"777\",\"778\",\"779\",\"780\",\"781\",\"782\",\"783\",\"784\",\"785\",\"786\",\"787\",\"788\",\"789\",\"790\",\"791\",\"792\",\"793\",\"794\",\"795\",\"796\",\"797\",\"798\",\"799\",\"800\",\"801\",\"802\",\"803\",\"804\",\"805\",\"806\",\"807\",\"808\",\"809\",\"810\",\"811\",\"812\",\"813\",\"814\",\"815\",\"816\",\"817\",\"818\",\"819\",\"820\",\"821\",\"822\",\"823\",\"824\",\"825\",\"826\",\"827\",\"828\",\"829\",\"830\",\"831\",\"832\",\"833\",\"834\",\"835\",\"836\",\"837\",\"838\",\"839\",\"840\",\"841\",\"842\",\"843\",\"844\",\"845\",\"846\",\"847\",\"848\",\"849\",\"850\",\"851\",\"852\",\"853\",\"854\",\"855\",\"856\",\"857\",\"858\",\"859\",\"860\",\"861\",\"862\",\"863\",\"864\",\"865\",\"866\",\"867\",\"868\",\"869\",\"870\",\"871\",\"872\",\"873\",\"874\",\"875\",\"876\",\"877\",\"878\",\"879\",\"880\",\"881\",\"882\",\"883\",\"884\",\"885\",\"886\",\"887\",\"888\",\"889\",\"890\",\"891\",\"892\",\"893\",\"894\",\"895\",\"896\",\"897\",\"898\",\"899\",\"900\",\"901\",\"902\",\"903\",\"904\",\"905\",\"906\",\"907\",\"908\",\"909\",\"910\",\"911\",\"912\",\"913\",\"914\",\"915\",\"916\",\"917\",\"918\",\"919\",\"920\",\"921\",\"922\",\"923\",\"924\",\"925\",\"926\",\"927\",\"928\",\"929\",\"930\",\"931\",\"932\",\"933\",\"934\",\"935\",\"936\",\"937\",\"938\",\"939\",\"940\",\"941\",\"942\",\"943\",\"944\",\"945\",\"946\",\"947\",\"948\",\"949\",\"950\",\"951\",\"952\",\"953\",\"954\",\"955\",\"956\",\"957\",\"958\",\"959\",\"960\",\"961\",\"962\",\"963\",\"964\",\"965\",\"966\",\"967\",\"968\",\"969\",\"970\",\"971\",\"972\",\"973\",\"974\",\"975\",\"976\",\"977\",\"978\",\"979\",\"980\",\"981\",\"982\",\"983\",\"984\",\"985\",\"986\",\"987\",\"988\",\"989\",\"990\",\"991\",\"992\",\"993\",\"994\",\"995\",\"996\",\"997\",\"998\",\"999\",\"1000\",\"1001\",\"1002\",\"1003\",\"1004\",\"1005\",\"1006\",\"1007\",\"1008\",\"1009\"],[\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\",\"AAPL\"],[\"2018-02-12\",\"2018-02-13\",\"2018-02-14\",\"2018-02-15\",\"2018-02-16\",\"2018-02-20\",\"2018-02-21\",\"2018-02-22\",\"2018-02-23\",\"2018-02-26\",\"2018-02-27\",\"2018-02-28\",\"2018-03-01\",\"2018-03-02\",\"2018-03-05\",\"2018-03-06\",\"2018-03-07\",\"2018-03-08\",\"2018-03-09\",\"2018-03-12\",\"2018-03-13\",\"2018-03-14\",\"2018-03-15\",\"2018-03-16\",\"2018-03-19\",\"2018-03-20\",\"2018-03-21\",\"2018-03-22\",\"2018-03-23\",\"2018-03-26\",\"2018-03-27\",\"2018-03-28\",\"2018-03-29\",\"2018-04-02\",\"2018-04-03\",\"2018-04-04\",\"2018-04-05\",\"2018-04-06\",\"2018-04-09\",\"2018-04-10\",\"2018-04-11\",\"2018-04-12\",\"2018-04-13\",\"2018-04-16\",\"2018-04-17\",\"2018-04-18\",\"2018-04-19\",\"2018-04-20\",\"2018-04-23\",\"2018-04-24\",\"2018-04-25\",\"2018-04-26\",\"2018-04-27\",\"2018-04-30\",\"2018-05-01\",\"2018-05-02\",\"2018-05-03\",\"2018-05-04\",\"2018-05-07\",\"2018-05-08\",\"2018-05-09\",\"2018-05-10\",\"2018-05-11\",\"2018-05-14\",\"2018-05-15\",\"2018-05-16\",\"2018-05-17\",\"2018-05-18\",\"2018-05-21\",\"2018-05-22\",\"2018-05-23\",\"2018-05-24\",\"2018-05-25\",\"2018-05-29\",\"2018-05-30\",\"2018-05-31\",\"2018-06-01\",\"2018-06-04\",\"2018-06-05\",\"2018-06-06\",\"2018-06-07\",\"2018-06-08\",\"2018-06-11\",\"2018-06-12\",\"2018-06-13\",\"2018-06-14\",\"2018-06-15\",\"2018-06-18\",\"2018-06-19\",\"2018-06-20\",\"2018-06-21\",\"2018-06-22\",\"2018-06-25\",\"2018-06-26\",\"2018-06-27\",\"2018-06-28\",\"2018-06-29\",\"2018-07-02\",\"2018-07-03\",\"2018-07-05\",\"2018-07-06\",\"2018-07-09\",\"2018-07-10\",\"2018-07-11\",\"2018-07-12\",\"2018-07-13\",\"2018-07-16\",\"2018-07-17\",\"2018-07-18\",\"2018-07-19\",\"2018-07-20\",\"2018-07-23\",\"2018-07-24\",\"2018-07-25\",\"2018-07-26\",\"2018-07-27\",\"2018-07-30\",\"2018-07-31\",\"2018-08-01\",\"2018-08-02\",\"2018-08-03\",\"2018-08-06\",\"2018-08-07\",\"2018-08-08\",\"2018-08-09\",\"2018-08-10\",\"2018-08-13\",\"2018-08-14\",\"2018-08-15\",\"2018-08-16\",\"2018-08-17\",\"2018-08-20\",\"2018-08-21\",\"2018-08-22\",\"2018-08-23\",\"2018-08-24\",\"2018-08-27\",\"2018-08-28\",\"2018-08-29\",\"2018-08-30\",\"2018-08-31\",\"2018-09-04\",\"2018-09-05\",\"2018-09-06\",\"2018-09-07\",\"2018-09-10\",\"2018-09-11\",\"2018-09-12\",\"2018-09-13\",\"2018-09-14\",\"2018-09-17\",\"2018-09-18\",\"2018-09-19\",\"2018-09-20\",\"2018-09-21\",\"2018-09-24\",\"2018-09-25\",\"2018-09-26\",\"2018-09-27\",\"2018-09-28\",\"2018-10-01\",\"2018-10-02\",\"2018-10-03\",\"2018-10-04\",\"2018-10-05\",\"2018-10-08\",\"2018-10-09\",\"2018-10-10\",\"2018-10-11\",\"2018-10-12\",\"2018-10-15\",\"2018-10-16\",\"2018-10-17\",\"2018-10-18\",\"2018-10-19\",\"2018-10-22\",\"2018-10-23\",\"2018-10-24\",\"2018-10-25\",\"2018-10-26\",\"2018-10-29\",\"2018-10-30\",\"2018-10-31\",\"2018-11-01\",\"2018-11-02\",\"2018-11-05\",\"2018-11-06\",\"2018-11-07\",\"2018-11-08\",\"2018-11-09\",\"2018-11-12\",\"2018-11-13\",\"2018-11-14\",\"2018-11-15\",\"2018-11-16\",\"2018-11-19\",\"2018-11-20\",\"2018-11-21\",\"2018-11-23\",\"2018-11-26\",\"2018-11-27\",\"2018-11-28\",\"2018-11-29\",\"2018-11-30\",\"2018-12-03\",\"2018-12-04\",\"2018-12-06\",\"2018-12-07\",\"2018-12-10\",\"2018-12-11\",\"2018-12-12\",\"2018-12-13\",\"2018-12-14\",\"2018-12-17\",\"2018-12-18\",\"2018-12-19\",\"2018-12-20\",\"2018-12-21\",\"2018-12-24\",\"2018-12-26\",\"2018-12-27\",\"2018-12-28\",\"2018-12-31\",\"2019-01-02\",\"2019-01-03\",\"2019-01-04\",\"2019-01-07\",\"2019-01-08\",\"2019-01-09\",\"2019-01-10\",\"2019-01-11\",\"2019-01-14\",\"2019-01-15\",\"2019-01-16\",\"2019-01-17\",\"2019-01-18\",\"2019-01-22\",\"2019-01-23\",\"2019-01-24\",\"2019-01-25\",\"2019-01-28\",\"2019-01-29\",\"2019-01-30\",\"2019-01-31\",\"2019-02-01\",\"2019-02-04\",\"2019-02-05\",\"2019-02-06\",\"2019-02-07\",\"2019-02-08\",\"2019-02-11\",\"2019-02-12\",\"2019-02-13\",\"2019-02-14\",\"2019-02-15\",\"2019-02-19\",\"2019-02-20\",\"2019-02-21\",\"2019-02-22\",\"2019-02-25\",\"2019-02-26\",\"2019-02-27\",\"2019-02-28\",\"2019-03-01\",\"2019-03-04\",\"2019-03-05\",\"2019-03-06\",\"2019-03-07\",\"2019-03-08\",\"2019-03-11\",\"2019-03-12\",\"2019-03-13\",\"2019-03-14\",\"2019-03-15\",\"2019-03-18\",\"2019-03-19\",\"2019-03-20\",\"2019-03-21\",\"2019-03-22\",\"2019-03-25\",\"2019-03-26\",\"2019-03-27\",\"2019-03-28\",\"2019-03-29\",\"2019-04-01\",\"2019-04-02\",\"2019-04-03\",\"2019-04-04\",\"2019-04-05\",\"2019-04-08\",\"2019-04-09\",\"2019-04-10\",\"2019-04-11\",\"2019-04-12\",\"2019-04-15\",\"2019-04-16\",\"2019-04-17\",\"2019-04-18\",\"2019-04-22\",\"2019-04-23\",\"2019-04-24\",\"2019-04-25\",\"2019-04-26\",\"2019-04-29\",\"2019-04-30\",\"2019-05-01\",\"2019-05-02\",\"2019-05-03\",\"2019-05-06\",\"2019-05-07\",\"2019-05-08\",\"2019-05-09\",\"2019-05-10\",\"2019-05-13\",\"2019-05-14\",\"2019-05-15\",\"2019-05-16\",\"2019-05-17\",\"2019-05-20\",\"2019-05-21\",\"2019-05-22\",\"2019-05-23\",\"2019-05-24\",\"2019-05-28\",\"2019-05-29\",\"2019-05-30\",\"2019-05-31\",\"2019-06-03\",\"2019-06-04\",\"2019-06-05\",\"2019-06-06\",\"2019-06-07\",\"2019-06-10\",\"2019-06-11\",\"2019-06-12\",\"2019-06-13\",\"2019-06-14\",\"2019-06-17\",\"2019-06-18\",\"2019-06-19\",\"2019-06-20\",\"2019-06-21\",\"2019-06-24\",\"2019-06-25\",\"2019-06-26\",\"2019-06-27\",\"2019-06-28\",\"2019-07-01\",\"2019-07-02\",\"2019-07-03\",\"2019-07-05\",\"2019-07-08\",\"2019-07-09\",\"2019-07-10\",\"2019-07-11\",\"2019-07-12\",\"2019-07-15\",\"2019-07-16\",\"2019-07-17\",\"2019-07-18\",\"2019-07-19\",\"2019-07-22\",\"2019-07-23\",\"2019-07-24\",\"2019-07-25\",\"2019-07-26\",\"2019-07-29\",\"2019-07-30\",\"2019-07-31\",\"2019-08-01\",\"2019-08-02\",\"2019-08-05\",\"2019-08-06\",\"2019-08-07\",\"2019-08-08\",\"2019-08-09\",\"2019-08-12\",\"2019-08-13\",\"2019-08-14\",\"2019-08-15\",\"2019-08-16\",\"2019-08-19\",\"2019-08-20\",\"2019-08-21\",\"2019-08-22\",\"2019-08-23\",\"2019-08-26\",\"2019-08-27\",\"2019-08-28\",\"2019-08-29\",\"2019-08-30\",\"2019-09-03\",\"2019-09-04\",\"2019-09-05\",\"2019-09-06\",\"2019-09-09\",\"2019-09-10\",\"2019-09-11\",\"2019-09-12\",\"2019-09-13\",\"2019-09-16\",\"2019-09-17\",\"2019-09-18\",\"2019-09-19\",\"2019-09-20\",\"2019-09-23\",\"2019-09-24\",\"2019-09-25\",\"2019-09-26\",\"2019-09-27\",\"2019-09-30\",\"2019-10-01\",\"2019-10-02\",\"2019-10-03\",\"2019-10-04\",\"2019-10-07\",\"2019-10-08\",\"2019-10-09\",\"2019-10-10\",\"2019-10-11\",\"2019-10-14\",\"2019-10-15\",\"2019-10-16\",\"2019-10-17\",\"2019-10-18\",\"2019-10-21\",\"2019-10-22\",\"2019-10-23\",\"2019-10-24\",\"2019-10-25\",\"2019-10-28\",\"2019-10-29\",\"2019-10-30\",\"2019-10-31\",\"2019-11-01\",\"2019-11-04\",\"2019-11-05\",\"2019-11-06\",\"2019-11-07\",\"2019-11-08\",\"2019-11-11\",\"2019-11-12\",\"2019-11-13\",\"2019-11-14\",\"2019-11-15\",\"2019-11-18\",\"2019-11-19\",\"2019-11-20\",\"2019-11-21\",\"2019-11-22\",\"2019-11-25\",\"2019-11-26\",\"2019-11-27\",\"2019-11-29\",\"2019-12-02\",\"2019-12-03\",\"2019-12-04\",\"2019-12-05\",\"2019-12-06\",\"2019-12-09\",\"2019-12-10\",\"2019-12-11\",\"2019-12-12\",\"2019-12-13\",\"2019-12-16\",\"2019-12-17\",\"2019-12-18\",\"2019-12-19\",\"2019-12-20\",\"2019-12-23\",\"2019-12-24\",\"2019-12-26\",\"2019-12-27\",\"2019-12-30\",\"2019-12-31\",\"2020-01-02\",\"2020-01-03\",\"2020-01-06\",\"2020-01-07\",\"2020-01-08\",\"2020-01-09\",\"2020-01-10\",\"2020-01-13\",\"2020-01-14\",\"2020-01-15\",\"2020-01-16\",\"2020-01-17\",\"2020-01-21\",\"2020-01-22\",\"2020-01-23\",\"2020-01-24\",\"2020-01-27\",\"2020-01-28\",\"2020-01-29\",\"2020-01-30\",\"2020-01-31\",\"2020-02-03\",\"2020-02-04\",\"2020-02-05\",\"2020-02-06\",\"2020-02-07\",\"2020-02-10\",\"2020-02-11\",\"2020-02-12\",\"2020-02-13\",\"2020-02-14\",\"2020-02-18\",\"2020-02-19\",\"2020-02-20\",\"2020-02-21\",\"2020-02-24\",\"2020-02-25\",\"2020-02-26\",\"2020-02-27\",\"2020-02-28\",\"2020-03-02\",\"2020-03-03\",\"2020-03-04\",\"2020-03-05\",\"2020-03-06\",\"2020-03-09\",\"2020-03-10\",\"2020-03-11\",\"2020-03-12\",\"2020-03-13\",\"2020-03-16\",\"2020-03-17\",\"2020-03-18\",\"2020-03-19\",\"2020-03-20\",\"2020-03-23\",\"2020-03-24\",\"2020-03-25\",\"2020-03-26\",\"2020-03-27\",\"2020-03-30\",\"2020-03-31\",\"2020-04-01\",\"2020-04-02\",\"2020-04-03\",\"2020-04-06\",\"2020-04-07\",\"2020-04-08\",\"2020-04-09\",\"2020-04-13\",\"2020-04-14\",\"2020-04-15\",\"2020-04-16\",\"2020-04-17\",\"2020-04-20\",\"2020-04-21\",\"2020-04-22\",\"2020-04-23\",\"2020-04-24\",\"2020-04-27\",\"2020-04-28\",\"2020-04-29\",\"2020-04-30\",\"2020-05-01\",\"2020-05-04\",\"2020-05-05\",\"2020-05-06\",\"2020-05-07\",\"2020-05-08\",\"2020-05-11\",\"2020-05-12\",\"2020-05-13\",\"2020-05-14\",\"2020-05-15\",\"2020-05-18\",\"2020-05-19\",\"2020-05-20\",\"2020-05-21\",\"2020-05-22\",\"2020-05-26\",\"2020-05-27\",\"2020-05-28\",\"2020-05-29\",\"2020-06-01\",\"2020-06-02\",\"2020-06-03\",\"2020-06-04\",\"2020-06-05\",\"2020-06-08\",\"2020-06-09\",\"2020-06-10\",\"2020-06-11\",\"2020-06-12\",\"2020-06-15\",\"2020-06-16\",\"2020-06-17\",\"2020-06-18\",\"2020-06-19\",\"2020-06-22\",\"2020-06-23\",\"2020-06-24\",\"2020-06-25\",\"2020-06-26\",\"2020-06-29\",\"2020-06-30\",\"2020-07-01\",\"2020-07-02\",\"2020-07-06\",\"2020-07-07\",\"2020-07-08\",\"2020-07-09\",\"2020-07-10\",\"2020-07-13\",\"2020-07-14\",\"2020-07-15\",\"2020-07-16\",\"2020-07-17\",\"2020-07-20\",\"2020-07-21\",\"2020-07-22\",\"2020-07-23\",\"2020-07-24\",\"2020-07-27\",\"2020-07-28\",\"2020-07-29\",\"2020-07-30\",\"2020-07-31\",\"2020-08-03\",\"2020-08-04\",\"2020-08-05\",\"2020-08-06\",\"2020-08-07\",\"2020-08-10\",\"2020-08-11\",\"2020-08-12\",\"2020-08-13\",\"2020-08-14\",\"2020-08-17\",\"2020-08-18\",\"2020-08-19\",\"2020-08-20\",\"2020-08-21\",\"2020-08-24\",\"2020-08-25\",\"2020-08-26\",\"2020-08-27\",\"2020-08-28\",\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\"2020-09-04\",\"2020-09-08\",\"2020-09-09\",\"2020-09-10\",\"2020-09-11\",\"2020-09-14\",\"2020-09-15\",\"2020-09-16\",\"2020-09-17\",\"2020-09-18\",\"2020-09-21\",\"2020-09-22\",\"2020-09-23\",\"2020-09-24\",\"2020-09-25\",\"2020-09-28\",\"2020-09-29\",\"2020-09-30\",\"2020-10-01\",\"2020-10-02\",\"2020-10-05\",\"2020-10-06\",\"2020-10-07\",\"2020-10-08\",\"2020-10-09\",\"2020-10-12\",\"2020-10-13\",\"2020-10-14\",\"2020-10-15\",\"2020-10-16\",\"2020-10-19\",\"2020-10-20\",\"2020-10-21\",\"2020-10-22\",\"2020-10-23\",\"2020-10-26\",\"2020-10-27\",\"2020-10-28\",\"2020-10-29\",\"2020-10-30\",\"2020-11-02\",\"2020-11-03\",\"2020-11-04\",\"2020-11-05\",\"2020-11-06\",\"2020-11-09\",\"2020-11-10\",\"2020-11-11\",\"2020-11-12\",\"2020-11-13\",\"2020-11-16\",\"2020-11-17\",\"2020-11-18\",\"2020-11-19\",\"2020-11-20\",\"2020-11-23\",\"2020-11-24\",\"2020-11-25\",\"2020-11-27\",\"2020-11-30\",\"2020-12-01\",\"2020-12-02\",\"2020-12-03\",\"2020-12-04\",\"2020-12-07\",\"2020-12-08\",\"2020-12-09\",\"2020-12-10\",\"2020-12-11\",\"2020-12-14\",\"2020-12-15\",\"2020-12-16\",\"2020-12-17\",\"2020-12-18\",\"2020-12-21\",\"2020-12-22\",\"2020-12-23\",\"2020-12-24\",\"2020-12-28\",\"2020-12-29\",\"2020-12-30\",\"2020-12-31\",\"2021-01-04\",\"2021-01-05\",\"2021-01-06\",\"2021-01-07\",\"2021-01-08\",\"2021-01-11\",\"2021-01-12\",\"2021-01-13\",\"2021-01-14\",\"2021-01-15\",\"2021-01-19\",\"2021-01-20\",\"2021-01-21\",\"2021-01-22\",\"2021-01-25\",\"2021-01-26\",\"2021-01-27\",\"2021-01-28\",\"2021-01-29\",\"2021-02-01\",\"2021-02-02\",\"2021-02-03\",\"2021-02-04\",\"2021-02-05\",\"2021-02-08\",\"2021-02-09\",\"2021-02-10\",\"2021-02-11\",\"2021-02-12\",\"2021-02-16\",\"2021-02-17\",\"2021-02-18\",\"2021-02-19\",\"2021-02-22\",\"2021-02-23\",\"2021-02-24\",\"2021-02-25\",\"2021-02-26\",\"2021-03-01\",\"2021-03-02\",\"2021-03-03\",\"2021-03-04\",\"2021-03-05\",\"2021-03-08\",\"2021-03-09\",\"2021-03-10\",\"2021-03-11\",\"2021-03-12\",\"2021-03-15\",\"2021-03-16\",\"2021-03-17\",\"2021-03-18\",\"2021-03-19\",\"2021-03-22\",\"2021-03-23\",\"2021-03-24\",\"2021-03-25\",\"2021-03-26\",\"2021-03-29\",\"2021-03-30\",\"2021-03-31\",\"2021-04-01\",\"2021-04-05\",\"2021-04-06\",\"2021-04-07\",\"2021-04-08\",\"2021-04-09\",\"2021-04-12\",\"2021-04-13\",\"2021-04-14\",\"2021-04-15\",\"2021-04-16\",\"2021-04-19\",\"2021-04-20\",\"2021-04-21\",\"2021-04-22\",\"2021-04-23\",\"2021-04-26\",\"2021-04-27\",\"2021-04-28\",\"2021-04-29\",\"2021-04-30\",\"2021-05-03\",\"2021-05-04\",\"2021-05-05\",\"2021-05-06\",\"2021-05-07\",\"2021-05-10\",\"2021-05-11\",\"2021-05-12\",\"2021-05-13\",\"2021-05-14\",\"2021-05-17\",\"2021-05-18\",\"2021-05-19\",\"2021-05-20\",\"2021-05-21\",\"2021-05-24\",\"2021-05-25\",\"2021-05-26\",\"2021-05-27\",\"2021-05-28\",\"2021-06-01\",\"2021-06-02\",\"2021-06-03\",\"2021-06-04\",\"2021-06-07\",\"2021-06-08\",\"2021-06-09\",\"2021-06-10\",\"2021-06-11\",\"2021-06-14\",\"2021-06-15\",\"2021-06-16\",\"2021-06-17\",\"2021-06-18\",\"2021-06-21\",\"2021-06-22\",\"2021-06-23\",\"2021-06-24\",\"2021-06-25\",\"2021-06-28\",\"2021-06-29\",\"2021-06-30\",\"2021-07-01\",\"2021-07-02\",\"2021-07-06\",\"2021-07-07\",\"2021-07-08\",\"2021-07-09\",\"2021-07-12\",\"2021-07-13\",\"2021-07-14\",\"2021-07-15\",\"2021-07-16\",\"2021-07-19\",\"2021-07-20\",\"2021-07-21\",\"2021-07-22\",\"2021-07-23\",\"2021-07-26\",\"2021-07-27\",\"2021-07-28\",\"2021-07-29\",\"2021-07-30\",\"2021-08-02\",\"2021-08-03\",\"2021-08-04\",\"2021-08-05\",\"2021-08-06\",\"2021-08-09\",\"2021-08-10\",\"2021-08-11\",\"2021-08-12\",\"2021-08-13\",\"2021-08-16\",\"2021-08-17\",\"2021-08-18\",\"2021-08-19\",\"2021-08-20\",\"2021-08-23\",\"2021-08-24\",\"2021-08-25\",\"2021-08-26\",\"2021-08-27\",\"2021-08-30\",\"2021-08-31\",\"2021-09-01\",\"2021-09-02\",\"2021-09-03\",\"2021-09-07\",\"2021-09-08\",\"2021-09-09\",\"2021-09-10\",\"2021-09-13\",\"2021-09-14\",\"2021-09-15\",\"2021-09-16\",\"2021-09-17\",\"2021-09-20\",\"2021-09-21\",\"2021-09-22\",\"2021-09-23\",\"2021-09-24\",\"2021-09-27\",\"2021-09-28\",\"2021-09-29\",\"2021-09-30\",\"2021-10-01\",\"2021-10-04\",\"2021-10-05\",\"2021-10-06\",\"2021-10-07\",\"2021-10-08\",\"2021-10-11\",\"2021-10-12\",\"2021-10-13\",\"2021-10-14\",\"2021-10-15\",\"2021-10-18\",\"2021-10-19\",\"2021-10-20\",\"2021-10-21\",\"2021-10-22\",\"2021-10-25\",\"2021-10-26\",\"2021-10-27\",\"2021-10-28\",\"2021-10-29\",\"2021-11-01\",\"2021-11-02\",\"2021-11-03\",\"2021-11-04\",\"2021-11-05\",\"2021-11-08\",\"2021-11-09\",\"2021-11-10\",\"2021-11-11\",\"2021-11-12\",\"2021-11-15\",\"2021-11-16\",\"2021-11-17\",\"2021-11-18\",\"2021-11-19\",\"2021-11-22\",\"2021-11-23\",\"2021-11-24\",\"2021-11-26\",\"2021-11-29\",\"2021-11-30\",\"2021-12-01\",\"2021-12-02\",\"2021-12-03\",\"2021-12-06\",\"2021-12-07\",\"2021-12-08\",\"2021-12-09\",\"2021-12-10\",\"2021-12-13\",\"2021-12-14\",\"2021-12-15\",\"2021-12-16\",\"2021-12-17\",\"2021-12-20\",\"2021-12-21\",\"2021-12-22\",\"2021-12-23\",\"2021-12-27\",\"2021-12-28\",\"2021-12-29\",\"2021-12-30\",\"2021-12-31\",\"2022-01-03\",\"2022-01-04\",\"2022-01-05\",\"2022-01-06\",\"2022-01-07\",\"2022-01-10\",\"2022-01-11\",\"2022-01-12\",\"2022-01-13\",\"2022-01-14\",\"2022-01-18\",\"2022-01-19\",\"2022-01-20\",\"2022-01-21\",\"2022-01-24\",\"2022-01-25\",\"2022-01-26\",\"2022-01-27\",\"2022-01-28\",\"2022-01-31\",\"2022-02-01\",\"2022-02-02\",\"2022-02-03\",\"2022-02-04\",\"2022-02-07\",\"2022-02-08\",\"2022-02-09\",\"2022-02-10\",\"2022-02-11\"],[39.625,40.487499,40.759998,42.447498,43.09,43.012501,43.2075,42.950001,43.4175,44.087502,44.775002,44.814999,44.634998,43.200001,43.802502,44.477501,43.735001,43.869999,44.490002,45.072498,45.647499,45.080002,44.625,44.662498,44.330002,43.810001,43.759998,42.5,42.0975,42.017502,43.419998,41.8125,41.952499,41.66,41.91,41.220001,43.145,42.7425,42.470001,43.25,43.057499,43.352501,43.695,43.7575,44.122501,44.452499,43.439999,42.650002,41.7075,41.4175,40.654999,41.029999,41,40.532501,41.602501,43.807499,43.970001,44.5625,46.294998,46.247501,46.637501,46.935001,47.372501,47.252499,46.695,46.517502,47,46.797501,47,47.095001,46.587502,47.192501,47.057499,46.900002,46.93,46.805,46.997501,47.91,48.267502,48.407501,48.535,47.7925,47.837502,47.8475,48.105,47.887501,47.5075,46.970001,46.285,46.587502,46.8125,46.529999,45.849998,45.747501,46.307499,46.025002,46.572498,45.955002,46.947498,46.314999,46.355,47.375,47.677502,47.125,47.3825,47.77,47.880001,47.4375,47.945,47.422501,47.945,47.669998,48.112499,48.264999,48.6525,48.747501,47.974998,47.575001,49.782501,50.145,51.7575,52,52.330002,51.512501,52.3825,51.84,52.327499,52.540001,52.305,52.9375,53.360001,54.525002,54.200001,53.525002,53.662498,54.150002,54.287498,54.752499,55.037498,55.8125,56.627499,57.102501,57.247501,56.557499,55.462502,55.237499,54.502499,56.235001,55.880001,56.4375,55.537498,54.447498,54.625,55.060001,55.195,54.205002,54.9375,55.25,55.955002,56.197498,56.987499,56.8125,57.512501,57.695,56.990002,55.552502,55.91,56.365002,53.630001,55.105,55.290001,54.732498,55.575001,54.465,54.514999,54.947498,53.9575,55.650002,54.427502,53.974998,54.797501,52.787498,54.220001,54.762501,52.387501,51.075001,50.48,51.4925,52.494999,51.387501,49.75,47.907501,48.474998,47.0975,47.625,47.5,44.592499,44.932499,43.735001,43.560001,42.877499,44.182499,45.665001,45.072498,46.115002,45.237499,42.939999,43.372501,41.25,42.915001,42.599998,42.622501,42.25,41.362499,41.345001,41.5,40.099998,39.215,37.037498,37.075001,38.959999,39.375,39.6325,38.7225,35.994999,36.1325,37.174999,37.389999,37.822498,38.125,38.220001,37.712502,37.567501,38.27,38.549999,39.375,39.102501,38.537498,38.5275,38.869999,38.947498,39.0625,40.8125,41.5275,41.740002,41.852501,43.215,43.662498,43.099998,42.247501,42.762501,42.525002,42.8475,42.427502,42.8125,42.427502,42.797501,42.950001,42.895,43.540001,43.427502,43.302502,43.580002,43.57,43.922501,43.985001,43.6675,43.467499,42.580002,43.872501,45,45.5625,45.974998,46.212502,46.450001,47.087502,46.557499,47.505001,48.834999,47.877499,47.915001,47.1875,47.237499,47.4575,47.91,47.772499,48.3125,48.697498,49.112499,49.105,50.080002,49.669998,50.212502,49.799999,49.645,49.865002,49.884998,50.779999,50.7075,51.107498,51.84,51.7075,51.224998,51.099998,50.764999,52.470001,52.459999,52.7225,51.072498,51.470001,50.474998,50.099998,49.355,46.927502,46.602501,46.567501,47.477501,46.732498,45.880001,46.305,46.165001,44.950001,45.049999,44.73,44.105,44.487499,44.057499,43.900002,43.860001,46.07,45.77,46.627499,47.952499,48.715,48.487499,48.674999,47.887501,48.224998,49.012501,49.919998,50.092499,49.700001,49.634998,49.607498,49.442501,50.072498,49.669998,50.7925,50.352501,50.82,50.837502,50.202499,49.799999,50.462502,50.827499,50.612499,51.022499,51.147499,51.012501,51,51.447498,50.912498,52.115002,51.9175,52.2225,51.869999,52.115002,52.189999,54.105,53.474998,51.3825,49.497501,49.077499,48.852501,50.049999,50.325001,49.904999,50.255001,50.790001,50.865002,51.07,52.654999,52.720001,53.247501,53.297501,52.357498,51.465,51.965,51.025002,52.125,52.540001,51.607498,52.0975,53,53.512501,53.709999,53.465,54.517502,56.200001,55,54.432499,54.990002,55.264999,55.502499,55.345001,54.737499,55.2575,54.637501,55,55.134998,55.224998,56.267502,55.764999,54.607498,56.41,56.567501,56.455002,56.7575,56.982498,58.237499,58.724998,59.0975,58.342499,58.772499,58.647499,59.380001,60.290001,60.525002,61.127499,60.790001,61.855,62.2425,61.189999,61.810001,62.384998,64.332497,64.262497,64.192497,64.684998,64.672501,64.574997,65.387497,65.282501,65.9375,65.919998,66.449997,66.974998,66.385002,65.922501,65.647499,65.677498,66.735001,66.394997,66.650002,66.817497,64.577499,65.267502,65.947502,66.870003,67.5,67.150002,67.202499,66.945,67.864998,69.25,69.892502,69.949997,69.875,70.557503,70.1325,71.172501,71.205002,72.779999,72.364998,72.482498,74.059998,74.287498,73.447502,74.959999,74.290001,76.809998,77.650002,77.910004,79.175003,77.962502,78.397499,79.067497,79.297501,79.644997,79.480003,80.0625,77.514999,78.150002,81.112503,80.135002,80.232498,76.074997,78.827499,80.879997,80.642502,80.592499,78.544998,80.900002,80.3675,81.047501,81.184998,78.839996,80,80.657501,79.654999,74.315002,75.237503,71.6325,70.275002,64.315002,70.57,75.917503,74.110001,73.879997,70.5,65.9375,69.285004,69.347504,63.985001,66.222504,60.487499,61.877499,59.942501,61.8475,61.794998,57.02,59.09,62.6875,61.630001,63.1875,62.685001,63.900002,61.625,60.084999,60.700001,62.724998,67.699997,65.684998,67.175003,67.077499,70,70.599998,71.845001,71.172501,69.487503,69.07,68.402496,68.967499,69.300003,70.449997,71.269997,71.182503,72.489998,71.5625,72.292503,73.764999,75.114998,75.805,76.410004,77.025002,79.457497,78.037498,76.127502,75.087502,78.292503,78.7575,79.169998,79.665001,78.942497,80.875,79.035004,79.192497,79.8125,79.4375,80.1875,81.165001,81.097504,80.837502,82.5625,83.035004,86.974998,87.327499,86.18,83.3125,87.864998,88.787498,87.852501,88.660004,87.834999,91,91.25,90.175003,91.102501,88.3125,90.019997,91.279999,91.962502,92.5,93.852501,94.18,96.262497,95.334999,97.264999,94.839996,98.989998,96.5625,96.987503,96.417503,99.172501,96.692497,96.997498,90.987503,93.709999,94.3675,93.75,94.1875,102.885002,108.199997,109.1325,109.377502,110.404999,113.205002,112.599998,111.970001,110.497498,114.43,114.830002,116.0625,114.352501,115.982498,115.75,119.262497,128.697495,124.697502,126.18,127.142502,126.012497,127.580002,132.759995,137.589996,126.910004,120.07,113.949997,117.260002,120.360001,114.57,114.720001,118.330002,115.230003,109.720001,110.400002,104.540001,112.68,111.620003,105.169998,108.43,115.010002,114.550003,113.790001,117.639999,112.889999,113.910004,115.699997,114.620003,116.25,115.279999,120.059998,125.269997,121,118.720001,121.279999,119.959999,116.199997,116.669998,117.449997,116.389999,114.010002,115.489998,115.050003,112.370003,111.059998,109.110001,109.660004,114.139999,117.949997,118.32,120.5,115.550003,117.190002,119.620003,119.440002,118.919998,119.550003,118.610001,117.589996,118.639999,117.18,113.910004,115.550003,116.57,116.970001,121.010002,122.019997,123.519997,122.599998,122.309998,124.370003,124.529999,120.5,122.43,122.599998,124.339996,127.410004,128.899994,128.960007,125.019997,131.610001,132.160004,131.320007,133.990005,138.050003,135.580002,134.080002,133.520004,128.889999,127.720001,128.360001,132.429993,129.190002,128.5,128.759995,130.800003,128.779999,127.779999,128.660004,133.800003,136.279999,143.070007,143.600006,143.429993,139.520004,135.830002,133.75,135.729996,135.759995,136.300003,137.350006,136.029999,136.619995,136.479996,135.899994,134.350006,135.490005,131.25,129.199997,130.240005,128.009995,123.760002,124.940002,124.68,122.589996,123.75,128.410004,124.809998,121.75,120.980003,120.93,119.029999,121.690002,122.540001,120.400002,121.410004,125.699997,124.050003,122.879997,119.900002,120.330002,123.330002,122.82,119.540001,120.349998,121.650002,120.110001,121.650002,123.660004,123.870003,126.5,125.830002,128.949997,129.800003,132.520004,132.440002,134.940002,133.820007,134.300003,133.509995,135.020004,132.360001,133.039993,132.160004,134.830002,135.009995,134.309998,136.470001,131.779999,132.039993,131.190002,129.199997,127.889999,130.850006,129.410004,123.5,123.400002,124.580002,126.25,126.82,126.559998,123.160004,125.230003,127.82,126.010002,127.82,126.959999,126.440002,125.57,125.080002,124.279999,124.68,124.07,126.169998,126.599998,127.209999,127.019997,126.529999,127.82,129.940002,130.369995,129.800003,130.710007,130.300003,132.130005,133.770004,134.449997,133.460007,133.410004,134.800003,136.169998,136.600006,137.899994,140.070007,143.539993,141.580002,142.75,146.210007,144.029999,148.100006,149.240005,148.460007,143.75,143.460007,145.529999,145.940002,147.550003,148.270004,149.119995,144.809998,144.690002,144.380005,146.360001,145.809998,147.270004,146.979996,146.350006,146.199997,146.440002,146.050003,146.190002,148.970001,148.539993,150.229996,149.800003,145.029999,147.440002,148.309998,149.449997,149.809998,148.350006,147.479996,149,152.660004,152.830002,153.869995,153.759995,154.970001,156.979996,155.490005,155,150.630005,150.350006,148.559998,148.440002,148.820007,143.800003,143.929993,144.449997,146.649994,145.660004,145.470001,143.25,142.470001,143.660004,141.899994,141.759995,139.490005,139.470001,143.059998,144.029999,142.270004,143.229996,141.240005,142.110001,143.770004,143.449997,147.009995,148.699997,148.809998,149.690002,148.679993,149.330002,149.360001,149.820007,147.220001,148.990005,148.660004,150.389999,151.580002,151.889999,151.410004,150.199997,150.020004,148.960007,148.429993,150.369995,149.940002,151,153.710007,157.649994,161.679993,161.119995,160.75,159.570007,159.369995,159.990005,167.479996,158.740005,164.020004,164.289993,169.080002,172.130005,174.910004,175.210007,181.119995,175.25,175.110001,179.279999,169.929993,168.279999,171.559998,173.039993,175.850006,177.089996,180.160004,179.330002,179.470001,178.089996,177.830002,182.630005,179.610001,172.699997,172.889999,169.080002,172.320007,176.119995,175.779999,171.339996,171.509995,170,166.979996,164.419998,160.020004,158.979996,163.5,162.449997,165.710007,170.160004,174.009995,174.75,174.479996,171.679993,172.860001,171.729996,176.050003,174.139999,172.330002],[40.9725,41.1875,41.884998,43.272499,43.705002,43.564999,43.529999,43.487499,43.912498,44.8475,45.119999,45.154999,44.945,44.075001,44.435001,44.5625,43.962502,44.279999,45,45.5975,45.875,45.130001,45.060001,44.779999,44.3675,44.200001,43.772499,43.169998,42.48,43.275002,43.787498,42.505001,42.9375,42.235001,42.1875,43.002499,43.557499,43.119999,43.272499,43.5,43.48,43.75,43.959999,44.047501,44.735001,44.705002,43.8475,42.805,41.73,41.5825,41.355,41.432499,41.0825,41.814999,42.299999,44.4375,44.375,46.0625,46.9175,46.555,46.849998,47.592499,47.514999,47.3825,46.767502,47.115002,47.227501,46.952499,47.317501,47.220001,47.125,47.209999,47.412498,47.1875,47,47.057499,47.564999,48.355,48.485001,48.52,48.549999,48,47.9925,48.1525,48.220001,47.892502,47.540001,47.305,46.5825,46.799999,47.087502,46.537498,46.23,46.6325,46.82,46.552502,46.797501,46.825001,46.987499,46.602501,47.107498,47.669998,47.82,47.445,47.852501,47.959999,48.162498,47.967499,47.950001,48.137501,48.107498,47.990002,48.415001,48.712502,48.990002,48.797501,48.049999,48.035,50.439999,52.095001,52.185001,52.3125,52.375,51.952499,52.445,52.275002,52.737499,52.639999,52.685001,53.452499,54.487499,54.794998,54.297501,54.09,54.262501,54.224998,54.685001,55.134998,55.872501,57.064999,57.217499,57.294998,57.4175,56.837502,56.342499,55.462502,56.075001,56.25,57.087502,56.709999,55.737499,55.462502,54.904999,55.57,55.34,55.314999,55.705002,55.9375,56.610001,56.459999,57.355,57.5,58.3675,58.087502,57.102501,56.200001,56.817501,56.587502,54.875,55.720001,55.4575,55.747501,55.66,54.935001,55.314999,55.84,55.8125,56.057499,55.345001,55.047501,54.922501,53.794998,55.112499,55.59,53.412498,51.0975,51.18,52.514999,52.529999,51.502499,49.962502,49.294998,48.619999,47.9925,48.7425,47.674999,45.3675,45.067501,44.150002,43.737499,43.692501,45.322498,45.700001,45.0825,46.235001,45.5975,43.695,43.622501,42.522499,42.947498,42.98,43.142502,42.27,42.087502,41.8825,41.862499,40.5275,39.540001,37.887501,39.307499,39.192501,39.630001,39.84,39.712502,36.43,37.137501,37.2075,37.955002,38.6325,38.4925,38.424999,37.817501,38.3475,38.970001,39.415001,39.470001,39.182499,38.785,38.619999,39.532501,39.0825,39.532501,41.537498,42.25,42.244999,42.915001,43.77,43.892502,43.485001,42.665001,42.802502,42.75,43.119999,42.814999,42.924999,42.860001,43.330002,43.092499,43.25,43.967499,43.825001,43.75,43.727501,43.787498,44.4375,44,43.872501,43.610001,43.267502,44.779999,45.6675,45.825001,46.025002,46.8325,47.0975,47.247501,47.372501,49.0825,49.422501,47.994999,48.220001,47.439999,47.389999,47.52,47.919998,48.615002,49.125,49.092499,49.275002,50.057499,50.712502,50.185001,50.25,50.035,49.962502,50.342499,50.845001,51.037498,51.235001,51.9375,52.119999,51.939999,51.25,51.4925,50.849998,53.827499,53.162498,52.959999,52.209999,51.855,51.334999,50.419998,49.712502,47.369999,47.424999,47.9375,48.1175,47.724998,46.087502,47,46.427502,45.134998,45.535,45.147499,44.837502,44.807499,44.497501,44.48,44.9575,46.247501,46.3675,47.98,48.842499,49,48.9925,49.197498,48.397499,48.740002,50.072498,49.970001,50.1525,50.212502,50.040001,49.814999,50.247501,50.392502,49.875,51.122501,50.782501,51.110001,51.27,50.349998,50.377499,50.932499,51.0975,51,51.467499,51.5275,51.272499,51.470001,51.625,51.807499,52.227501,52.287498,52.310001,52.432499,52.66,52.540001,55.342499,54.5075,51.607498,49.662498,49.517502,49.889999,50.8825,50.689999,50.512501,53.035,51.610001,51.285,51.790001,53.182499,53.337502,53.412498,53.610001,53.012501,51.797501,52.137501,51.43,52.330002,52.612499,51.744999,52.369999,53.4925,53.605,54.110001,54.195,55.927502,56.605,55.197498,55.032501,55.205002,55.712502,55.939999,55.639999,54.959999,55.622501,55.375,55.235001,55.240002,56.145,57.055,55.895,55.240002,56.872501,57.482498,57.014999,56.947498,57.610001,59.41,59.532501,59.412498,58.810001,59.037498,59.395,60.247501,60.549999,60.810001,61.200001,61.682499,62.3125,62.4375,61.325001,62.2925,63.982498,64.462502,64.547501,64.372498,65.087502,65.110001,65.6175,65.697502,66.195,66.220001,66.445,66.857498,67,66.519997,66.002502,65.794998,66.610001,66.790001,66.995003,67,67.0625,64.8825,65.827499,66.472504,67.75,67.699997,67.517502,67.775002,68.139999,68.824997,70.197502,70.442497,70.474998,70.294998,70.662498,71.0625,71.222504,72.495003,73.4925,73.172501,73.419998,75.150002,75.144997,74.989998,75.224998,76.110001,77.607498,78.167503,79.267502,79.392502,78.875,78.925003,79.684998,79.754997,79.997498,79.889999,80.832497,77.942497,79.599998,81.962502,81.022499,80.669998,78.372498,79.910004,81.190002,81.305,80.849998,80.387497,80.974998,81.805,81.555,81.495003,79.9375,81.142502,81.162498,80.112503,76.044998,75.6325,74.470001,71.5,69.602501,75.360001,76,75.849998,74.887497,72.705002,69.522499,71.610001,70.305,67.5,69.980003,64.769997,64.402496,62.5,63.209999,62.9575,57.125,61.922501,64.5625,64.669998,63.967499,63.880001,65.622498,62.18,61.287498,61.424999,65.777496,67.925003,66.842499,67.517502,68.425003,72.0625,71.582497,72.050003,71.737503,70.419998,69.3125,69.474998,70.4375,70.752502,71.135002,71.457497,72.417503,73.6325,74.75,73.422501,75.25,75.809998,76.292503,77.587502,79.262497,79.922501,78.987503,77.447502,76.974998,79.125,79.629997,79.879997,80.222504,79.807503,81.059998,79.677498,80.860001,80.287498,80.587502,80.860001,81.550003,81.404999,82.9375,83.400002,86.402496,88.692497,87.764999,86.949997,86.419998,88.300003,88.849998,88.362503,89.139999,89.864998,93.095001,92.197502,91.25,91.330002,90.542503,91.495003,91.839996,92.6175,93.945,94.654999,95.375,96.317497,95.980003,99.955002,97.254997,99.247498,97.404999,97.147499,98.5,99.25,97.974998,97.077499,92.970001,94.904999,94.550003,95.230003,96.297501,106.415001,111.637497,110.790001,110.392502,114.412498,113.675003,113.775002,112.482498,113.275002,116.042503,115,116.087502,116,117.162498,118.392502,124.8675,128.785004,125.18,126.9925,127.485001,126.442497,131,134.800003,137.979996,128.839996,123.699997,118.989998,119.139999,120.5,115.230003,115.93,118.830002,116,112.199997,110.879997,110.190002,112.860001,112.110001,110.25,112.440002,115.32,115.309998,117.260002,117.720001,115.370003,116.650002,116.120003,115.550003,116.400002,117,125.18,125.389999,123.029999,121.199997,121.550003,120.419998,118.980003,118.709999,118.040001,116.550003,116.550003,117.279999,115.43,116.93,111.989998,110.68,111.489998,115.589996,119.620003,119.199997,121.989998,117.589996,119.629997,120.529999,119.669998,120.989998,120.669998,119.82,119.059998,118.769997,117.620003,115.849998,116.75,117.489998,120.970001,123.470001,123.370003,123.779999,122.860001,124.57,124.980003,125.949997,123.870003,122.760002,123.349998,127.900002,128.369995,129.580002,129.100006,128.309998,134.410004,132.429993,133.460007,137.339996,138.789993,135.990005,134.740005,133.610001,131.740005,131.050003,131.630005,132.630005,130.169998,129.690002,131.449997,131,130.220001,128.710007,132.490005,139.669998,139.850006,145.089996,144.300003,144.300003,141.990005,136.740005,135.380005,136.309998,135.770004,137.399994,137.419998,136.960007,137.880005,136.990005,136.389999,135.529999,136.009995,132.220001,130,130.710007,129.720001,126.709999,125.559998,126.459999,124.849998,127.93,128.720001,125.709999,123.599998,121.940002,121,122.059998,122.169998,123.209999,121.169998,124,127.220001,125.860001,123.18,121.43,123.870003,124.239998,122.900002,121.660004,121.480003,122.580002,120.400002,123.519997,124.18,126.160004,127.129997,127.919998,130.389999,133.039993,132.850006,134.660004,135,135,134.669998,135.470001,135.529999,133.75,134.149994,135.119995,135.059998,135.410004,135.020004,137.070007,133.559998,134.070007,131.490005,130.449997,129.75,131.259995,129.539993,126.269997,124.639999,126.150002,127.889999,126.93,126.989998,124.919998,127.720001,128,127.940002,128.320007,127.389999,127.639999,125.800003,125.349998,125.239998,124.849998,126.160004,126.32,128.460007,127.75,128.190002,127.440002,130.539993,130.600006,130.889999,132.550003,131.509995,132.410004,134.080002,134.320007,134.639999,133.889999,135.25,136.490005,137.410004,137.330002,140,143.149994,144.889999,144.059998,145.649994,146.320007,147.460007,149.570007,150,149.759995,144.070007,147.100006,146.130005,148.199997,148.720001,149.830002,149.210007,146.970001,146.550003,146.330002,146.949997,148.039993,147.789993,147.839996,147.110001,146.699997,147.710007,146.720001,149.050003,149.440002,151.190002,151.679993,150.720001,148,148.5,150.190002,150.860001,150.320007,149.119995,148.75,153.490005,152.800003,154.979996,154.720001,154.630005,157.259995,157.039993,156.110001,155.479996,151.419998,151.070007,149.440002,148.970001,148.820007,144.839996,144.600006,146.429993,147.080002,147.470001,145.960007,144.75,144.449997,144.380005,142.919998,142.210007,142.240005,142.149994,144.220001,144.179993,144.809998,143.25,141.399994,143.880005,144.899994,146.839996,149.169998,149.75,149.639999,150.179993,149.369995,150.839996,149.729996,153.169998,149.940002,149.699997,151.570007,151.970001,152.429993,152.199997,151.570007,151.429993,150.130005,149.429993,150.399994,151.880005,151.490005,155,158.669998,161.020004,165.699997,161.800003,162.139999,160.449997,161.190002,165.520004,170.300003,164.199997,164.960007,167.880005,171.580002,175.960007,176.75,179.630005,182.130005,177.740005,179.5,181.139999,173.470001,170.580002,173.199997,175.860001,176.850006,180.419998,181.330002,180.630005,180.570007,179.229996,182.880005,182.940002,180.169998,175.300003,174.139999,172.5,175.179993,177.179993,176.619995,173.779999,172.539993,171.080002,169.679993,166.330002,162.300003,162.759995,164.389999,163.839996,170.350006,175,174.839996,175.880005,176.240005,174.100006,173.949997,175.350006,176.649994,175.479996,173.080002],[39.377499,40.412498,40.720001,42.25,42.942501,42.855,42.752499,42.927502,43.384998,44.052502,44.540001,44.512501,43.165001,43.112499,43.630001,44.032501,43.567501,43.767502,44.3475,45.052502,44.810001,44.452499,44.517502,44.404999,43.415001,43.735001,42.814999,42.150002,41.235001,41.610001,41.73,41.297501,41.724998,41.1175,41.220001,41.192501,43.02,42.049999,42.462502,42.8825,42.924999,43.259998,43.462502,43.7075,44.102501,44.220001,43.165001,41.357498,41.022499,40.305,40.602501,40.842499,40.157501,40.459999,41.317501,43.450001,43.610001,44.5425,46.1875,45.9175,46.305,46.912498,46.862499,46.965,46.275002,46.5,46.59,46.532501,46.727501,46.695,46.439999,46.552502,46.912498,46.717499,46.695,46.535,46.9375,47.837502,48.09,47.98,48.084999,47.442501,47.552502,47.787498,47.610001,47.555,47.064999,46.799999,45.862499,46.432499,46.235001,46.174999,45.182499,45.634998,46.0075,45.950001,45.727501,45.855,45.884998,46.07,46.299999,47.325001,47.544998,46.9025,47.327499,47.724998,47.605,47.299999,47.482498,47.422501,47.5425,47.389999,48.012501,48.107498,48.4025,47.525002,47.267502,47.334999,49.327499,50.087502,51.369999,51.767502,51.689999,51.130001,51.799999,51.6675,51.924999,52.064999,52.0825,52.8675,53.290001,53.7775,53.5075,53.459999,53.650002,53.7775,54.0825,54.73,54.852501,55.599998,56.5,56.657501,56.275002,55.325001,55.177502,54.1175,54.139999,54.959999,55.642502,55.630001,54.317501,54.279999,53.825001,54.787498,54.322498,54.157501,54.924999,54.939999,55.884998,56.005001,56.587502,56.657501,57.445,56.682499,55.145,55.049999,55.5625,54.012501,53.080002,54.209999,54.317501,54.189999,54.834999,53.25,54.357498,54.735001,53.674999,53.634998,54.1875,53.1675,51.522499,52.317501,54.154999,54.202499,51.357498,49.5425,50.422501,51.032501,51.6875,50.5625,48.447498,47.862499,46.482498,46.724998,47.365002,46.247501,43.877499,44.137501,43.025002,42.564999,42.720001,43.732498,44.424999,44.2575,45.302502,44.067501,42.605,42.075001,40.8325,41.75,42.255001,42.387501,41.32,40.682499,41.0975,39.772499,38.825001,37.407501,36.647499,36.68,37.517502,38.637501,39.119999,38.557499,35.5,35.950001,36.474998,37.130001,37.407501,37.715,37.877499,37.305,37.512501,38.25,38.314999,38.994999,38.154999,37.924999,37.935001,38.580002,38.415001,38.5275,40.057499,41.139999,41.482498,41.82,43.087502,43.212502,42.584999,42.105,42.3125,42.424999,42.48,42.345001,42.4375,42.372501,42.747501,42.575001,42.845001,43.487499,43.2925,43.182499,43.23,43.2225,43.4925,43.634998,43.485001,43.005001,42.375,43.837502,44.842499,45.23,45.639999,45.935001,46.447498,46.48,46.182499,47.452499,47.695,46.650002,46.145,46.637501,46.8825,47.134998,47.095001,47.762501,48.287498,48.285,48.982498,49.084999,49.807499,49.544998,49.610001,49.052502,49.502499,49.639999,49.6525,50.630001,50.584999,50.974998,51.762501,51.279999,50.529999,50.965,49.7775,52.307499,52.032501,52.557499,50.875,50.2075,50.4375,49.165001,48.192501,45.712502,46.352501,46.505001,47.209999,46.689999,45.07,46.174999,45.637501,44.452499,44.654999,44.477501,44,44.1675,43.747501,42.567501,43.630001,45.285,45.537498,46.442501,47.904999,48.400002,48.3475,48.400002,47.575001,48.0425,48.802502,49.327499,49.5075,49.537498,49.5425,48.822498,49.337502,49.892502,49.262501,50.162498,50.34,50.672501,50.724998,49.602501,49.702499,50.389999,50.427502,50.549999,51,50.875,50.817501,50.924999,50.59,50.9025,51.822498,51.7925,51.682499,51.785,52.110001,51.827499,52.825001,51.685001,50.407501,48.145,48.509998,48.455002,49.8475,49.822498,49.787498,50.119999,50.647499,49.9175,50.959999,52.5075,52.580002,52.900002,52.6875,50.25,51.264999,50.8825,50.830002,51.665001,51.799999,51.055,51.830002,52.877499,53.127499,52.767502,52.927502,54.432499,55.715,54.255001,54.389999,54.779999,54.860001,55.092499,54.3675,54.412498,54.297501,54.285,54.7075,54.32,55.197498,56.049999,54.482498,53.782501,55.9725,56.459999,56.0825,56.41,56.825001,58.077499,58.6675,58.720001,58.299999,58.380001,58.572498,59.330002,59.904999,60.305,60.452499,60.720001,61.68,60.642502,60.302502,59.314999,62.290001,63.845001,64.080002,63.842499,64.527496,64.212502,64.57,65.230003,65.267502,65.525002,65.752502,66.057503,66.347504,65.099998,65.294998,65.209999,65.629997,65.625,66.327499,66.474998,65.862503,64.072502,65.169998,65.682503,66.824997,66.227501,66.464996,67.125,66.830002,67.732498,69.245003,69.699997,69.779999,69.737503,69.639999,70.092499,70.730003,71.175003,72.029999,71.305,72.379997,73.797501,74.125,73.1875,74.370003,74.290001,76.550003,77.0625,77.787498,78.042503,77.387497,78.022499,78.75,79,79.327499,78.912498,79.379997,76.220001,78.047501,80.345001,79.6875,77.072502,75.555,78.407501,79.737503,80.065002,79.5,78.462502,79.677498,80.3675,80.837502,80.712502,78.652496,80,79.552498,77.625,72.307503,71.532501,71.625,68.239998,64.092499,69.43,71.449997,73.282501,72.852501,70.307503,65.75,67.342499,67.964996,62,63.237499,60,59.599998,59.279999,60.6525,57,53.1525,58.575001,61.075001,61.59,61.762501,62.349998,63,59.782501,59.224998,59.7425,62.345001,64.75,65.307503,66.175003,66.457497,69.512497,70.157501,70.587502,69.214996,69.212502,66.357498,68.050003,68.717499,69.25,69.987503,69.550003,70.972504,72.087502,71.462502,71.580002,73.614998,74.717499,75.4925,76.072502,76.809998,77.727501,75.802498,75.3825,75.052498,77.580002,78.252502,79.129997,78.967499,78.837502,79.125,78.272499,78.907501,79.1175,79.302498,79.732498,80.574997,80.195,80.807503,81.830002,83.002502,86.522499,83.870003,83.555,83.144997,86.18,87.772499,87.305,86.287498,87.787498,90.567497,89.629997,89.392502,88.254997,87.82,90,90.977501,90.910004,92.467499,93.057503,94.089996,94.672501,94.705002,95.2575,93.877502,96.489998,95.904999,95.839996,96.0625,96.7425,96.602501,92.010002,89.144997,93.480003,93.247498,93.712502,93.767502,100.824997,107.892502,108.387497,108.897499,109.797501,110.292503,110,109.107498,110.297501,113.927498,113.044998,113.962502,114.0075,115.610001,115.732498,119.25,123.9375,123.052498,125.082497,123.832497,124.577499,126,130.529999,127,120.5,110.889999,112.68,115.260002,112.5,110,112.800003,113.610001,112.040001,108.709999,106.089996,103.099998,109.160004,106.769997,105,107.669998,112.779999,113.57,113.620003,115.830002,112.220001,113.550003,112.25,114.129997,114.589996,114.919998,119.279999,119.650002,119.620003,118.150002,118.809998,115.660004,115.629997,116.449997,114.589996,114.279999,112.879997,114.540001,111.099998,112.199997,107.720001,107.32,108.730003,112.349998,116.870003,116.129997,116.050003,114.129997,116.440002,118.57,117.870003,118.150002,118.959999,118,116.809998,117.290001,113.75,112.589996,115.169998,116.220001,116.809998,120.010002,120.889999,122.209999,121.519997,122.25,123.089996,121,120.150002,120.550003,121.540001,124.129997,126.559998,128.039993,126.120003,123.449997,129.649994,130.779999,131.100006,133.509995,134.339996,133.399994,131.720001,126.760002,128.429993,126.379997,127.860001,130.229996,128.5,126.860001,128.490005,128.759995,127,126.940002,128.550003,133.589996,135.020004,136.539993,141.369995,140.410004,136.699997,130.210007,130.929993,134.610001,133.610001,134.589996,135.860001,134.919998,135.850006,134.399994,133.770004,133.690002,132.789993,129.470001,127.410004,128.800003,125.599998,118.389999,122.230003,120.540001,121.199997,122.790001,125.010002,121.839996,118.620003,117.57,116.209999,118.790001,119.449997,121.260002,119.160004,120.419998,124.720001,122.339996,120.32,119.68,120.260002,122.139999,120.07,119,118.919998,120.730003,118.860001,121.150002,122.489998,123.07,125.650002,125.139999,128.520004,129.470001,130.630005,131.929993,131.660004,133.639999,133.279999,133.339996,131.809998,131.300003,131.410004,132.160004,133.559998,134.110001,133.080002,132.449997,131.070007,131.830002,126.699997,127.970001,127.129997,129.479996,126.809998,122.769997,122.25,124.260002,125.849998,125.169998,124.779999,122.860001,125.099998,125.209999,125.940002,126.32,126.419998,125.080002,124.550003,123.940002,124.050003,123.129997,123.849998,124.830002,126.209999,126.519997,125.940002,126.099998,127.07,129.389999,128.460007,129.649994,130.240005,129.210007,131.619995,133.229996,132.929993,132.809998,133.350006,134.350006,135.869995,135.759995,137.75,140.070007,142.660004,140.669998,142.649994,144,143.630005,147.679993,147.089996,145.880005,141.669998,142.960007,144.630005,145.809998,146.919998,147.699997,145.550003,142.539993,144.580002,144.110001,145.25,145.179993,146.279999,146.169998,145.630005,145.520004,145.300003,145.529999,145.839996,148.270004,146.470001,149.089996,146.149994,144.5,146.779999,147.889999,149.149994,147.800003,147.509995,146.830002,148.610001,151.289993,152.339996,152.399994,153.089996,154.389999,153.979996,153.949997,148.699997,148.75,146.910004,146.369995,147.220001,145.759995,141.270004,142.779999,143.699997,145.639999,145.559998,143.820007,141.690002,142.029999,141.279999,139.110001,138.270004,139.360001,138.369995,142.720001,142.559998,141.809998,141.039993,139.199997,141.509995,143.509995,143.160004,146.550003,148.119995,147.869995,148.639999,147.619995,149.009995,148.490005,149.720001,146.410004,147.800003,148.649994,149.820007,150.639999,150.059998,150.160004,150.059998,147.850006,147.679993,147.479996,149.429993,149.339996,150.990005,153.050003,156.529999,161,159.059998,159.639999,156.360001,158.789993,159.919998,164.529999,157.800003,159.720001,164.279999,168.339996,170.699997,173.919998,174.690002,175.529999,172.210007,172.309998,170.75,169.690002,167.460007,169.119995,172.149994,175.270004,177.070007,178.529999,178.139999,178.089996,177.259995,177.710007,179.119995,174.639999,171.639999,171.029999,168.169998,170.820007,174.820007,171.789993,171.089996,169.410004,165.940002,164.179993,162.300003,154.699997,157.020004,157.820007,158.279999,162.800003,169.509995,172.309998,173.330002,172.119995,170.679993,170.949997,171.429993,174.899994,171.550003,168.039993],[40.677502,41.084999,41.842499,43.247501,43.107498,42.962502,42.767502,43.125,43.875,44.7425,44.5975,44.529999,43.75,44.052502,44.205002,44.1675,43.7575,44.235001,44.994999,45.43,44.9925,44.610001,44.662498,44.505001,43.825001,43.810001,42.817501,42.212502,41.235001,43.192501,42.084999,41.619999,41.945,41.669998,42.0975,42.9025,43.200001,42.095001,42.512501,43.3125,43.110001,43.535,43.682499,43.955002,44.560001,44.459999,43.200001,41.43,41.310001,40.735001,40.912498,41.055,40.580002,41.314999,42.275002,44.142502,44.2225,45.9575,46.290001,46.512501,46.84,47.509998,47.147499,47.037498,46.610001,47.044998,46.747501,46.577499,46.907501,46.790001,47.09,47.037498,47.145,46.974998,46.875,46.717499,47.560001,47.9575,48.327499,48.494999,48.365002,47.924999,47.807499,48.07,47.674999,47.700001,47.209999,47.185001,46.422501,46.625,46.365002,46.23,45.5425,46.107498,46.040001,46.375,46.2775,46.794998,45.98,46.349998,46.9925,47.645,47.587502,46.970001,47.7575,47.8325,47.727501,47.862499,47.599998,47.970001,47.860001,47.9025,48.25,48.705002,48.552502,47.744999,47.477501,47.572498,50.375,51.8475,51.997501,52.267502,51.7775,51.8125,52.220001,51.8825,52.217499,52.4375,52.560001,53.330002,54.395,53.865002,53.759998,53.762501,53.872501,54.040001,54.485001,54.924999,55.744999,56.2575,56.907501,57.09,56.717499,55.775002,55.325001,54.5825,55.962502,55.267502,56.602501,55.959999,54.470001,54.560001,54.592499,55.0075,54.415001,55.197498,55.547501,55.105,56.237499,56.435001,56.814999,57.32,58.017502,56.997501,56.072498,55.942501,56.717499,54.09,53.612499,55.5275,54.34,55.537498,55.297501,54.005001,54.827499,55.162498,55.682499,53.772499,54.950001,54.075001,53.060001,53.325001,54.715,55.555,51.869999,50.397499,50.942501,52.487499,52.122501,51.1175,48.5425,48.057499,46.700001,47.852501,48.3825,46.465,44.244999,44.195,43.072498,43.654999,43.560001,45.235001,44.887501,44.645,46.205002,44.172501,43.68,42.122501,42.400002,42.157501,42.275002,42.737499,41.369999,40.985001,41.517502,40.2225,39.2075,37.682499,36.7075,39.2925,39.037498,39.057499,39.435001,39.48,35.547501,37.064999,36.982498,37.6875,38.327499,38.450001,38.072498,37.5,38.267502,38.735001,38.965,39.205002,38.325001,38.48,38.174999,39.439999,39.075001,38.669998,41.3125,41.610001,41.630001,42.8125,43.544998,43.560001,42.735001,42.602501,42.357498,42.7225,42.544998,42.700001,42.605,42.732498,43.0075,42.764999,43.2425,43.557499,43.5825,43.717499,43.287498,43.7425,43.962502,43.8825,43.630001,43.125,43.227501,44.724998,45.227501,45.427502,45.932499,46.529999,47.005001,46.6325,47.040001,48.772499,47.762501,47.185001,46.697498,47.1175,47.18,47.487499,47.810001,48.505001,48.837502,48.922501,49.25,50.025002,49.875,50.154999,49.737499,49.717499,49.807499,49.8125,50.782501,50.965,51.1325,51.869999,51.790001,51.32,51.075001,51.1525,50.1675,52.630001,52.287498,52.9375,52.119999,50.715,50.724998,50.18,49.294998,46.43,47.165001,47.73,47.52,47.25,45.772499,46.650002,45.695,44.915001,44.7425,44.557499,44.345001,44.575001,43.767502,43.325001,44.91,45.634998,46.305,47.537498,48.145,48.702499,48.547501,48.537498,48.185001,48.4725,49.612499,49.467499,49.865002,49.695,49.645,48.892502,49.950001,49.935001,49.48,50.387501,50.682499,51.102501,51.057499,50.005001,50.310001,50.807499,50.4375,50.825001,51.302502,51.125,50.837502,51.415001,50.647499,51.805,52.209999,52.1675,51.755001,51.935001,52.419998,52.195,53.259998,52.107498,51.005001,48.334999,49.25,49.759998,50.857498,50.247501,50.119999,52.2425,50.6875,50.435001,51.625,52.587502,52.59,53.16,53.115002,50.66,51.622501,51.040001,51.3825,52.252499,52.185001,51.424999,52.297501,53.32,53.314999,53.5425,54.174999,55.897499,55.772499,54.6875,54.974998,55.174999,55.692501,55.240002,54.432499,54.68,54.419998,55.2575,54.9725,54.705002,55.9925,56.147499,54.740002,55.205002,56.752499,56.764999,56.099998,56.7575,57.522499,59.052502,58.967499,58.830002,58.592499,58.82,59.102501,60.127499,59.990002,60.794998,60.895,61.645,62.262501,60.822498,60.814999,62.189999,63.955002,64.375,64.282501,64.309998,64.857498,65.035004,65.550003,65.489998,66.1175,65.660004,66.440002,66.775002,66.572502,65.797501,65.502502,65.445,66.592499,66.072502,66.959999,66.8125,66.040001,64.862503,65.434998,66.394997,67.677498,66.730003,67.120003,67.692497,67.864998,68.787498,69.964996,70.102501,69.934998,70.004997,69.860001,71,71.067497,72.477501,72.449997,72.879997,73.412498,75.087502,74.357498,74.949997,74.597504,75.797501,77.407501,77.582497,79.239998,78.169998,77.834999,78.809998,79.682503,79.142502,79.425003,79.807503,79.577499,77.237503,79.422501,81.084999,80.967499,77.377502,77.165001,79.712502,80.362503,81.302498,80.0075,80.387497,79.902496,81.800003,81.217499,81.237503,79.75,80.904999,80.074997,78.262497,74.544998,72.019997,73.162498,68.379997,68.339996,74.702499,72.330002,75.684998,73.230003,72.2575,66.542503,71.334999,68.857498,62.057499,69.4925,60.552502,63.215,61.6675,61.195,57.310001,56.092499,61.720001,61.380001,64.610001,61.935001,63.702499,63.572498,60.227501,61.232498,60.352501,65.6175,64.857498,66.517502,66.997498,68.3125,71.762497,71.107498,71.672501,70.699997,69.232498,67.092499,69.025002,68.7575,70.7425,70.792503,69.644997,71.932503,73.449997,72.267502,73.290001,74.389999,75.157501,75.934998,77.532501,78.752502,77.852501,76.912498,77.385002,76.927498,78.739998,78.285004,79.807503,79.212502,79.722504,79.182503,79.527496,79.5625,79.485001,80.462502,80.834999,81.279999,80.580002,82.875,83.364998,85.997498,88.209999,83.974998,84.699997,85.747498,88.019997,87.897499,87.932503,87.43,89.717499,91.6325,90.014999,91.209999,88.407501,90.445,91.199997,91.027496,91.027496,93.462502,93.172501,95.342499,95.752502,95.919998,95.477501,97.057503,97.724998,96.522499,96.327499,98.357498,97,97.272499,92.845001,92.614998,94.809998,93.252502,95.040001,96.190002,106.260002,108.9375,109.665001,110.0625,113.902496,111.112503,112.727501,109.375,113.010002,115.010002,114.907501,114.607498,115.5625,115.707497,118.275002,124.370003,125.857498,124.824997,126.522499,125.010002,124.807503,129.039993,134.179993,131.399994,120.879997,120.959999,112.82,117.32,113.489998,112,115.360001,115.540001,112.129997,110.339996,106.839996,110.080002,111.809998,107.120003,108.220001,112.279999,114.959999,114.089996,115.809998,116.790001,113.019997,116.5,113.160004,115.080002,114.970001,116.970001,124.400002,121.099998,121.190002,120.709999,119.019997,115.980003,117.510002,116.870003,115.75,115.040001,115.050003,116.599998,111.199997,115.32,108.860001,108.769997,110.440002,114.949997,119.029999,118.690002,116.32,115.970001,119.489998,119.209999,119.260002,120.300003,119.389999,118.029999,118.639999,117.339996,113.849998,115.169998,116.029999,116.589996,119.050003,122.720001,123.080002,122.940002,122.25,123.75,124.379997,121.779999,123.239998,122.410004,121.779999,127.879997,127.809998,128.699997,126.660004,128.229996,131.880005,130.960007,131.970001,136.690002,134.869995,133.720001,132.690002,129.410004,131.009995,126.599998,130.919998,132.050003,128.979996,128.800003,130.889999,128.910004,127.139999,127.830002,132.029999,136.869995,139.070007,142.919998,143.160004,142.059998,137.089996,131.960007,134.139999,134.990005,133.940002,137.389999,136.759995,136.910004,136.009995,135.389999,135.130005,135.369995,133.190002,130.839996,129.710007,129.869995,126,125.860001,125.349998,120.989998,121.260002,127.790001,125.120003,122.059998,120.129997,121.419998,116.360001,121.089996,119.980003,121.959999,121.029999,123.989998,125.57,124.760002,120.529999,119.989998,123.389999,122.540001,120.089996,120.589996,121.209999,121.389999,119.900002,122.150002,123,125.900002,126.209999,127.900002,130.360001,133,131.240005,134.429993,132.029999,134.5,134.160004,134.839996,133.110001,133.5,131.940002,134.320007,134.720001,134.389999,133.580002,133.479996,131.460007,132.539993,127.849998,128.100006,129.740005,130.210007,126.849998,125.910004,122.769997,124.970001,127.449997,126.269997,124.849998,124.690002,127.309998,125.43,127.099998,126.900002,126.849998,125.279999,124.610001,124.279999,125.059998,123.540001,125.889999,125.900002,126.739998,127.129997,126.110001,127.349998,130.479996,129.639999,130.149994,131.789993,130.460007,132.300003,133.979996,133.699997,133.410004,133.110001,134.779999,136.330002,136.960007,137.270004,139.960007,142.020004,144.570007,143.240005,145.110001,144.5,145.639999,149.149994,148.479996,146.389999,142.449997,146.149994,145.399994,146.800003,148.559998,148.990005,146.770004,144.979996,145.639999,145.860001,145.520004,147.360001,146.949997,147.059998,146.139999,146.089996,145.600006,145.860001,148.889999,149.100006,151.119995,150.190002,146.360001,146.699997,148.190002,149.710007,149.619995,148.360001,147.539993,148.600006,153.119995,151.830002,152.509995,153.649994,154.300003,156.690002,155.110001,154.070007,148.970001,149.550003,148.119995,149.029999,148.789993,146.059998,142.940002,143.429993,145.850006,146.830002,146.919998,145.369995,141.910004,142.830002,141.5,142.649994,139.139999,141.110001,142,143.289993,142.899994,142.809998,141.509995,140.910004,143.759995,144.839996,146.550003,148.759995,149.259995,149.479996,148.690002,148.639999,149.320007,148.850006,152.570007,149.800003,148.960007,150.020004,151.490005,150.960007,151.279999,150.440002,150.809998,147.919998,147.869995,149.990005,150,151,153.490005,157.869995,160.550003,161.020004,161.410004,161.940002,156.809998,160.240005,165.300003,164.770004,163.759995,161.839996,165.320007,171.179993,175.080002,174.559998,179.449997,175.740005,174.330002,179.300003,172.259995,171.139999,169.75,172.990005,175.639999,176.279999,180.330002,179.289993,179.380005,178.199997,177.570007,182.009995,179.699997,174.919998,172,172.169998,172.190002,175.080002,175.529999,172.190002,173.070007,169.800003,166.229996,164.509995,162.410004,161.619995,159.779999,159.690002,159.220001,170.330002,174.779999,174.610001,175.839996,172.899994,172.389999,171.660004,174.830002,176.279999,172.119995,168.639999],[243278000,130196800,162579600,204588800,160704400,135722000,149886400,123967600,135249600,152648800,155712400,151128400,195208000,153816000,113605600,95154000,126814000,95096400,128740800,128828400,126774000,117473600,90975200,157618800,133787200,78597600,148219600,165963200,164115200,150164800,163690400,166674000,153594000,150347200,121112000,138422000,107732800,140021200,116070800,113634400,89726400,91557200,100497200,86313600,106421600,83018000,139235200,261964400,146062000,134768000,113528400,111852000,142623200,169709600,214277600,266157600,136272800,224805200,169805600,113611200,92844800,111957200,104848800,83115200,94780800,76732400,69176000,73190800,73603200,60962800,80233600,92936000,69844000,90056400,74762000,109931200,93770000,105064800,86264000,83734400,85388800,106627200,73234000,67644400,86553600,86440400,246876800,73939600,134314000,82514800,102847600,108801600,126652400,98276800,101141200,69460800,90950800,70925200,55819200,66416800,69940800,79026400,63756400,75326000,72164400,50055600,60172400,62138000,65573600,81147200,82704800,63957600,74791600,66839600,76304000,96096000,84118000,157492000,271742800,249616000,133789600,101701600,102349600,90102000,93970400,98444800,103563600,82992000,115230400,114001600,141708000,121150800,104639200,76072400,75532800,73905600,82100400,91107200,109019200,195175200,173360400,109560400,133332000,137160000,150479200,158066000,142996000,197114800,166825600,127997200,148780400,126286800,108495200,106435200,384986800,110773600,98217600,95938800,120724800,91717600,94403200,99152800,114619200,128168000,134322000,118655600,107564000,167962400,212497600,161351600,123164000,116736000,91541600,130325200,132314800,115168400,155071200,163702000,119423200,189033600,183742000,146640000,153435600,233292800,365314800,264654800,127531600,133697600,101450400,137463200,204542000,187531600,243204000,185915200,147713200,167701200,271300800,124496800,94496000,179994000,165549600,184250000,167080000,158126000,163210000,165377200,172393600,169126400,248104000,189126800,142510800,127594400,162814800,177151600,135366000,196189200,259092000,382978400,148676800,234330000,212468400,169165600,140014000,148158800,365248800,234428400,219111200,164101200,180396400,143122800,108092800,129756800,114843600,122278800,119284800,135004000,121576000,92522400,101766000,134142000,104768400,166348800,244439200,162958400,130672400,125982000,144406400,112958400,126966800,95280000,83973600,89134000,89960800,87342800,98507200,75891200,104457600,68998800,75652800,87493600,68280800,111341600,112861600,103544800,109744800,78949600,83241600,99185600,95997600,128044000,129870400,124130000,94318000,156171600,104879200,126585600,124140800,204136800,169630800,175381200,199202000,119393600,83121600,94256000,111448000,91062800,93087200,76457200,74106400,103526800,143072800,86781200,83603200,111042800,70146400,102785600,115627200,96783200,77758000,93292000,70162400,74172800,74596400,88818800,186139600,259309200,127985200,83569600,129772400,155054800,105358000,139634400,164834800,229722400,146118800,106178800,132125600,131516400,154449200,113459200,118994400,146118800,94858800,111792800,113924800,84873600,108174400,161584400,123872000,119093600,90105200,122737600,104883600,107731600,73012800,86698400,75046000,58676400,106204000,84496800,86056000,191202400,72881600,84281200,104270000,83598800,124442400,109012000,67740800,45448000,69062000,101354400,82312000,71588400,80767200,70380800,67789600,67467200,56430000,74162400,83717200,89111600,73420800,59966400,55638400,70475600,86693600,135742800,277125600,216071600,163448400,209572000,143299200,133457600,108038000,98478800,89927600,188874000,146189600,108909600,110481600,97654400,107537200,86141600,89014800,187272000,104174400,103493200,63755200,83962000,84573600,80092000,76752400,95654800,77449200,109237600,127111600,177158400,128906800,159053200,84632400,73274800,101360000,88242400,221652400,76662000,124763200,87613600,75334000,101408000,103909600,139223200,138449200,114426000,138478800,122306000,111820000,74770400,113013600,166795600,96427600,87360000,73903200,67585200,97433600,87247200,82293600,75828800,69275200,73477200,96572800,142839600,124522000,139162000,151125200,103272000,79897600,75864400,94940400,69986400,81821200,87388800,102734400,89182800,100206400,86703200,76167200,106234400,121395200,65325200,84020400,105207600,65235600,46617600,94487200,114430400,67181600,74424400,106075600,128042400,90420400,78756800,137310400,133587600,128186000,114158400,116028400,98369200,275978000,98572000,48478800,93121200,146266000,144114400,100805600,135480400,146322800,118387200,108872000,132079200,170108400,140644800,121532000,161954400,121923600,108829200,137816400,110843200,101832400,104472000,146537600,161940000,162234000,216229200,126743200,199588400,173788400,136616400,118826800,105425600,117684000,109348800,94323200,113730400,94747600,80113600,152531200,93984000,100566000,129554000,222195200,230673600,198054800,320605600,426510000,341397200,319475600,219178400,187572800,226176800,286744800,285290000,255598800,418474000,370732000,322423600,324056000,300233600,271857200,401693200,336752800,287531200,303602000,252087200,204216800,167976400,197002000,176218400,165934000,129880000,201820400,202887200,168895200,161834800,131022800,194994800,131154400,157125200,215250000,130015200,180991600,116862400,124814400,126161200,117087600,112004800,137280800,183064000,240616800,133568000,147751200,142333600,115215200,133838400,145946400,162301200,200622400,158929200,166348400,135178400,101729600,111504800,102688800,81803200,125522000,112945200,133560800,153532400,80791200,87642800,104491200,87560400,137250400,95654400,147712400,166651600,201662400,200146000,138808800,165428800,114406400,96820400,264476000,135445200,212155600,192623200,137522400,205256800,130646000,140223200,110737200,114041600,118655600,112424400,117092000,125642800,90257200,191649200,170989200,153198000,110577600,92186800,90318000,103433200,89001600,197004400,185438800,121214000,103625600,90329200,158130000,374336800,308151200,173071600,121776800,202428800,198045600,212403600,187902400,165598000,210082000,165565200,119561600,105633600,145538000,126907200,338054800,345937600,211495600,163022400,155552400,187630000,225702700,151948100,200119000,257599600,332607200,231366600,176940500,182274400,180860300,140150100,184642000,154679000,178011000,287104900,195713800,183055400,150718700,167743300,149981400,137672400,99382200,142675200,116120400,144712000,106243800,161498200,96849000,83477200,100506900,240226800,262330500,150712000,112559200,115393800,120639300,124423700,89946000,101988000,82572600,111850700,92276800,143937800,146129200,190272600,122866900,107624400,138235500,126387100,114457900,154515300,138023400,112295000,103162300,81581900,91183000,74271000,76322100,74113000,73604300,127959300,113874200,76499200,46691300,169410200,127728200,89004200,78967600,78260400,86712000,82225500,115089200,81312200,86939800,79184500,157243700,98208600,94359800,192541500,121251600,168904800,88223700,54930100,124486200,121047300,96452100,99116600,143301900,97664900,155088000,109578200,105158200,100384500,91951100,88636800,90221800,111598500,90757300,104319500,120150900,114459400,157611700,98390600,140843800,142621100,177523800,106239800,83305400,89880900,84183100,75693800,71297200,76774200,73046600,64280000,60145100,80576300,97918500,96856700,87668800,103916400,158273000,111039900,148199500,164560400,116307900,102260900,112966300,178155000,153766600,154376600,129525800,111943300,103026500,88105100,92403800,115227900,111932600,121229700,185549500,111912300,95467100,88530500,98844700,94071200,80819200,85671900,118323800,75089100,88651200,80171300,83466700,88844600,106686700,91420000,91266500,87222800,89347100,84922400,94264200,94812300,68847100,84566500,78657500,66905100,66015800,107760100,151101000,109839500,75135100,137564700,84000900,78128300,78973300,88071200,126142800,112172300,105861300,81918000,74244600,63342900,92612000,76857100,79295400,63092900,72009500,56575900,94625600,71311100,67637100,59278900,76229200,75169300,71057600,74403800,56877900,71186400,53522400,96906500,62746300,91815000,96721700,108953300,79663300,74783600,60214200,68711000,70783700,62111300,64556100,63261400,52485800,78852600,108181800,104911600,105575500,99890800,76299700,100827100,127050800,106820300,93251400,121434600,96350000,74993500,77338200,71447400,72434100,104818600,118931200,56699500,70382000,62880000,64786600,56368300,46397700,54067400,48908700,69023100,48493500,72282600,59318800,103296000,92229700,86326000,86960300,59947400,60131800,48606400,58991300,48597200,55721500,90956700,86453100,80313700,71115500,57808700,82278300,74420200,57305700,140646400,102404300,109296300,83281300,68034100,129868800,123478900,75834000,76404300,64838200,53477900,74150700,108972300,74602000,88934200,94639600,98322000,80861100,83221100,61732700,58718700,64452200,73035900,78762700,69907100,67885200,85589200,76378900,58418800,61421000,58883400,50720600,60893400,56094900,100077900,124850400,74588300,69122000,54511500,60394600,65414600,55020900,56787900,65187100,41000000,63632600,59222800,59256200,88807000,137827700,117305600,117467900,96041900,69463600,76959800,88748200,174048100,152052500,136739200,117938300,107497000,120405400,116998900,108923700,115228100,153237000,139380400,131063300,150185800,195432700,107499100,91185900,92135300,68356600,74919600,79144300,62348900,59773000,64062300,104487900,99310400,94537600,96904000,86580100,106765600,76138300,74805200,84505800,80355000,90956700,94815000,91420500,122848900,162294600,115798400,108275300,121954600,179935700,115541600,86213900,84914300,89418100,82391400,77251200,74829200,71285000,90865900,98566000],[39.013626,39.404457,40.130978,41.4785,41.344238,41.205158,41.018135,41.361019,42.08033,42.912361,42.773281,42.708546,41.960449,42.250572,42.396843,42.36087,41.967644,42.425621,43.154522,43.571732,43.15213,42.785267,42.835629,42.684566,42.032383,42.017998,41.066093,40.485836,39.548332,41.425747,40.363552,39.917576,40.229279,39.96553,40.375542,41.147617,41.432949,40.37315,40.773567,41.540852,41.346634,41.754242,41.895714,42.15707,42.737324,42.641418,41.432949,39.735352,39.620255,39.068768,39.239017,39.375687,38.92012,39.625053,40.545788,42.336895,42.41362,44.07766,44.396549,44.609951,44.924065,45.566654,45.393353,45.287441,44.875843,45.294659,45.008236,44.844551,45.162273,45.049152,45.337986,45.287441,45.390942,45.227261,45.130989,44.979347,45.790497,46.173214,46.529453,46.69072,46.565556,46.141918,46.02879,46.281528,45.901218,45.925301,45.453518,45.429459,44.695324,44.890289,44.639961,44.509975,43.848053,44.392044,44.327053,44.64959,44.555717,45.053963,44.269287,44.625523,45.244118,45.872341,45.816971,45.222454,45.980648,46.05286,45.951767,46.081745,45.829014,46.185249,46.079342,46.120258,46.45483,46.892895,46.746078,45.968613,45.711079,45.802536,48.500759,49.918484,50.062904,50.322845,49.851086,49.884781,50.277126,50.127365,50.451031,50.663597,50.781952,51.525898,52.554882,52.042805,51.941345,51.943775,52.050056,52.21188,52.641827,53.06694,53.859203,54.35437,54.982384,55.158699,54.798805,53.888187,53.453415,52.736027,54.069347,53.397861,54.687702,54.066921,52.627335,52.714291,52.745682,53.146652,52.5742,53.330223,53.668392,53.240849,54.335049,54.525856,54.893005,55.38092,56.054825,55.069332,54.175629,54.050026,54.798805,52.260185,51.798836,53.649055,52.501736,53.658718,53.426838,52.17807,52.972744,53.29641,53.798809,51.953438,53.091106,52.245693,51.265038,51.521076,52.864056,53.675636,50.115288,48.692612,49.219166,50.711899,50.534962,49.56057,47.064003,46.593773,45.277626,46.395016,46.908875,45.049774,42.897392,42.848923,41.760605,42.325352,42.23325,43.857235,43.520325,43.285206,44.797699,42.827099,42.349602,40.839542,41.108589,40.873478,40.987392,41.43581,40.109959,39.736691,40.252972,38.99741,38.013321,36.534771,35.58947,38.095737,37.848499,37.867886,38.233894,38.277527,34.464794,35.936085,35.856094,36.539623,37.16013,37.278896,36.912891,36.35783,37.101952,37.555214,37.77821,38.010906,37.1577,37.30798,37.012264,38.238739,37.884865,37.492191,40.054211,40.342659,40.362041,41.508526,42.218708,42.23325,41.433392,41.482075,41.243511,41.598907,41.426075,41.577003,41.484501,41.60865,41.876423,41.640297,42.105244,42.411942,42.436298,42.567741,42.149052,42.592087,42.806305,42.728409,42.482548,41.990826,42.09063,43.54874,44.038033,44.232773,44.724495,45.306278,45.768784,45.406082,45.802864,47.489799,46.506355,45.944054,45.469364,45.878326,45.939178,46.23859,46.552616,47.229336,47.553093,47.635853,47.954742,48.709362,48.563297,48.835934,48.42942,48.409946,48.497581,48.502453,49.446938,49.624638,49.787727,50.505836,50.427937,49.970299,49.731743,49.807205,48.84811,51.245846,50.912354,51.545265,50.749256,49.381207,49.390942,48.860283,48.183395,45.383011,46.101433,46.653694,46.448421,46.184509,44.74033,45.598042,44.664585,43.902168,43.733559,43.552727,43.34502,43.569839,42.780548,42.348022,43.897274,44.605934,45.260822,46.465527,47.05933,47.604259,47.452766,47.442982,47.098431,47.37944,48.49374,48.352009,48.740547,48.574387,48.525505,47.789978,48.823631,48.808971,48.364235,49.251263,49.539616,49.950142,49.906155,48.877396,49.175514,49.661789,49.300133,49.678898,50.145634,49.97213,49.691124,50.255596,49.505402,50.636806,51.032673,50.991119,50.587933,50.76387,51.237934,51.018005,52.058994,50.93248,49.854836,47.245049,48.139416,48.637909,49.710667,49.301029,49.175934,51.258446,49.732746,49.484993,50.65258,51.596954,51.599403,52.158669,52.114513,49.705765,50.650127,50.078602,50.414654,51.268261,51.20203,50.456348,51.312408,52.315655,52.310745,52.533966,53.154549,54.844608,54.721958,53.657391,53.939476,54.135712,54.643467,54.199493,53.407204,53.650036,53.394928,54.21666,53.937027,53.674576,54.937809,55.08989,53.708908,54.16515,55.683495,55.695763,55.043289,55.6884,56.438992,57.940178,57.856773,57.721874,57.488842,57.712059,57.989231,58.994923,58.860016,59.64986,59.747974,60.483841,61.08971,59.676838,59.669476,61.018581,62.750332,63.162426,63.071663,63.09866,63.826881,64.001572,64.508385,64.449333,65.066864,64.616653,65.384254,65.713905,65.514633,64.751945,64.461632,64.405052,65.534309,65.022583,65.895966,65.750824,64.990601,63.831814,64.39521,65.339951,66.602066,65.669632,66.053436,66.616837,66.786591,67.694435,68.85321,68.988541,68.8237,68.892586,68.749886,69.871773,69.938194,71.325806,71.298737,71.721893,72.245949,73.894341,73.175926,73.759003,73.412117,74.59304,76.177452,76.34967,77.980835,76.927841,76.59816,77.557678,78.416298,77.884888,78.162903,78.53933,78.312988,76.01017,78.160446,79.796509,79.680885,76.147942,75.93882,78.445831,79.08551,80.010574,78.923019,79.297852,78.81942,80.691216,80.116608,80.136345,78.668999,79.808334,78.989594,77.201668,73.534546,71.04377,72.170776,67.453102,67.413658,73.689919,71.349571,74.659096,72.237381,71.278053,65.640533,70.368057,67.924149,61.21632,68.550545,59.731716,62.358128,60.831604,60.365501,56.533173,55.332169,60.883392,60.548,63.734219,61.095474,62.839012,62.710785,59.411125,60.4025,59.534428,64.728058,63.978367,65.615868,66.089363,67.386528,70.789764,70.143631,70.700981,69.741669,68.29406,66.18306,68.089378,67.8255,69.783592,69.832916,68.700966,70.957466,72.454384,71.287926,72.29657,73.381653,74.138748,74.905716,76.688599,77.895309,77.005104,76.075348,76.542702,76.090172,77.882942,77.432907,78.938828,78.350304,78.854752,78.320625,78.661873,78.696487,78.619843,79.586708,79.955139,80.395294,79.702919,81.972939,82.457596,85.061455,87.24987,83.060959,83.778076,84.814163,87.061935,86.940765,86.975395,86.478363,88.740959,90.635117,89.035233,90.217216,87.445229,89.460548,90.207321,90.03669,90.03669,92.445206,92.158356,94.304741,94.710281,94.875954,94.438278,96.001076,96.661308,95.471901,95.279015,97.286919,95.944191,96.213715,91.834419,91.606918,93.778023,92.237488,94.005524,95.143005,105.103409,107.75177,108.471352,108.864517,112.662712,110.101242,111.701553,108.379547,111.981483,113.963264,113.861702,113.564423,114.510742,114.654411,117.198555,123.238091,124.712036,123.688942,125.370995,123.872269,123.6716,127.865578,132.958786,130.204102,119.779846,119.859116,111.793205,116.252251,112.4571,110.980667,114.310081,114.488449,111.109474,109.335777,105.867622,109.078148,110.792389,106.145073,107.235069,111.258118,113.913719,113.051643,114.755989,115.727074,111.991386,115.439705,112.130104,114.032646,113.923622,115.905441,123.267815,119.997841,120.087021,119.611397,117.936768,114.924438,116.440514,115.806351,114.696533,113.992996,114.002907,115.538795,110.187935,114.270454,107.869255,107.780052,109.43486,113.903816,117.946678,117.812683,115.460205,115.112793,118.606766,118.328835,118.378471,119.410782,118.507507,117.157555,117.763054,116.472664,113.008446,114.318695,115.172348,115.728203,118.170029,121.812904,122.170235,122.031281,121.346375,122.835281,123.460625,120.879845,122.329056,121.505188,120.879845,126.934746,126.865265,127.74868,125.723778,127.282166,130.905182,129.991989,130.994507,135.679642,133.873093,132.731583,131.709213,128.453476,130.041611,125.664223,129.952286,131.073944,128.026627,127.847946,129.922516,127.957153,126.200226,126.885124,131.054077,135.858276,138.042053,141.863586,142.101822,141.009933,136.076675,130.984604,133.148483,133.992203,132.949966,136.374466,135.95195,136.101074,135.206406,134.590057,134.331604,134.570175,132.403061,130.066956,128.943634,129.102661,125.255539,125.116371,124.609375,120.275139,120.543549,127.034966,124.380737,121.338821,119.420227,120.702599,115.672493,120.37455,119.271111,121.23941,120.314903,123.257423,124.828072,124.022873,119.817856,119.281052,122.660965,121.815987,119.380455,119.877495,120.493843,120.672783,119.191589,121.428284,122.27327,125.156136,125.464302,127.144318,129.589798,132.214172,130.4646,133.635727,131.249908,133.705307,133.36734,134.04332,132.323532,132.711227,131.160461,133.526382,133.924026,133.595963,132.790756,132.691345,130.683289,131.756882,127.094612,127.34314,128.97345,129.660538,126.314713,125.378685,122.25193,124.44265,126.91217,125.737152,124.323151,124.163826,126.772774,124.900703,126.56366,126.364502,126.314713,124.751335,124.084167,123.755562,124.532265,123.018684,125.358765,125.368721,126.205177,126.593529,125.577835,126.812599,129.929382,129.092926,129.600784,131.233856,129.909485,131.741714,133.414612,133.135818,132.847031,132.548309,134.211243,135.7547,136.382065,136.69075,139.3694,141.4207,143.959946,142.635544,144.497665,143.890228,145.025421,148.520599,147.853439,145.772247,141.848877,145.533264,144.786423,146.180511,147.93309,148.361282,146.15065,144.368195,145.025421,145.244492,144.905945,146.738159,146.329895,146.439438,145.741333,145.691467,145.20282,145.462097,148.483841,148.693283,150.707748,149.780304,145.960739,146.299805,147.785751,149.30162,149.211838,147.955276,147.137512,148.194641,152.702301,151.415817,152.093964,153.23085,153.879089,156.262573,154.686874,153.649719,148.563614,149.142044,147.715927,148.623444,148.384094,145.66156,142.550079,143.038727,145.452133,146.429474,146.519211,144.973434,141.522873,142.440384,141.113998,142.260864,138.760437,140.725067,141.61264,142.899109,142.510178,142.420425,141.123978,140.52562,143.367828,144.444885,146.150223,148.354187,148.852829,149.07222,148.284393,148.234528,148.912674,148.443954,152.153809,149.391357,148.55365,149.610764,151.076767,150.548203,151.087509,150.248581,150.618103,147.731781,147.681839,149.799149,149.809128,150.807861,153.294693,157.669113,160.345718,160.815109,161.20462,161.733948,156.610474,160.036102,165.089676,164.560349,163.55162,161.634064,165.10965,170.962173,174.857224,174.337875,179.221664,175.516388,174.108185,179.071854,172.040802,170.922241,169.533997,172.769882,175.416504,176.055695,180.10054,179.061859,179.151749,177.973251,177.344055,181.778397,179.471344,174.697418,171.781143,171.950928,171.970901,174.857224,175.306641,171.970901,172.849792,169.583939,166.018478,164.300659,162.203354,161.414337,159.576691,159.486801,159.01741,170.113266,174.557602,174.387817,175.616257,172.679993,172.389999,171.660004,174.830002,176.279999,172.119995,168.639999]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>symbol<\\/th>\\n      <th>date<\\/th>\\n      <th>open<\\/th>\\n      <th>high<\\/th>\\n      <th>low<\\/th>\\n      <th>close<\\/th>\\n      <th>volume<\\/th>\\n      <th>adjusted<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[3,4,5,6,7,8]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\r\nGetting Signals\r\r\n\r\r\n\r\r\nStock_data <- Stock_data %>%\r\r\n  group_by(symbol) %>%\r\r\n  tq_mutate(select = close,\r\r\n            mutate_fun = MACD,\r\r\n            col_rename = c(\"MACD\", \"Signal\"))%>%\r\r\n  tq_mutate(select = adjusted, mutate_fun = RSI) %>%\r\r\n  tq_mutate(select = adjusted, mutate_fun = BBands, col_rename = \"Bbands\") %>%\r\r\n  tq_mutate_xy(x = close, y =volume, mutate_fun = EVWMA, col_rename = \"EVWMA\") %>%\r\r\n  mutate(Stock_movement = case_when(\r\r\n    lag(close)- close > 0 ~ \"Decrease\",\r\r\n    close > lag(close) ~ \"Increase\",\r\r\n    close == lag(close) ~\"No Change\"\r\r\n  ) \r\r\n           ) %>%\r\r\n  select(symbol:adjusted,Stock_movement,MACD,rsi,EVWMA)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n#str(Stock_data)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nStock_data <- Stock_data %>% mutate(forecast1day = lead(close,1),\r\r\n                                    forecast1week = lead(close,7),\r\r\n                                    forecast2week = lead(close,14),\r\r\n                                    forecast4week = lead(close,31),\r\r\n                                    forecast2month = lead(close,60),\r\r\n                                    forecast4month = lead(close,120))\r\r\n\r\r\nStock_data %>% head(10) %>% select(close, forecast1day,forecast1week)\r\r\n\r\r\n\r\r\n# A tibble: 10 x 4\r\r\n# Groups:   symbol [1]\r\r\n   symbol close forecast1day forecast1week\r\r\n   <chr>  <dbl>        <dbl>         <dbl>\r\r\n 1 AAPL    40.7         41.1          43.1\r\r\n 2 AAPL    41.1         41.8          43.9\r\r\n 3 AAPL    41.8         43.2          44.7\r\r\n 4 AAPL    43.2         43.1          44.6\r\r\n 5 AAPL    43.1         43.0          44.5\r\r\n 6 AAPL    43.0         42.8          43.8\r\r\n 7 AAPL    42.8         43.1          44.1\r\r\n 8 AAPL    43.1         43.9          44.2\r\r\n 9 AAPL    43.9         44.7          44.2\r\r\n10 AAPL    44.7         44.6          43.8\r\r\n\r\r\nStock_data <- Stock_data[complete.cases(Stock_data),]\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n#Stock_data %>% colnames()\r\r\n\r\r\n\r\r\n\r\r\ncurrent <- lm(close ~ volume + MACD+ rsi+ EVWMA , data = Stock_data)\r\r\nforecast1daymodel <- lm(forecast1day ~ volume + MACD+ rsi+ EVWMA , data = Stock_data)\r\r\nforecast1weekmodel <- lm(forecast1week ~ volume + MACD+ rsi+ EVWMA , data = Stock_data)\r\r\nforecast2weekmodel <- lm(forecast2week ~ volume + MACD+ rsi+ EVWMA , data = Stock_data)\r\r\nforecastmonthmodel <-lm(forecast4week~ volume + MACD+ rsi+ EVWMA , data = Stock_data)\r\r\nforecast2monthmodel <- lm(forecast2month~ volume + MACD+ rsi+ EVWMA , data = Stock_data)\r\r\nforecast4monthmodel <- lm(forecast4month~ volume + MACD+ rsi+ EVWMA , data = Stock_data)\r\r\n#summary(current)\r\r\n#summary(forecast1weekmodel)\r\r\n#summary(forecast2weekmodel)\r\r\n#summary(forecast1weekmodel)\r\r\n#summary(forecastmonthmodel)\r\r\n#summary(forecast2monthmodel)\r\r\n#summary(forecast4monthmodel)\r\r\n\r\r\nStock_data$currentPrediction <- predict(current, Stock_data)\r\r\nStock_data$day1prediction <- predict(forecast1daymodel, Stock_data)\r\r\nStock_data$day7prediction <- predict(forecast1weekmodel, Stock_data)\r\r\nStock_data$day14prediction <- predict(forecast2weekmodel, Stock_data)\r\r\nStock_data$day31prediction <- predict(forecastmonthmodel, Stock_data)\r\r\n\r\r\ncorrelations <- c(cor(Stock_data$currentPrediction,Stock_data$close),cor(Stock_data$day1prediction, Stock_data$forecast1day),cor(Stock_data$day7prediction, Stock_data$forecast1week),cor(Stock_data$day14prediction, Stock_data$forecast2week),cor(Stock_data$day31prediction, Stock_data$forecast4week))\r\r\n\r\r\nPredictions_cor <- c(\"Current\", \"1dayModel\",\"1weekModel\",\"2weekModel\",\"1monthModel\")\r\r\n\r\r\ncbind(Predictions_cor,round(correlations,3)) %>% knitr::kable()\r\r\n\r\r\n\r\r\nPredictions_cor\r\r\n\r\r\nCurrent\r\r\n0.999\r\r\n1dayModel\r\r\n0.998\r\r\n1weekModel\r\r\n0.992\r\r\n2weekModel\r\r\n0.985\r\r\n1monthModel\r\r\n0.968\r\r\n\r\r\nStock_data %>% ggplot()+\r\r\n  geom_point(aes(x = date, Stock_data$day7prediction), color =\"red\")+\r\r\n  geom_point(aes(x = date, Stock_data$forecast2week), color =\"blue\")\r\r\n\r\r\n\r\r\n\r\r\nStock_data %>% ggplot()+\r\r\n  geom_point(aes(x = date, Stock_data$day31prediction), color =\"red\")+\r\r\n  geom_point(aes(x = date, Stock_data$forecast4week), color =\"blue\")\r\r\n\r\r\n\r\r\n\r\r\nStock_data %>% ggplot()+\r\r\n  geom_point(aes(x = date, Stock_data$day7prediction), color =\"red\")+\r\r\n  geom_point(aes(x = date, Stock_data$forecast1week), color =\"blue\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nstart_date <- Sys.Date()\r\r\nstart_date\r\r\n\r\r\n\r\r\n[1] \"2022-02-12\"\r\r\n\r\r\nretrieval_date <- start_date - months(6)\r\r\n\r\r\nnew_stock_data <- tq_get(Stock, get = \"stock.prices\", from = retrieval_date, to = start_date)\r\r\n\r\r\n\r\r\nnew_stock_data <- new_stock_data %>%\r\r\n  group_by(symbol) %>%\r\r\n  tq_mutate(select = close,\r\r\n            mutate_fun = MACD,\r\r\n            col_rename = c(\"MACD\", \"Signal\"))%>%\r\r\n  tq_mutate(select = adjusted, mutate_fun = RSI) %>%\r\r\n  tq_mutate(select = adjusted, mutate_fun = BBands, col_rename = \"Bbands\") %>%\r\r\n  tq_mutate_xy(x = close, y =volume, mutate_fun = EVWMA, col_rename = \"EVWMA\") %>%\r\r\n  mutate(Stock_movement = case_when(\r\r\n    lag(close)- close > 0 ~ \"Decrease\",\r\r\n    close > lag(close) ~ \"Increase\",\r\r\n    close == lag(close) ~\"No Change\"\r\r\n  ) \r\r\n           ) %>%\r\r\n  select(symbol:adjusted,Stock_movement,MACD,rsi,EVWMA)\r\r\n\r\r\nnew_stock_data$monthforecast <- predict(forecastmonthmodel, new_stock_data)\r\r\nnew_stock_data$week2forecast <- predict(forecast2weekmodel, new_stock_data)\r\r\nnew_stock_data$month2forecast <- predict(forecast2monthmodel, new_stock_data)\r\r\nnew_stock_data$month4forecast <- predict(forecast4monthmodel, new_stock_data)\r\r\n\r\r\nnew_stock_data$monthforecastdate <- new_stock_data$date + days(31)\r\r\nnew_stock_data$week2forecastdate <- new_stock_data$date +days(14)\r\r\nnew_stock_data$month2forecastdate <- new_stock_data$date+ days(60)\r\r\nnew_stock_data$month4forecastdate <- new_stock_data$date +days(120)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nnew_stock_data%>% ggplot()+\r\r\n  geom_line(aes(x = monthforecastdate, y = monthforecast),color = \"red\")+\r\r\n  geom_line(aes(x = week2forecastdate, y = week2forecast),color = \"green\")+\r\r\n  geom_line(aes(x = date, y = close), color = \"blue\")+\r\r\n  geom_line(aes(x = month2forecastdate, y = month2forecast) ,color = \"orange\")+\r\r\n  geom_line(aes(x = month4forecastdate, y = month4forecast), color = \"gray\")\r\r\n\r\r\n\r\r\n\r\r\n#new_stock_data %>% select(monthforecastdate,monthforecast) %>% filter(monthforecastdate > \"2022-01-01\")\r\r\n\r\r\nnew_stock_data %>% ggplot()+geom_line(aes(x = month4forecastdate, y = month4forecast), color = 'red')+\r\r\n  geom_line(aes(x = date, y = close), color = 'blue')\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nx<-new_stock_data %>%\r\r\n  ggplot()+geom_line(data = new_stock_data   %>% filter(month2forecastdate > Sys.Date()),aes(x = month2forecastdate, y = month2forecast), color = 'red')+\r\r\n  geom_line(aes(x = date, y = close), color = 'blue')+\r\r\n  geom_line(data = new_stock_data %>% filter(month4forecastdate > Sys.Date()),aes(x = month4forecastdate,y = month4forecast), color = 'green')+\r\r\n  geom_line(data = new_stock_data %>% filter(monthforecastdate >Sys.Date()), aes(x = monthforecastdate,y = monthforecast), color = 'orange')+\r\r\n  labs(title = \"Price Predictions\")+\r\r\n  xlab(\"Date\")+\r\r\n  ylab(\"Price\")\r\r\n\r\r\nprint(x)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/stock-price-predictions/distill-preview.png",
    "last_modified": "2022-02-12T11:17:47-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/stock-returns/",
    "title": "Stock Returns",
    "description": "Manipulating Stock Returns in order to compare companies.",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-12",
    "categories": [],
    "contents": "\r\r\nIntroduction\r\r\nCovid has hit the world in many strange and unexpected ways. Recently the stock market has been very volatile. Gamestop stock is rising up to 400 a share from previous shares below 10 dollars. A failing video game store, that sells hard copy video games saw prices increase exponently due to a Reddit group. There are stories about college students making millions of dollars from investing using apps like Robinhood.\r\r\nDue to the ease of entering the market through free online platforms, there’s a whole new market of investors that are adding a new and unexpected force to the market.\r\r\nFor this paper, I am going to look into the best way to compare stocks! As there are many many different ways to compare stocks, I’m going to focus on a few key points that I find useful.\r\r\nData\r\r\nInstead of using pre gathered data, I’ve decided to gather the data my self. To get up to date stock data, I’m using the package tidyquant to gather data from Yahoo Finance.\r\r\nI’ve set up a vector named stocks of multiple stocks with with very different listed prices, such as Google with a current price of 2300 and MTNB (a pharmaceticul comapny) with a price of 90 cents.\r\r\nUpdating Dates\r\r\nWhen running this report, I use multiple different dates that depend on the current date. For example I am pulling initially 4 years of data starting in 2018. When looking at the data, for my daily and monthly returns, I may only care about 1 month of data or even 6 months of data. To do this, I will need to filter the date column based on my current date. To make this process easier and capable of being run at any date and pull fresh information. I’ve created date objects are are dependent on the current date. The current date is aquired using sys.date().\r\r\n\r\r\n\r\r\nToday = Sys.Date()\r\r\nmonth1_date = Today - months(1)\r\r\nmonth6_date = Today - months(6)\r\r\nYear_4_date = Today - years(4)\r\r\n\r\r\n\r\r\n\r\r\nMutating DATA\r\r\nRight now I have the following variables: Company symobl, date, open, high, low, close, volume and adjusted price. Using Tq_transmute, I will use the adjusted price to calculate the daily, weekly and monthly returns. This will allow me to look at the % returns and be useful when comparing stocks that have such different prices. I also wanted a good way to compare stockst that have very different prices. As mentioned above, Google has a price of 2300 dollars per share, while MTNB has a price of 90 cents per share. If we were to compare the stocks side by side, we could do a free y scale for the y axis, but I would question whether that is actually a good representation.I don’t believe looking at the stocks side by side with a free scale is a fair representation of comparing stocks. We look further into that later on.\r\r\nFirst\r\r\n\r\r\n\r\r\ndailyReturns <- data %>%\r\r\n  group_by(symbol) %>%\r\r\n  tq_transmute(select = adjusted, mutate_fun = periodReturn, period = \"daily\", type = \"arithmetic\")\r\r\n\r\r\ndailyReturns %>%\r\r\n  filter(date >= month1_date)%>%\r\r\n  summarise(AverageReturn = mean(daily.returns))%>%\r\r\nggplot()+\r\r\n  geom_col(aes(x = symbol, y = AverageReturn, fill = symbol))+\r\r\n   geom_hline(yintercept = 0, alpha =.5)+\r\r\n  geom_text(aes(x = symbol, y = AverageReturn, label = ((round(AverageReturn,4))*100)),vjust = 1)+\r\r\n  scale_y_continuous(labels = scales::percent)+\r\r\n  theme_classic()+\r\r\n  labs(title = \"Average Daily Return\",\r\r\n       subtitle = \"Current Month : April 2021\")\r\r\n\r\r\n\r\r\n\r\r\ndailyReturns %>%\r\r\n  filter(date >= month6_date)%>%\r\r\n  summarise(AverageReturn = mean(daily.returns))%>%\r\r\nggplot()+\r\r\n  geom_col(aes(x = symbol, y = AverageReturn, fill = symbol))+\r\r\n   geom_hline(yintercept = 0, alpha =.5)+\r\r\n  geom_text(aes(x = symbol, y = AverageReturn, label = (round(AverageReturn,4))*100),vjust = -.5)+\r\r\n  scale_y_continuous(labels = scales::percent)+\r\r\n  theme_classic()+\r\r\n  labs(title = \"Average Daily Return\",\r\r\n       subtitle = \"For the last 6 months\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMonthlyReturns <- data %>%\r\r\n  group_by(symbol) %>%\r\r\n  tq_transmute(select = adjusted, mutate_fun = periodReturn, period = \"monthly\", type = \"arithmetic\")%>%\r\r\n  mutate(month_1 = month(date),\r\r\n         Month = case_when(\r\r\n    month_1 == 1 ~ \"Jan\",\r\r\n    month_1 == 2 ~ \"Feb\",\r\r\n    month_1 == 3 ~ \"Mar\",\r\r\n    month_1 == 4 ~ \"Apr\",\r\r\n    month_1 == 5 ~ \"May\",\r\r\n    month_1 == 6 ~ \"Jun\",\r\r\n    month_1 == 7 ~ \"Jul\",\r\r\n    month_1 == 8 ~ \"Aug\",\r\r\n    month_1 == 9 ~ \"Sep\",\r\r\n    month_1 == 10 ~ \"Oct\",\r\r\n    month_1 == 11 ~ \"Nov\",\r\r\n    month_1 == 12 ~ \"Dec\"\r\r\n  ))\r\r\n\r\r\n#Creating Factors\r\r\nMonthlyReturns$Month <- factor(MonthlyReturns$Month, levels = c(\"Jan\", \"Feb\", \"Mar\",\"Apr\",\"May\",\r\r\n                                                        \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\r\r\n                                                        \"Nov\",\"Dec\"))\r\r\nMonthlyReturns %>%\r\r\n  filter(date >= month6_date)%>%\r\r\n  group_by(symbol)%>%\r\r\n  summarise(TotalReturn = sum(monthly.returns))%>%\r\r\nggplot()+\r\r\n  geom_col(aes(x = symbol, y = TotalReturn, fill = symbol))+\r\r\n   geom_hline(yintercept = 0, alpha =.5)+\r\r\n  geom_text(aes(x = symbol, y = TotalReturn, label = ((round(TotalReturn,1))*100)),vjust = 1)+\r\r\n  scale_y_continuous(labels = scales::percent)+\r\r\n  theme_classic()+\r\r\n  labs(title = \"Monthly Return\",\r\r\n       subtitle = \"6 Month Total Return\")\r\r\n\r\r\n\r\r\n\r\r\nMonthlyReturns %>%\r\r\n  filter(date >= month1_date)%>%\r\r\n  group_by(symbol)%>%\r\r\n  summarise(TotalReturn = sum(monthly.returns))%>%\r\r\nggplot()+\r\r\n  geom_col(aes(x = symbol, y = TotalReturn, fill = symbol), position = position_dodge2(width = 1, preserve = \"single\"))+\r\r\n   geom_hline(yintercept = 0, alpha =.5)+\r\r\n  geom_text(aes(x = symbol, y = TotalReturn, label =(round(TotalReturn,3))*100),vjust = 1)+\r\r\n  scale_y_continuous(labels = scales::percent)+\r\r\n  theme_classic()+\r\r\n  labs(title = \"Monthly Return\",\r\r\n       subtitle = \"Current Months Total Return\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nAbove we can see that looking from the six month perspective, MRNA and RIOT have had large gains gains of 150% and 410% respectively. Google and Southwest (LUV) both seem to have great returns these last six months. The figure below shows the current months return. Google still has a large gain. What’s noticable is that RIOT and MRNA have large losses of 28% and 19% this last month.\r\r\n\r\r\n\r\r\nyearlyReturns<- data %>%\r\r\n  group_by(symbol) %>%\r\r\n  tq_transmute(select = adjusted, mutate_fun = periodReturn, period = \"yearly\", type = \"arithmetic\")%>%\r\r\n  mutate(Year = year(date))\r\r\nyearlyReturns\r\r\n\r\r\n\r\r\n# A tibble: 60 x 4\r\r\n# Groups:   symbol [12]\r\r\n   symbol date       yearly.returns  Year\r\r\n   <chr>  <date>              <dbl> <dbl>\r\r\n 1 MSFT   2018-12-31         0.159   2018\r\r\n 2 MSFT   2019-12-31         0.576   2019\r\r\n 3 MSFT   2020-12-31         0.425   2020\r\r\n 4 MSFT   2021-12-31         0.525   2021\r\r\n 5 MSFT   2022-02-11        -0.123   2022\r\r\n 6 GOOG   2018-12-31        -0.0155  2018\r\r\n 7 GOOG   2019-12-31         0.291   2019\r\r\n 8 GOOG   2020-12-31         0.310   2020\r\r\n 9 GOOG   2021-12-31         0.652   2021\r\r\n10 GOOG   2022-02-11        -0.0729  2022\r\r\n# ... with 50 more rows\r\r\n\r\r\nyearlyReturns %>%\r\r\n  filter(date >= \"2021-01-01\")%>%\r\r\n  group_by(symbol)%>%\r\r\n  rename(Yearly_Return = yearly.returns)%>%\r\r\nggplot()+\r\r\n  geom_col(aes(x = symbol, y = Yearly_Return, fill = symbol))+\r\r\n  geom_hline(yintercept = 0, alpha =.5)+\r\r\n  geom_text(aes(x = symbol, y = Yearly_Return, label = (round(Yearly_Return,3))*100),vjust = 1)+\r\r\n  scale_y_continuous(labels = scales::percent)+\r\r\n  theme_classic()+\r\r\n  labs(title = \"Yearly Return\",\r\r\n       subtitle = \"Total Return in 2021\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nLooping through my Stocks\r\r\nI’ve decided that I also want to view my stocks using facet wrap on company symbol. Right now I have 12 stocks that I’m analyzing. If I use the facet by the current nubmer of company’s I end up getting something that is unreadable. In order to get this process done, in a timely manner I first need to break my stocks vector into three parts, 4 groups of 3. After that I used the list funciton to group the stock groups into a list. I know have a list with three items and each item contains four stocks.\r\r\nNow I can create a for loop that can go pull out the 4 stocks in each stock group and plot them in my ggplot that I created. I then used the facet command and the corresponding plots are much easier to understand!\r\r\nIn the future I will figure out a way where I don’t need to break up my list before the for loop. Ideally it would make the most sense to have a loop that would pull every four items. At this point in time, I do not know how to do this.\r\r\n\r\r\n\r\r\nstocks1 <- stocks[1:4]\r\r\nstocks2 <- stocks[5:8]\r\r\nstocks3 <- stocks[9:12]\r\r\n\r\r\n\r\r\n\r\r\nStock_block <- list(stocks1, stocks2, stocks3)\r\r\n\r\r\n\r\r\n\r\r\nfor (stock_group in Stock_block){\r\r\nx <- MonthlyReturns %>%\r\r\n  filter(symbol %in% stock_group)%>%\r\r\n  filter(date >= month6_date)%>%\r\r\nggplot()+\r\r\n  geom_col(aes(x = Month, y = monthly.returns, fill = symbol), position = position_dodge2(width = 1, preserve = \"single\"))+\r\r\n   geom_hline(yintercept = 0, alpha =.5)+\r\r\n  scale_y_continuous(labels = scales::percent)+\r\r\n  theme_classic()+\r\r\n  labs(title = \"Monthly Return\",\r\r\n       subtitle = \"Current Months Total Return\")+\r\r\n    facet_wrap(~symbol, ncol = 2, scales = \"free_y\")\r\r\nprint(x)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\nfor (stock_group in Stock_block){\r\r\nx <- yearlyReturns%>%\r\r\n    filter(symbol %in% stock_group)%>%\r\r\n  ggplot()+\r\r\n  geom_col(aes(x = Year, y = yearly.returns, fill = symbol))+\r\r\n  geom_hline(yintercept = 0, alpha = .5)+\r\r\n  scale_y_continuous(labels = scales::percent)+\r\r\n  labs(title = \"Annual Returns\",\r\r\n       subtitle = \"Sorted by Company\",\r\r\n       y = \"Annual Returns\", x = \"\")+\r\r\n  facet_wrap(~symbol, ncol = 2, scales = \"free_y\")+\r\r\n  theme_classic()\r\r\nprint(x)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nFinal look\r\r\nThe last attempt I will use to observe the stocks is based on the price trends. Using the for loop and stock groups I will facet through the stocks 4 at a time and compare the prices. It can be seen from below that even when using free scale, or not using free scale, theres no easy way to compare stocks with such different price differences.\r\r\nTo come up with a way to fix this, I decided to normalize the prices themselves.\r\r\n\r\r\n\r\r\nmultiple_dates <- rev(list(month1_date, month6_date, Year_4_date))\r\r\n\r\r\n\r\r\nfor (dates in multiple_dates){\r\r\nprint(data %>%\r\r\n   filter(date >= dates)%>%\r\r\n  ggplot()+\r\r\n  geom_line(aes(x = date, y = adjusted),color = \"red\")+\r\r\n  facet_wrap(~symbol, scales = \"free_y\")+\r\r\n    theme(axis.text.x = element_blank()))\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\nnormalize_funciton <- function(x){\r\r\n  return((x - min(x))/(max(x)- min(x)))\r\r\n}\r\r\n\r\r\nfor (dates in multiple_dates){\r\r\nprint(data %>%\r\r\n        filter(date >= dates)%>%\r\r\n group_by(symbol)%>%\r\r\n  mutate(scaled_Prices = normalize_funciton(adjusted))%>%\r\r\n   ggplot()+\r\r\n  geom_line(aes(x = date, y = scaled_Prices),color = \"red\")+\r\r\n  facet_wrap(~symbol))\r\r\n\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\nfor (dates in multiple_dates){\r\r\nfor (stock_group in Stock_block){\r\r\nprint(data %>%\r\r\n        filter(date >= dates)%>%\r\r\n        filter(symbol %in% stock_group)%>%\r\r\n group_by(symbol)%>%\r\r\n  mutate(scaled_Prices = normalize_funciton(adjusted))%>%\r\r\n   ggplot()+\r\r\n  geom_line(aes(x = date, y = scaled_Prices),color = \"red\")+\r\r\n  facet_wrap(~symbol))\r\r\n\r\r\n}\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\nfor (dates in multiple_dates){\r\r\n   for (stock_group in Stock_block){\r\r\nprint(data %>%\r\r\n  filter(date >= dates)%>%\r\r\n    filter(symbol %in% stock_group)%>%\r\r\n  group_by(symbol)%>%\r\r\n  mutate(scaled_Prices = normalize_funciton(adjusted))%>%\r\r\n  ggplot()+\r\r\n  geom_line(aes(x = date, y =scaled_Prices, color = symbol)))\r\r\n}\r\r\n\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/stock-returns/distill-preview.png",
    "last_modified": "2022-02-12T16:58:23-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/networks-hw-2/",
    "title": "Networks Hw 2",
    "description": "A closer look at Enrons Emails",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\r\nLooking at Nodes and Edges:\r\r\n\r\r\n\r\r\nls()\r\r\n\r\r\n\r\r\n[1] \"network_edgelist\" \"network_igraph\"   \"network_statnet\" \r\r\n\r\r\nvcount(network_igraph)\r\r\n\r\r\n\r\r\n[1] 184\r\r\n\r\r\necount(network_igraph)\r\r\n\r\r\n\r\r\n[1] 125409\r\r\n\r\r\nprint(network_statnet)\r\r\n\r\r\n\r\r\n Network attributes:\r\r\n  vertices = 184 \r\r\n  directed = TRUE \r\r\n  hyper = FALSE \r\r\n  loops = FALSE \r\r\n  multiple = FALSE \r\r\n  bipartite = FALSE \r\r\n  total edges= 3010 \r\r\n    missing edges= 0 \r\r\n    non-missing edges= 3010 \r\r\n\r\r\n Vertex attribute names: \r\r\n    vertex.names \r\r\n\r\r\n Edge attribute names not shown \r\r\n\r\r\n#print(network_igraph)\r\r\n\r\r\n\r\r\n\r\r\nIt looks like the igraph and statnet variables are showing different edges. The network igraph is showing 184 nodes and 125409 edges. The network statnet is showing 184 nodes, and 3010 edges.\r\r\n\r\r\nWeighted, Directed, Single Mode Network?\r\r\n\r\r\n\r\r\nis_bipartite(network_igraph)\r\r\n\r\r\n\r\r\n[1] FALSE\r\r\n\r\r\nis_directed(network_igraph)\r\r\n\r\r\n\r\r\n[1] TRUE\r\r\n\r\r\nis_weighted(network_igraph)\r\r\n\r\r\n\r\r\n[1] FALSE\r\r\n\r\r\nUsing the Network Igraph set, we have a single mode network, which is directed, and is not weighted.\r\r\n\r\r\nLooking at Vertex and Edge Attributes:\r\r\n\r\r\n\r\r\nvertex_attr_names(network_igraph)\r\r\n\r\r\n\r\r\n[1] \"Email\" \"Name\"  \"Note\" \r\r\n\r\r\nnetwork::list.vertex.attributes(network_statnet)\r\r\n\r\r\n\r\r\n[1] \"na\"           \"vertex.names\"\r\r\n\r\r\nedge_attr_names(network_igraph)\r\r\n\r\r\n\r\r\n[1] \"Time\"      \"Reciptype\" \"Topic\"     \"LDC_topic\"\r\r\n\r\r\nnetwork::list.edge.attributes(network_statnet)\r\r\n\r\r\n\r\r\n[1] \"LDC_topic\"      \"LDC_topic_desc\" \"LDC_topic_name\"\r\r\n[4] \"na\"             \"Reciptype\"      \"Time\"          \r\r\n[7] \"Topic\"         \r\r\n\r\r\nIgraph Attribute Names: Email, Name, Note\r\r\nIgraph edge names: Time, Reciptype, Topic, LDC_topic\r\r\nStatnet attribute names: na, vertex.names\r\r\nstatnet edge names: LDC_topic, LDC_topic_desc, LDC_topic_name, na, Reciptype, Time, Topic\r\r\n\r\r\nAccessing Attribute DATA:\r\r\n\r\r\n\r\r\nV(network_igraph)$Name %>% head()\r\r\n\r\r\n\r\r\n[1] \"Albert Meyers\"    \"Thomas Martin\"    \"Andrea Ring\"     \r\r\n[4] \"Andrew Lewis\"     \"Andy Zipper\"      \"Jeffrey Shankman\"\r\r\n\r\r\nV(network_igraph)$Email %>% head()\r\r\n\r\r\n\r\r\n[1] \"albert.meyers\" \"a..martin\"     \"andrea.ring\"   \"andrew.lewis\" \r\r\n[5] \"andy.zipper\"   \"a..shankman\"  \r\r\n\r\r\nV(network_igraph)$Note %>% head()\r\r\n\r\r\n\r\r\n[1] \"Employee, Specialist\"         \"Vice President\"              \r\r\n[3] \"NA\"                           \"Director\"                    \r\r\n[5] \"Vice President, Enron Online\" \"President, Enron Global Mkts\"\r\r\n\r\r\n(network_igraph)$Carrier %>% head()\r\r\n\r\r\n\r\r\nNULL\r\r\n\r\r\nhead(network_statnet %v% \"vertex.names\")\r\r\n\r\r\n\r\r\n[1] 1 2 3 4 5 6\r\r\n\r\r\nhead(network_statnet %e% \"Time\")\r\r\n\r\r\n\r\r\n[1] \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\"\r\r\n[4] \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\"\r\r\n\r\r\nhead(network_statnet %e% \"LDC_topic\")\r\r\n\r\r\n\r\r\n[1] \"-1\" \"-1\" \"-1\" \"-1\" \"-1\" \"-1\"\r\r\n\r\r\n\r\r\nSummarizing Attribute DATA\r\r\n\r\r\n\r\r\nsummary(E(network_igraph)$Time)\r\r\n\r\r\n\r\r\n   Length     Class      Mode \r\r\n   125409 character character \r\r\n\r\r\nsummary(network_statnet %e% \"Distance\")\r\r\n\r\r\n\r\r\nLength  Class   Mode \r\r\n     0   NULL   NULL \r\r\n\r\r\n #### Dyad Census\r\r\n\r\r\n\r\r\nigraph::dyad.census(network_igraph)\r\r\n\r\r\n\r\r\n$mut\r\r\n[1] 30600\r\r\n\r\r\n$asym\r\r\n[1] 64208\r\r\n\r\r\n$null\r\r\n[1] -77972\r\r\n\r\r\nsna::dyad.census(network_statnet)\r\r\n\r\r\n\r\r\n     Mut Asym  Null\r\r\n[1,] 913 1184 14739\r\r\n\r\r\nThe dyad census for null using igraph is coming up -77,972. This seems wrong.\r\r\n\r\r\nTriad Census\r\r\n\r\r\n\r\r\nigraph::triad.census(network_igraph)\r\r\n\r\r\n\r\r\n [1] 700234  19530 249694   8409   2695   5176   7060  13227   1180\r\r\n[10]     59   6781   1023   1137    786   2782   1611\r\r\n\r\r\nsna::triad.census(network_statnet)\r\r\n\r\r\n\r\r\n        003    012    102 021D 021U 021C 111D  111U 030T 030C  201\r\r\n[1,] 700234 150250 118974 8409 2695 5176 7060 13227 1180   59 6781\r\r\n     120D 120U 120C  210  300\r\r\n[1,] 1023 1137  786 2782 1611\r\r\n\r\r\n\r\r\nTransitivity\r\r\n\r\r\n\r\r\ntransitivity(network_igraph)\r\r\n\r\r\n\r\r\n[1] 0.3725138\r\r\n\r\r\ngtrans(network_statnet)\r\r\n\r\r\n\r\r\n[1] 0.3580924\r\r\n\r\r\ntransitivity(network_igraph, type = \"global\")\r\r\n\r\r\n\r\r\n[1] 0.3725138\r\r\n\r\r\ntransitivity(network_igraph, type = \"average\")\r\r\n\r\r\n\r\r\n[1] 0.5055302\r\r\n\r\r\ntransitivity(network_igraph, type = \"local\") %>% head()\r\r\n\r\r\n\r\r\n[1] 0.0023288309 0.0013788877 0.0008393993 0.0031740105 0.0007847921\r\r\n[6] 0.0017129438\r\r\n\r\r\nThe transitivity for igraph and statnet data sets were pretty close.\r\r\nThe global transitivity is .3725 while the average transitivity is higher at .5. This means that actors with fewer connections will have higher transitivity. This could be due to overweighted groups or this could be similar to different departments that know a lot of people in their department, but do not know others in the other departments.\r\r\n\r\r\nLocal Transitivity\r\r\n\r\r\n\r\r\nNames <- V(network_igraph)$Name\r\r\nNames %>% head()\r\r\n\r\r\n\r\r\n[1] \"Albert Meyers\"    \"Thomas Martin\"    \"Andrea Ring\"     \r\r\n[4] \"Andrew Lewis\"     \"Andy Zipper\"      \"Jeffrey Shankman\"\r\r\n\r\r\nLocal_transivity <- transitivity(network_igraph, type = \"local\")\r\r\n\r\r\ntransitivity_tibble <- tibble(Names = Names, Local_transivity = Local_transivity)\r\r\n\r\r\ntransitivity_tibble %>% arrange(desc(Local_transivity))\r\r\n\r\r\n\r\r\n# A tibble: 184 x 2\r\r\n   Names            Local_transivity\r\r\n   <chr>                       <dbl>\r\r\n 1 Thomas Martin             0.0571 \r\r\n 2 Joe Quenet                0.0179 \r\r\n 3 Mark Haedicke             0.0159 \r\r\n 4 Kim Ward                  0.0157 \r\r\n 5 Peter Keavey              0.0146 \r\r\n 6 Monika Causholli          0.0134 \r\r\n 7 David Delainey            0.00917\r\r\n 8 Susan Pereira             0.00909\r\r\n 9 Larry Campbell            0.00810\r\r\n10 NA                        0.00641\r\r\n# ... with 174 more rows\r\r\n\r\r\ngtrans(network_statnet)\r\r\n\r\r\n\r\r\n[1] 0.3580924\r\r\n\r\r\nFor some reason I am unable to pull local transitivity by type using the method used in HW 1 (vids = V()). I’m not sure if these local transivitys are correct. I ordered it by descending so the largest transivity would be thomas Martin at .05.\r\r\n\r\r\nDistances in the Network\r\r\n\r\r\n\r\r\n#distances(network_igraph, \"Thomas Martin\",\"Andrea Ring\")\r\r\n\r\r\n\r\r\n\r\r\naverage.path.length(network_igraph)\r\r\n\r\r\n\r\r\n[1] 2.390464\r\r\n\r\r\naverage.path.length(network_igraph, directed = F)\r\r\n\r\r\n\r\r\n[1] 2.085787\r\r\n\r\r\nI took these vertex names, so I’m a bit confused why these are not showing up correctly.\r\r\n\r\r\nIdentifying Isolates\r\r\n\r\r\n\r\r\nigraph::components(network_igraph)\r\r\n\r\r\n\r\r\n$membership\r\r\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n [33] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n [65] 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n [97] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1\r\r\n[129] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n[161] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n\r\r\n$csize\r\r\n[1] 182   1   1\r\r\n\r\r\n$no\r\r\n[1] 3\r\r\n\r\r\n#Isolates\r\r\nisolates(network_statnet)\r\r\n\r\r\n\r\r\n[1]  72 118\r\r\n\r\r\nas.vector(network_statnet %v% \"vertex.names\")[c(isolates(network_statnet))]\r\r\n\r\r\n\r\r\n[1]  72 118\r\r\n\r\r\nFor some reason it seems that Network statnet vertex.names is only showing numbers, not actually the names. I wonder if the statnet was set up incorrectly.\r\r\nThere are two isolates. \r\r\nDensity\r\r\n\r\r\n\r\r\ngraph.density(network_igraph)\r\r\n\r\r\n\r\r\n[1] 3.72443\r\r\n\r\r\nnetwork.density(network_statnet)\r\r\n\r\r\n\r\r\n[1] 0.08939178\r\r\n\r\r\ngraph.density(network_igraph, loops = TRUE)\r\r\n\r\r\n\r\r\n[1] 3.704188\r\r\n\r\r\ngden(network_statnet, diag = FALSE)\r\r\n\r\r\n\r\r\n[1] 0.08939178\r\r\n\r\r\nThe Igraph density is over 1, and the network density is around 9%. Very different densities.\r\r\n\r\r\nVertex Degrees\r\r\n\r\r\n\r\r\nigraph::degree(network_igraph) %>% head()\r\r\n\r\r\n\r\r\n[1] 114 428 391 104 957 381\r\r\n\r\r\nsna::degree(network_statnet) %>% head()\r\r\n\r\r\n\r\r\n[1] 10 32 21  9 59 30\r\r\n\r\r\nThere is a significant difference between degrees from the igraph dataset compared to the statnet data.\r\r\n\r\r\n\r\r\n\r\r\nnetwork_degree <- data.frame(Name = V(network_igraph)$Name,\r\r\n                             degree = igraph::degree(network_igraph, loops =FALSE))\r\r\nnetwork_degree %>% arrange(desc(degree)) %>% slice(1:10)\r\r\n\r\r\n\r\r\n              Name degree\r\r\n1    Jeff Dasovich  13967\r\r\n2    James Steffes   9404\r\r\n3       Tana Jones   9307\r\r\n4  Richard Shapiro   8994\r\r\n5               NA   6591\r\r\n6      Steven Kean   6384\r\r\n7    John Lavorato   6177\r\r\n8  Michael Grigsby   5860\r\r\n9      Mark Taylor   5693\r\r\n10  Louise Kitchen   5362\r\r\n\r\r\nJeff Dasovich has the highest number of degrees around 13K. He must be very high up in the company.\r\r\n\r\r\nDegree in Directed Networks\r\r\n\r\r\n\r\r\nsna::degree(network_statnet, cmode = \"indegree\")%>% head()\r\r\n\r\r\n\r\r\n[1]  4 21 10  6 30 17\r\r\n\r\r\nsna::degree(network_statnet, cmode = \"outdegree\") %>% head()\r\r\n\r\r\n\r\r\n[1]  6 11 11  3 29 13\r\r\n\r\r\nigraph::degree(network_igraph, mode =\"in\", loops = FALSE) %>% head()\r\r\n\r\r\n\r\r\n[1]  78 334 224  88 614 210\r\r\n\r\r\nigraph::degree(network_igraph, mode =\"out\", loops = FALSE)%>%head()\r\r\n\r\r\n\r\r\n[1]  36  92 167  16 325 169\r\r\n\r\r\nDegree_network <- data.frame(Name = V(network_igraph)$Name,\r\r\n           total_degrees = igraph::degree(network_igraph, loops = FALSE),\r\r\n           in_degree = igraph::degree(network_igraph, mode =\"in\", loops = FALSE),\r\r\n           out_degree = igraph::degree(network_igraph, mode =\"out\", loops = FALSE) ) %>% arrange(desc(total_degrees))\r\r\n\r\r\nDegree_network %>% slice(1:10)\r\r\n\r\r\n\r\r\n              Name total_degrees in_degree out_degree\r\r\n1    Jeff Dasovich         13967      2612      11355\r\r\n2    James Steffes          9404      4988       4416\r\r\n3       Tana Jones          9307      2268       7039\r\r\n4  Richard Shapiro          8994      6893       2101\r\r\n5               NA          6591      2698       3893\r\r\n6      Steven Kean          6384      2676       3708\r\r\n7    John Lavorato          6177      3352       2825\r\r\n8  Michael Grigsby          5860      1097       4763\r\r\n9      Mark Taylor          5693      3694       1999\r\r\n10  Louise Kitchen          5362      2087       3275\r\r\n\r\r\nAs expected from someone high up in the company. They would mostly have out degree connections, with a select few in degree connections.\r\r\n\r\r\nSummary Statistics\r\r\n\r\r\n\r\r\nsummary(Degree_network)\r\r\n\r\r\n\r\r\n     Name           total_degrees       in_degree     \r\r\n Length:184         Min.   :    0.0   Min.   :   0.0  \r\r\n Class :character   1st Qu.:  212.8   1st Qu.: 150.5  \r\r\n Mode  :character   Median :  512.5   Median : 314.0  \r\r\n                    Mean   : 1184.0   Mean   : 592.0  \r\r\n                    3rd Qu.: 1401.2   3rd Qu.: 655.2  \r\r\n                    Max.   :13967.0   Max.   :6893.0  \r\r\n   out_degree      \r\r\n Min.   :    0.00  \r\r\n 1st Qu.:   30.75  \r\r\n Median :  159.00  \r\r\n Mean   :  591.99  \r\r\n 3rd Qu.:  600.50  \r\r\n Max.   :11355.00  \r\r\n\r\r\n\r\r\nDegree Distribution\r\r\n\r\r\n\r\r\nhist(Degree_network$total_degrees, main = \"Enron Degree Distribution\", xlab = \"Degree\")\r\r\n\r\r\n\r\r\n\r\r\nhist(Degree_network$out_degree, main =\"Enron Out-Degree Distribution\", xlab = \"Degree\")\r\r\n\r\r\n\r\r\n\r\r\nhist(Degree_network$in_degree, main = \"Enron In-Degree Distribution\", xlab = \"Degree\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMost people in the company have limited number of degrees of connections, while their are a select few with many connections.\r\r\nNetwork Degree Centralization\r\r\n\r\r\n\r\r\n#centralization(network_statnet, degree, cmode= \"indegree\")\r\r\n#centralization(network_statnet, degree, cmode = \"outdegree\")\r\r\n\r\r\n\r\r\ncentr_degree(network_igraph, loops = FALSE, mode = \"in\")$centralization\r\r\n\r\r\n\r\r\n[1] 34.61991\r\r\n\r\r\ncentr_degree(network_igraph, loops = FALSE, mode = \"out\")$centralization\r\r\n\r\r\n\r\r\n[1] 59.13566\r\r\n\r\r\nThere is a higher centralization for out-degree nodes compared to in-degree nodes.\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/networks-hw-2/distill-preview.png",
    "last_modified": "2022-02-10T18:21:55-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/looking-into-alcohol-data-set/",
    "title": "Looking into Alcohol Data set",
    "description": "Trying to predict alcoholism based on a list of predictors",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-06",
    "categories": [],
    "contents": "\r\r\nIntroduction\r\r\nAlcohol abuse affects many different populations in the world, and it can ruin lives. Using the “Alcohol” data set from the Wooldridge package, I have created multiple models to help predict the likelihood of an individual abusing alcohol based on multiple predictors or covariates. The alcohol data set came with 33 variables, and 9822 observations. Here is a list of the 30 variables: abuse, status, unemrate, age, educ, married, famsize, white, exhealth, vghealth, goodhealth, fairhealth, northeast, midwest, south, centcity, outercity, qrt1, qrt2, qrt3, beertax, cigtax, ethanol, mothalc, fathalc, livealc, inwf, employ, agesq, beertaxsq, cigtaxsq, ethanolsq, educsq\r\r\nAt first glance I found it quite hard to decide on which variable to use in my model. Which variables have the strongest correlation to predicting whether someone will abuse alcohol? How do we choose? To start off, I choose variables for my models based on criteria that I believe would influence an individual to abuse alcohol. After I created those models, I determined how accurate those models were by using the R squared value for the LPM, and the AIC for the logit and Probit models. To create better models, I developed an automated process using the R^2 values from each variable in the model against the outcome variable (abuse). The process is outlined below. This paper has two objectives: 1. Create a best fit model that will help determine how likely an individual will abuse alcohol. 2. Create an automated process that identifies that the top N variables, and the best fit model to predict likelihood based on an outcome variable.\r\r\nMethod\r\r\nFor my initial models, I’ve decided that the variables that should influence the likelihood of an individual abusing alcohol are: status, age, education, fathalc, mothalc, beertax, and married. Some of these variables speak for them self. Fathalc and mothalc are used to determine whether the mother and fathers are alcoholics, 1 for yes and 0 for no. Status is used to identify if someone is out of the workforce, unemployed or employed using the 1, 2 and 3 respectively. Below are tables showing the distribution of each variable I have chosen.\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nage\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :25.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:31.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :38.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :39.18\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:46.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :59.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nbeertax\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :0.045\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:0.145\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :0.259\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :0.426\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:0.446\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :2.370\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\neduc\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. : 0.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:12.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :13.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :13.31\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:16.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :19.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nfathalc\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :0.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:0.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :0.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :0.1543\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:0.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :1.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nmarried\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :0.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:1.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :1.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :0.8164\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:1.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :1.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nstatus\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :1.000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:3.000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :3.000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :2.829\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:3.000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :3.000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nmothalc\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :0.00000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:0.00000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :0.00000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :0.04042\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:0.00000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :1.00000\r\r\n\r\r\n\r\r\n\r\r\nModels\r\r\nI’ve decided to use three types of models: LPM, Logit and Probit. I will create 5 nested models, and for ease of comparison, the LPM’s, Logits and Probits covariates will all match for each nested model. For example lpm1, logit1, and probit1 all use status, age, education, mother alcholic, and father alcoholic as covariates.\r\r\nBelow are the models used for LPM, Logit, and Probits.\r\r\n\r\r\n\r\r\n\r\r\n#LPM\r\r\nlpm1 <- lm(abuse ~ status+ age+ educ + fathalc, data = alcohol)\r\r\nlpm2 <-lm(abuse ~ status+ age+ educ+ fathalc +mothalc, data = alcohol)\r\r\nlpm3 <- lm(abuse ~ status+ age+ educ+ fathalc+ mothalc + beertax, \r\r\n           data = alcohol)\r\r\nlpm4 <- lm(abuse ~ status+ age+ educ+ fathalc+ mothalc + beertax+ married, \r\r\n           data = alcohol)\r\r\nlpm5 <- lm(abuse ~ status + age + educ + fathalc+ mothalc + beertax + married + \r\r\n             fathalc:mothalc, data = alcohol)\r\r\n\r\r\n# Logit\r\r\nlogit1 <- glm(abuse ~ status+ age+ educ+ fathalc, \r\r\n              family = binomial(link = logit), data = alcohol)\r\r\nlogit2 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc, \r\r\n              family = binomial(link = logit), data = alcohol)\r\r\nlogit3 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + beertax, \r\r\n              family = binomial(link = logit), data = alcohol)\r\r\nlogit4 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + beertax+ \r\r\n                married, family = binomial(link = logit), data = alcohol)\r\r\nlogit5 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + beertax+ \r\r\n                married+ fathalc:mothalc, \r\r\n              family = binomial(link = logit), data = alcohol)\r\r\n\r\r\n# Probit\r\r\nprobit1 <- glm(abuse ~ status+ age+ educ+ fathalc, \r\r\n               family = binomial(link = probit), data = alcohol)\r\r\nprobit2 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc , \r\r\n               family = binomial(link = probit), data = alcohol)\r\r\nprobit3 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + \r\r\n                 beertax, family = binomial(link = probit), data = alcohol)\r\r\nprobit4 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + \r\r\n        beertax+ married, family = binomial(link = probit), data = alcohol)\r\r\nprobit5 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + \r\r\n                            beertax+ married+ fathalc:mothalc, \r\r\n                family = binomial(link = probit), data = alcohol)\r\r\n\r\r\n\r\r\n\r\r\nInital Observations\r\r\nBelow are the beta coefficients for the LPM, Logit and Probit models:\r\r\n\r\r\n\r\r\n================================================================================\r\r\n                                              LPM's                             \r\r\n                 ---------------------------------------------------------------\r\r\n                 Model 1      Model 2      Model 3      Model 4      Model 5    \r\r\n--------------------------------------------------------------------------------\r\r\n(Intercept)         0.15 ***     0.15 ***     0.15 ***     0.16 ***     0.16 ***\r\r\n                   (0.03)       (0.03)       (0.03)       (0.03)       (0.03)   \r\r\nstatus             -0.01        -0.01        -0.01        -0.01        -0.01    \r\r\n                   (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nage                 0.00         0.00         0.00         0.00         0.00    \r\r\n                   (0.00)       (0.00)       (0.00)       (0.00)       (0.00)   \r\r\neduc               -0.00 **     -0.00 **     -0.00 **     -0.00 **     -0.00 ** \r\r\n                   (0.00)       (0.00)       (0.00)       (0.00)       (0.00)   \r\r\nfathalc             0.05 ***     0.05 ***     0.05 ***     0.05 ***     0.05 ***\r\r\n                   (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nmothalc                          0.04 **      0.04 **      0.05 **      0.04 *  \r\r\n                                (0.02)       (0.02)       (0.02)       (0.02)   \r\r\nbeertax                                      -0.01        -0.01        -0.01    \r\r\n                                             (0.01)       (0.01)       (0.01)   \r\r\nmarried                                                   -0.03 ***    -0.03 ***\r\r\n                                                          (0.01)       (0.01)   \r\r\nfathalc:mothalc                                                         0.01    \r\r\n                                                                       (0.03)   \r\r\n--------------------------------------------------------------------------------\r\r\nR^2                 0.01         0.01         0.01         0.01         0.01    \r\r\nAdj. R^2            0.01         0.01         0.01         0.01         0.01    \r\r\nNum. obs.        9822         9822         9822         9822         9822       \r\r\n================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n=====================================================================================\r\r\n                                                Logits                               \r\r\n                 --------------------------------------------------------------------\r\r\n                 Model 1       Model 2       Model 3       Model 4       Model 5     \r\r\n-------------------------------------------------------------------------------------\r\r\n(Intercept)         -0.99 ***     -1.01 ***     -0.99 ***     -0.96 ***     -0.96 ***\r\r\n                    (0.14)        (0.14)        (0.14)        (0.14)        (0.14)   \r\r\nstatus              -0.06         -0.06         -0.06         -0.04         -0.04    \r\r\n                    (0.03)        (0.03)        (0.03)        (0.03)        (0.03)   \r\r\nage                  0.00          0.00          0.00          0.00          0.00    \r\r\n                    (0.00)        (0.00)        (0.00)        (0.00)        (0.00)   \r\r\neduc                -0.02 **      -0.02 **      -0.02 **      -0.02 **      -0.02 ** \r\r\n                    (0.01)        (0.01)        (0.01)        (0.01)        (0.01)   \r\r\nfathalc              0.28 ***      0.26 ***      0.26 ***      0.26 ***      0.26 ***\r\r\n                    (0.04)        (0.04)        (0.04)        (0.04)        (0.05)   \r\r\nmothalc                            0.21 **       0.21 **       0.22 **       0.22 *  \r\r\n                                  (0.08)        (0.08)        (0.08)        (0.11)   \r\r\nbeertax                                         -0.03         -0.03         -0.03    \r\r\n                                                (0.04)        (0.04)        (0.04)   \r\r\nmarried                                                       -0.18 ***     -0.18 ***\r\r\n                                                              (0.05)        (0.05)   \r\r\nfathalc:mothalc                                                             -0.01    \r\r\n                                                                            (0.16)   \r\r\n-------------------------------------------------------------------------------------\r\r\nAIC               6306.43       6301.83       6303.10       6290.39       6292.39    \r\r\nBIC               6342.39       6344.98       6353.45       6347.93       6357.12    \r\r\nLog Likelihood   -3148.21      -3144.91      -3144.55      -3137.20      -3137.19    \r\r\nDeviance          6296.43       6289.83       6289.10       6274.39       6274.39    \r\r\nNum. obs.         9822          9822          9822          9822          9822       \r\r\n=====================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\n=====================================================================================\r\r\n                                                Probits                              \r\r\n                 --------------------------------------------------------------------\r\r\n                 Model 1       Model 2       Model 3       Model 4       Model 5     \r\r\n-------------------------------------------------------------------------------------\r\r\n(Intercept)         -1.64 ***     -1.67 ***     -1.63 ***     -1.57 ***     -1.57 ***\r\r\n                    (0.27)        (0.27)        (0.27)        (0.27)        (0.27)   \r\r\nstatus              -0.11         -0.10         -0.10         -0.08         -0.08    \r\r\n                    (0.06)        (0.06)        (0.06)        (0.06)        (0.06)   \r\r\nage                  0.00          0.00          0.00          0.01          0.01    \r\r\n                    (0.00)        (0.00)        (0.00)        (0.00)        (0.00)   \r\r\neduc                -0.03 **      -0.03 **      -0.03 **      -0.04 **      -0.04 ** \r\r\n                    (0.01)        (0.01)        (0.01)        (0.01)        (0.01)   \r\r\nfathalc              0.52 ***      0.49 ***      0.49 ***      0.49 ***      0.50 ***\r\r\n                    (0.08)        (0.08)        (0.08)        (0.08)        (0.09)   \r\r\nmothalc                            0.39 **       0.39 **       0.41 **       0.43 *  \r\r\n                                  (0.15)        (0.15)        (0.15)        (0.20)   \r\r\nbeertax                                         -0.07         -0.06         -0.06    \r\r\n                                                (0.08)        (0.08)        (0.08)   \r\r\nmarried                                                       -0.34 ***     -0.34 ***\r\r\n                                                              (0.09)        (0.09)   \r\r\nfathalc:mothalc                                                             -0.04    \r\r\n                                                                            (0.29)   \r\r\n-------------------------------------------------------------------------------------\r\r\nAIC               6306.83       6302.30       6303.56       6290.77       6292.76    \r\r\nBIC               6342.79       6345.46       6353.90       6348.31       6357.49    \r\r\nLog Likelihood   -3148.42      -3145.15      -3144.78      -3137.39      -3137.38    \r\r\nDeviance          6296.83       6290.30       6289.56       6274.77       6274.76    \r\r\nNum. obs.         9822          9822          9822          9822          9822       \r\r\n=====================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\nInitial Results\r\r\nIt seems like the R^2 value did not change no matter how many observations we included. We were only able to show about .01 variation of the model. The initial models are not explaining much variance in the model, so I am now going to attempt to automate the process and explain more variation in the model. We can’t really tell too much from the logit and probit models, but we can keep track of the AIC, for when we compare these results to the new models I will develop.\r\r\nVariables with High Correlation\r\r\nThe first step in the process to pick my variables for my linear models would be to identify the covariates with the highest R squared value when running a linear model against the outcome variable “abuse”. Below are the process and results:\r\r\n\r\r\n\r\r\ncolumns <- (colnames(alcohol))\r\r\n\r\r\noutcome <- \"abuse\"\r\r\nmodels <- lapply(paste(outcome,\" ~\", columns), as.formula)\r\r\ny <- NULL\r\r\n\r\r\n\r\r\nfor (model in models){\r\r\n  linearmodel <- lm(model, data = alcohol)\r\r\n  x <- summary(linearmodel)\r\r\n  print(paste(format(model), \"R^2 value: \", round(x$r.squared,3)*100, \"%\"))\r\r\n  y <- rbind(y, data.frame(variable = as.character(model[3]),\r\r\n              \"Rvalue_Percent\" = round(x$r.squared,3)*100))\r\r\n}\r\r\n\r\r\nnew_data <- y[order(-y$Rvalue_Percent),]\r\r\nnew_data %>% slice(1:8)\r\r\ntop_8 <- new_data$variable[1:8]\r\r\n\r\r\n\r\r\n\r\r\nThe code above was used to run 32 different linear models with the abuse as the outcome variable. I then created a data frame in a loop that extracted the R^2 as a percentage. I then organized the table from highest to lowest and grabbed the top 8 variables.  The top 8 variables are below:\r\r\n\r\r\n\r\r\nvariable\r\r\n\r\r\n\r\r\nRvalue_Percent\r\r\n\r\r\n\r\r\nfamsize\r\r\n\r\r\n\r\r\n0.5\r\r\n\r\r\n\r\r\nfathalc\r\r\n\r\r\n\r\r\n0.4\r\r\n\r\r\n\r\r\nlivealc\r\r\n\r\r\n\r\r\n0.3\r\r\n\r\r\n\r\r\nexhealth\r\r\n\r\r\n\r\r\n0.2\r\r\n\r\r\n\r\r\nethanol\r\r\n\r\r\n\r\r\n0.2\r\r\n\r\r\n\r\r\nethanolsq\r\r\n\r\r\n\r\r\n0.2\r\r\n\r\r\n\r\r\neducsq\r\r\n\r\r\n\r\r\n0.2\r\r\n\r\r\n\r\r\nstatus\r\r\n\r\r\n\r\r\n0.1\r\r\n\r\r\n\r\r\nThe R values above are in percentage form. It should be noted that it seems the correlation between abuse and these variables list above is quite low. The highest correlation is family size and that is only .5 %.\r\r\nCreating New Models\r\r\nNow that I’ve identified the covariates with the highest correlation, the next step is to create models for my LPM, Logit, and Probit. Instead of rewriting the code for each model, I have created a framework that can also be applied to other data sets. Building onto the code that was used above to identify the top 8 variables, I then created 8 variables based on the results from above.\r\r\nFor the LM function and the GLM function to run, I first needed to create 5 variables (x1-x5). These variables need to be in the formula format, which was created using the lapply function. I then simply created 5 more models for the LPMS, Logits and Probits below, using the 5 variables I created. See the process below:\r\r\n\r\r\n\r\r\noutcome_variable <- \"abuse\"\r\r\n\r\r\nvar1 <- top_8[1]\r\r\nvar2 <- top_8[2]\r\r\nvar3 <- top_8[3]\r\r\nvar4 <- top_8[4]\r\r\nvar5 <- top_8[5]\r\r\nvar6 <- top_8[6]\r\r\nvar7 <- top_8[7]\r\r\nvar8 <- top_8[8]\r\r\n\r\r\n\r\r\nx1 <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",var4)\r\r\n             , as.formula)\r\r\nx2 <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",var4,\r\r\n                   \"+\",var5), as.formula)\r\r\nx3 <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",var4\r\r\n                   ,\"+\",var5,\"+\",var6), as.formula)\r\r\nx4 <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",var4\r\r\n                   ,\"+\",var5,\"+\",var6,\"+\",var7), as.formula)\r\r\nx5 <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",var4\r\r\n                   ,\"+\",var5,\"+\",var6,\"+\",var7,\"+\",var8), as.formula)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n#LPMS\r\r\nattach(alcohol)\r\r\nlm1 <- lm(x1[[1]])\r\r\nlm2 <- lm(x2[[1]])\r\r\nlm3 <- lm(x3[[1]])\r\r\nlm4 <- lm(x4[[1]])\r\r\nlm5 <- lm(x5[[1]])\r\r\n\r\r\n\r\r\n#Logits\r\r\n\r\r\nlogit1.1 <- glm(x1[[1]], family = binomial(link = logit))\r\r\nlogit1.2 <- glm(x2[[1]], family = binomial(link = logit))\r\r\nlogit1.3 <- glm(x3[[1]], family = binomial(link = logit))\r\r\nlogit1.4 <- glm(x4[[1]], family = binomial(link = logit))\r\r\nlogit1.5 <- glm(x5[[1]], family = binomial(link = logit))\r\r\n\r\r\n\r\r\n\r\r\n#Probits\r\r\n\r\r\nprobit1.1 <- glm(x1[[1]], family = binomial(link = probit))\r\r\nprobit1.2 <- glm(x2[[1]], family = binomial(link = probit))\r\r\nprobit1.3 <- glm(x3[[1]], family = binomial(link = probit))\r\r\nprobit1.4 <- glm(x4[[1]], family = binomial(link = probit))\r\r\nprobit1.5 <- glm(x5[[1]], family = binomial(link = probit))\r\r\n\r\r\ndetach(alcohol)\r\r\n\r\r\n\r\r\n\r\r\nBelow are the tables showing our new LPM, Logit and Probit models, using the new variables with the highest R squared values.\r\r\n\r\r\n\r\r\n============================================================================\r\r\n                                          LPM's                             \r\r\n             ---------------------------------------------------------------\r\r\n             Model 1      Model 2      Model 3      Model 4      Model 5    \r\r\n----------------------------------------------------------------------------\r\r\n(Intercept)     0.14 ***     0.07 ***     0.05         0.07         0.09    \r\r\n               (0.01)       (0.02)       (0.04)       (0.04)       (0.05)   \r\r\nfamsize        -0.01 ***    -0.01 ***    -0.01 ***    -0.01 ***    -0.01 ***\r\r\n               (0.00)       (0.00)       (0.00)       (0.00)       (0.00)   \r\r\nfathalc         0.04 ***     0.05 ***     0.05 ***     0.05 ***     0.05 ***\r\r\n               (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nlivealc         0.01         0.01         0.01         0.01         0.01    \r\r\n               (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nexhealth       -0.03 ***    -0.03 ***    -0.03 ***    -0.02 ***    -0.02 ***\r\r\n               (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nethanol                      0.03 ***     0.05         0.06         0.06    \r\r\n                            (0.01)       (0.04)       (0.04)       (0.04)   \r\r\nethanolsq                                -0.00        -0.01        -0.01    \r\r\n                                         (0.01)       (0.01)       (0.01)   \r\r\neducsq                                                -0.00 ***    -0.00 ***\r\r\n                                                      (0.00)       (0.00)   \r\r\nstatus                                                             -0.01    \r\r\n                                                                   (0.01)   \r\r\n----------------------------------------------------------------------------\r\r\nR^2             0.01         0.01         0.01         0.01         0.01    \r\r\nAdj. R^2        0.01         0.01         0.01         0.01         0.01    \r\r\nNum. obs.    9822         9822         9822         9822         9822       \r\r\n============================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\n====================================================================================\r\r\n                                               Logits                               \r\r\n                --------------------------------------------------------------------\r\r\n                Model 1       Model 2       Model 3       Model 4       Model 5     \r\r\n------------------------------------------------------------------------------------\r\r\n(Intercept)        -1.77 ***     -2.47 ***     -2.95 ***     -2.75 ***     -2.61 ***\r\r\n                   (0.08)        (0.20)        (0.51)        (0.51)        (0.53)   \r\r\nfamsize            -0.17 ***     -0.16 ***     -0.16 ***     -0.17 ***     -0.16 ***\r\r\n                   (0.02)        (0.02)        (0.02)        (0.02)        (0.02)   \r\r\nfathalc             0.43 **       0.45 **       0.44 **       0.44 **       0.44 ** \r\r\n                   (0.14)        (0.14)        (0.14)        (0.14)        (0.14)   \r\r\nlivealc             0.12          0.10          0.10          0.09          0.08    \r\r\n                   (0.13)        (0.13)        (0.13)        (0.13)        (0.13)   \r\r\nexhealth           -0.29 ***     -0.30 ***     -0.30 ***     -0.25 ***     -0.25 ***\r\r\n                   (0.07)        (0.07)        (0.07)        (0.07)        (0.07)   \r\r\nethanol                           0.34 ***      0.76          0.85 *        0.85 *  \r\r\n                                 (0.09)        (0.43)        (0.43)        (0.43)   \r\r\nethanolsq                                      -0.09         -0.11         -0.11    \r\r\n                                               (0.09)        (0.09)        (0.09)   \r\r\neducsq                                                       -0.00 ***     -0.00 ***\r\r\n                                                             (0.00)        (0.00)   \r\r\nstatus                                                                     -0.05    \r\r\n                                                                           (0.06)   \r\r\n------------------------------------------------------------------------------------\r\r\nAIC              6253.86       6240.92       6241.87       6230.89       6232.08    \r\r\nBIC              6289.82       6284.08       6292.22       6288.43       6296.81    \r\r\nLog Likelihood  -3121.93      -3114.46      -3113.94      -3107.45      -3107.04    \r\r\nDeviance         6243.86       6228.92       6227.87       6214.89       6214.08    \r\r\nNum. obs.        9822          9822          9822          9822          9822       \r\r\n====================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n====================================================================================\r\r\n                                               Probits                              \r\r\n                --------------------------------------------------------------------\r\r\n                Model 1       Model 2       Model 3       Model 4       Model 5     \r\r\n------------------------------------------------------------------------------------\r\r\n(Intercept)        -1.07 ***     -1.44 ***     -1.67 ***     -1.57 ***     -1.49 ***\r\r\n                   (0.04)        (0.10)        (0.26)        (0.26)        (0.28)   \r\r\nfamsize            -0.08 ***     -0.08 ***     -0.08 ***     -0.08 ***     -0.08 ***\r\r\n                   (0.01)        (0.01)        (0.01)        (0.01)        (0.01)   \r\r\nfathalc             0.22 **       0.23 **       0.23 **       0.23 **       0.23 ** \r\r\n                   (0.07)        (0.07)        (0.07)        (0.07)        (0.07)   \r\r\nlivealc             0.07          0.06          0.06          0.05          0.05    \r\r\n                   (0.07)        (0.07)        (0.07)        (0.07)        (0.07)   \r\r\nexhealth           -0.15 ***     -0.15 ***     -0.16 ***     -0.13 ***     -0.13 ***\r\r\n                   (0.04)        (0.04)        (0.04)        (0.04)        (0.04)   \r\r\nethanol                           0.18 ***      0.39          0.44 *        0.44 *  \r\r\n                                 (0.05)        (0.22)        (0.22)        (0.22)   \r\r\nethanolsq                                      -0.05         -0.06         -0.06    \r\r\n                                               (0.05)        (0.05)        (0.05)   \r\r\neducsq                                                       -0.00 ***     -0.00 ***\r\r\n                                                             (0.00)        (0.00)   \r\r\nstatus                                                                     -0.03    \r\r\n                                                                           (0.03)   \r\r\n------------------------------------------------------------------------------------\r\r\nAIC              6254.48       6241.18       6242.18       6231.09       6232.06    \r\r\nBIC              6290.44       6284.33       6292.53       6288.63       6296.79    \r\r\nLog Likelihood  -3122.24      -3114.59      -3114.09      -3107.55      -3107.03    \r\r\nDeviance         6244.48       6229.18       6228.18       6215.09       6214.06    \r\r\nNum. obs.        9822          9822          9822          9822          9822       \r\r\n====================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\nResults\r\r\nIt looks like the LPM R^2 values are still at .01. I believe this is due to the originally values being very small and .01 is the smallest value we can visualize on screen reg. After we identify the LPM model to use, I will extract the actual R^2 value and compare those to the actuals in the orginal LPM’s to see how much variance we reduced by using the new models. I will perform a similar method for the Logit and Probits, but with comparing the AIC and BIC values. Now lets identify the best fitting models.\r\r\n\r\r\nDetermining the best Fit models\r\r\nLPM’s\r\r\nTo identify the best fitting LPM model, I will use the LinearHypothesis function and compare the F values determine which models are the best fit, and whether they are statistically significant. To automate this process, I first created vectors with the corresponding variables in each model, which can be seen in m1 through m5. When using the Linear Hypothesis function, we also need to identify the difference in variables when comparing those models. For example, if we are comparing m5 to m1, then we would need the variables that model 5 and model 1 don’t share. We would need to identify the difference in variables for each model. To automate this process, I am using the function setdiff. Setdiff allows me to quickly identify the difference between vectors of strings. Once I identified the difference, I know can run the LinearHypothesis function. Below is the code used to perform these tasks and the results of each LinearHypothesis:\r\r\n\r\r\n\r\r\n#Hypothesis Tests\r\r\n\r\r\nm1 <- c(top_8[1:4])\r\r\nm2 <- c(top_8[1:5])\r\r\nm3 <- c(top_8[1:6])\r\r\nm4 <- c(top_8[1:7])\r\r\nm5 <- c(top_8[1:8])\r\r\n\r\r\n\r\r\nm5m1 <- setdiff(m5,m1)\r\r\nm5m2 <- setdiff(m5,m2)\r\r\nm5m3 <- setdiff(m5,m3)\r\r\nm5m4 <- setdiff(m5,m4)\r\r\nm4m3 <- setdiff(m4,m3)\r\r\n\r\r\n\r\r\nlinearHypothesis(lm5,m5m1)\r\r\n\r\r\n\r\r\nLinear hypothesis test\r\r\n\r\r\nHypothesis:\r\r\nethanol = 0\r\r\nethanolsq = 0\r\r\neducsq = 0\r\r\nstatus = 0\r\r\n\r\r\nModel 1: restricted model\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n\r\r\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)    \r\r\n1   9817 867.84                                 \r\r\n2   9813 865.21  4    2.6262 7.4463 5.52e-06 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlinearHypothesis(lm5,m5m2)\r\r\n\r\r\n\r\r\nLinear hypothesis test\r\r\n\r\r\nHypothesis:\r\r\nethanolsq = 0\r\r\neducsq = 0\r\r\nstatus = 0\r\r\n\r\r\nModel 1: restricted model\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n\r\r\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \r\r\n1   9816 866.44                                \r\r\n2   9813 865.21  3    1.2246 4.6296 0.003073 **\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlinearHypothesis(lm5,m5m3)\r\r\n\r\r\n\r\r\nLinear hypothesis test\r\r\n\r\r\nHypothesis:\r\r\neducsq = 0\r\r\nstatus = 0\r\r\n\r\r\nModel 1: restricted model\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n\r\r\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \r\r\n1   9815 866.42                                \r\r\n2   9813 865.21  2    1.2026 6.8196 0.001097 **\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlinearHypothesis(lm5,m5m4)\r\r\n\r\r\n\r\r\nLinear hypothesis test\r\r\n\r\r\nHypothesis:\r\r\nstatus = 0\r\r\n\r\r\nModel 1: restricted model\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n\r\r\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\r\r\n1   9814 865.32                           \r\r\n2   9813 865.21  1   0.10315 1.1699 0.2794\r\r\n\r\r\nlinearHypothesis(lm4,m4m3)\r\r\n\r\r\n\r\r\nLinear hypothesis test\r\r\n\r\r\nHypothesis:\r\r\neducsq = 0\r\r\n\r\r\nModel 1: restricted model\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq\r\r\n\r\r\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \r\r\n1   9815 866.42                                  \r\r\n2   9814 865.32  1    1.0994 12.469 0.0004156 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\n\r\r\nLPM Results\r\r\nAbove are the LinearHypothesis results from comparing the LPM models. From the Linear Hypthoesis tests, I have determined that model 4 is more parsimonious than model 5 (F = 1.17, p >.05). Now that I have identified the Model 4 as the LPM model, I will now perform a series of tests to determine if there is any non-linearity in the model and if we can trust the covariates in the model for predictions.\r\r\n\r\r\n\r\r\nTable 1: Vifs for Each Variable\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nfamsize\r\r\n\r\r\n\r\r\n1.010065\r\r\n\r\r\n\r\r\nfathalc\r\r\n\r\r\n\r\r\n2.493831\r\r\n\r\r\n\r\r\nlivealc\r\r\n\r\r\n\r\r\n2.501981\r\r\n\r\r\n\r\r\nexhealth\r\r\n\r\r\n\r\r\n1.042810\r\r\n\r\r\n\r\r\nethanol\r\r\n\r\r\n\r\r\n24.059350\r\r\n\r\r\n\r\r\nethanolsq\r\r\n\r\r\n\r\r\n24.032920\r\r\n\r\r\n\r\r\neducsq\r\r\n\r\r\n\r\r\n1.055115\r\r\n\r\r\n\r\r\nAfter running the vif test, I have identified that Ethanol and Ethanolsq both have vifs over 24, therefore we will drop them from the model. Now we will run the reset test in order to test for linearity in the model.\r\r\n\r\r\n\r\r\n    RESET test\r\r\n\r\r\ndata:  lm4\r\r\nRESET = 4.1787, df1 = 2, df2 = 9812, p-value = 0.01534\r\r\n\r\r\nThe p-value from the reset value test is below .05, therefor there is non-linearity in the model. Let’s try dropping the ethanol and ethanolsq covariates and check the linearity again.\r\r\n\r\r\n\r\r\nx4_new <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",\r\r\n                       var4,\"+\",var7), as.formula)\r\r\n\r\r\nattach(alcohol)\r\r\nlm4_new <- lm(x4_new[[1]])\r\r\ndetach(alcohol)\r\r\n\r\r\nresettest(lm4_new)\r\r\n\r\r\n\r\r\n\r\r\n    RESET test\r\r\n\r\r\ndata:  lm4_new\r\r\nRESET = 3.1639, df1 = 2, df2 = 9814, p-value = 0.0423\r\r\n\r\r\nThe Pvalue is still below .05, signifying non-linearity. There is still non linearity, but the its closer to the .05 mark. Lets check the residuals and outliers for non linearity as well. \r\r\n\r\r\n\r\r\npredicted <- lm4_new$fitted.values\r\r\nresiduals <- lm4_new$residuals\r\r\nplot(predicted,residuals)+ abline(h = 0)\r\r\n\r\r\n\r\r\n\r\r\ninteger(0)\r\r\n\r\r\nThis looks like nonlinearity to me. Let’s check into the outliers using the rstudent function.\r\r\n\r\r\n\r\r\nTable 2: Outlier Distribution Min and Max\r\r\n\r\r\n\r\r\nmax\r\r\n\r\r\n\r\r\nmin\r\r\n\r\r\n\r\r\n-0.68\r\r\n\r\r\n\r\r\n3.36\r\r\n\r\r\n\r\r\nWhen looking at the histogram, there are outliers slightly to the right of 3. The table also confirms slight outliers with a max standard deviation of 3.36. The outlier is not too large, so I am not going to delete/omit any data.\r\r\nNow lets check the Homoskedasticity assumption using the BP test function.\r\r\n\r\r\n\r\r\n    studentized Breusch-Pagan test\r\r\n\r\r\ndata:  lm4_new\r\r\nBP = 115.15, df = 5, p-value < 2.2e-16\r\r\n\r\r\nThe Model is significant with a p-value < .05 and the model does violate the homoscedasticity assumption. We have heteroskedasticity, and we need to estimate the robust errors. Below are the errors using the coeftest function. \r\r\n\r\r\n\r\r\nt test of coefficients:\r\r\n\r\r\n               Estimate  Std. Error t value  Pr(>|t|)    \r\r\n(Intercept)  0.16323181  0.01031484 15.8249 < 2.2e-16 ***\r\r\nfamsize     -0.01408535  0.00197431 -7.1343 1.042e-12 ***\r\r\nfathalc      0.04453690  0.01309825  3.4002 0.0006760 ***\r\r\nlivealc      0.00959970  0.01212474  0.7917 0.4285287    \r\r\nexhealth    -0.02133789  0.00621045 -3.4358 0.0005932 ***\r\r\neducsq      -0.00013612  0.00004134 -3.2928 0.0009955 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nWe still have the current issue of non-linearity in our data. Through trial and error, I have identified that one of our variables needed to be transformed. To figure this out, I changed each variable, first squaring then logging them. With that variable transformed, I then used the reset test to check for linearity. After multiple trials I was able to identity that edusq needed to be logged.\r\r\n\r\r\n\r\r\nlm4_linear <- lm(abuse~ famsize + fathalc + livealc+ exhealth + I(log(educsq+1)), data = alcohol)\r\r\n\r\r\nresettest(lm4_linear)\r\r\n\r\r\n\r\r\n\r\r\n    RESET test\r\r\n\r\r\ndata:  lm4_linear\r\r\nRESET = 2.473, df1 = 2, df2 = 9814, p-value = 0.08438\r\r\n\r\r\nAfter running the reset test using the logged variable, we know have a p-value >.05, which means our model has linearity. Please see the results above.\r\r\nNext we will identify the Logit and Probit Models\r\r\nNow that we have identified the best fitting LPM Model, we know need to determine what is the best fitting model for the logit and probit models. In order to perform this task we will use the likelihood ratio tests.\r\r\n\r\r\n\r\r\nlrtest(logit1.1,logit1.2)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol\r\r\n  #Df  LogLik Df Chisq Pr(>Chisq)    \r\r\n1   5 -3121.9                        \r\r\n2   6 -3114.5  1 14.94   0.000111 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlrtest(logit1.2,logit1.3)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)\r\r\n1   6 -3114.5                     \r\r\n2   7 -3113.9  1 1.0528     0.3049\r\r\n\r\r\nlrtest(logit1.3, logit1.4)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \r\r\n1   7 -3113.9                         \r\r\n2   8 -3107.4  1 12.976  0.0003156 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlrtest(logit1.4,logit1.5)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)\r\r\n1   8 -3107.4                     \r\r\n2   9 -3107.0  1 0.8124     0.3674\r\r\n\r\r\nFor The logit models, 2 is a better fit than 3 with (chisq = 1.0528, pr(>chisq >.05)). Model 4 is a better fit than model 5 with (chisq =.8124, pr(>chisq >.05)). I will choose logit1.4 as the logit model.\r\r\n\r\r\n\r\r\nlrtest(probit1.1,probit1.2)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \r\r\n1   5 -3122.2                         \r\r\n2   6 -3114.6  1 15.299  9.177e-05 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlrtest(probit1.2,probit1.3)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)\r\r\n1   6 -3114.6                     \r\r\n2   7 -3114.1  1 0.9922     0.3192\r\r\n\r\r\nlrtest(probit1.3,probit1.4)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \r\r\n1   7 -3114.1                         \r\r\n2   8 -3107.6  1 13.091  0.0002966 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlrtest(probit1.4,probit1.5)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)\r\r\n1   8 -3107.6                     \r\r\n2   9 -3107.0  1 1.0317     0.3098\r\r\n\r\r\nThe probit models have similar results when compared to the logit models. For The probit models, 2 is a better fit than 3 (chisq = .992, pr(>chisq >.05)). Model 4 is a better fit than (chisq =1.0317, pr(>chisq >.05)). I will choose probit1.4 as the probit model.\r\r\nFixing up logit models\r\r\nNow that we have our logit and Probit models, logit1.4 and Probit1.4. We also need to check the vifs like did in the LPM models. Her are the results below:\r\r\n\r\r\n\r\r\nTable 3: Logit Vifs\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nfamsize\r\r\n\r\r\n\r\r\n1.009437\r\r\n\r\r\n\r\r\nfathalc\r\r\n\r\r\n\r\r\n2.776656\r\r\n\r\r\n\r\r\nlivealc\r\r\n\r\r\n\r\r\n2.782741\r\r\n\r\r\n\r\r\nexhealth\r\r\n\r\r\n\r\r\n1.038261\r\r\n\r\r\n\r\r\nethanol\r\r\n\r\r\n\r\r\n24.005089\r\r\n\r\r\n\r\r\nethanolsq\r\r\n\r\r\n\r\r\n23.982810\r\r\n\r\r\n\r\r\neducsq\r\r\n\r\r\n\r\r\n1.050736\r\r\n\r\r\n\r\r\nTable 3: Probit Vifs\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nfamsize\r\r\n\r\r\n\r\r\n1.009902\r\r\n\r\r\n\r\r\nfathalc\r\r\n\r\r\n\r\r\n2.669483\r\r\n\r\r\n\r\r\nlivealc\r\r\n\r\r\n\r\r\n2.676435\r\r\n\r\r\n\r\r\nexhealth\r\r\n\r\r\n\r\r\n1.039080\r\r\n\r\r\n\r\r\nethanol\r\r\n\r\r\n\r\r\n24.036773\r\r\n\r\r\n\r\r\nethanolsq\r\r\n\r\r\n\r\r\n24.012263\r\r\n\r\r\n\r\r\neducsq\r\r\n\r\r\n\r\r\n1.052182\r\r\n\r\r\n\r\r\nWe are seeing similar results for what we identified in the LPM model. We need to get rid of the ethanol and ethanolsq covariates from the logit and probit models. I will also include the logged variable from the LPM from our earlier research. \r\r\nCreating new logit and Probit Models\r\r\n\r\r\n\r\r\nlogit1.4_new <- glm(abuse~ famsize + fathalc+ livealc+exhealth+ I(log(educsq+1))\r\r\n                    ,family = binomial(link = logit), data = alcohol)\r\r\nprobit1.4_new <- glm(abuse~ famsize + fathalc+ livealc+exhealth+ I(log(educsq+1)\r\r\n                                                                   ),family = \r\r\n                       binomial(link = probit), data = alcohol)\r\r\n\r\r\n  resettest(logit1.4_new)\r\r\n\r\r\n\r\r\n\r\r\n    RESET test\r\r\n\r\r\ndata:  logit1.4_new\r\r\nRESET = 2.473, df1 = 2, df2 = 9814, p-value = 0.08438\r\r\n\r\r\n  resettest(probit1.4_new)\r\r\n\r\r\n\r\r\n\r\r\n    RESET test\r\r\n\r\r\ndata:  probit1.4_new\r\r\nRESET = 2.473, df1 = 2, df2 = 9814, p-value = 0.08438\r\r\n\r\r\nThe new logit and probit models have now been created. I also did a quick reset test on each one. Both p-values are greater than .05, we have linearity in each model. Before we start speaking to the LPM, Logit and probit models, let’s look into the difference in R squared values from the LPM models and the AIC from the new logit and probit models to the old models.\r\r\nLPM Comparisons\r\r\n\r\r\n\r\r\na1 <- summary(lpm1)\r\r\na2 <- summary(lpm2)\r\r\na3 <- summary(lpm3)\r\r\na4 <- summary(lpm4)\r\r\na5 <- summary(lpm5)\r\r\na6 <-summary(lm4_linear)\r\r\na1 <-a1$r.squared\r\r\na2 <-a2$r.squared\r\r\na3 <-a3$r.squared\r\r\na4 <-a4$r.squared\r\r\na5 <-a5$r.squared\r\r\na6 <-a6$r.squared\r\r\n\r\r\nstring <- c(\"lpm1\",\"lpm2\",\"lpm3\",\"lpm4\",\"lpm5\",\"Newest LPM\")\r\r\nstring2 <- round(c(a1,a2,a3,a4,a5,a6),5)\r\r\ntable <- cbind(string,string2)\r\r\nkable(table, caption = \"R squared value by LPM\", col.names = c(\"\",\"\"))\r\r\n\r\r\n\r\r\n\r\r\nTable 4: R squared value by LPM\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nlpm1\r\r\n\r\r\n\r\r\n0.00584\r\r\n\r\r\n\r\r\nlpm2\r\r\n\r\r\n\r\r\n0.00664\r\r\n\r\r\n\r\r\nlpm3\r\r\n\r\r\n\r\r\n0.00672\r\r\n\r\r\n\r\r\nlpm4\r\r\n\r\r\n\r\r\n0.00827\r\r\n\r\r\n\r\r\nlpm5\r\r\n\r\r\n\r\r\n0.00829\r\r\n\r\r\n\r\r\nNewest LPM\r\r\n\r\r\n\r\r\n0.01142\r\r\n\r\r\n\r\r\nResults\r\r\nWe can see from the table above that with our current LPM model, we were able to increase the variance explained from .00829 (lpm5, which had the highest R squared value) all the way to .01142. This may seem small, but when looking at it from a percentage change, we were able to increase our R squared value by 37%. The automated method was able to increase the variance explained and increased our R squared values.\r\r\n#logit and Probit Comparisons\r\r\n\r\r\n\r\r\nL_AIC1 <- logit1$aic\r\r\nL_AIC2 <- logit2$aic\r\r\nL_AIC3 <- logit3$aic\r\r\nL_AIC4 <- logit4$aic\r\r\nL_AIC5 <- logit5$aic\r\r\nP_AIC1 <- probit1$aic\r\r\nP_AIC2 <- probit2$aic\r\r\nP_AIC3 <- probit3$aic\r\r\nP_AIC4 <- probit4$aic\r\r\nP_AIC5 <- probit5$aic\r\r\n\r\r\nx <- round(c(L_AIC1, L_AIC2 ,L_AIC3 ,L_AIC4, L_AIC5 , P_AIC1 ,P_AIC2 ,P_AIC3 , P_AIC4 ,P_AIC5 ),2)\r\r\nz <- c(\"logit1\",\"Logit2\",\"Logit3\",\"logit4\",\"logit5\",\"Probit1\",\"Probit2\",\"Probit3\",\"Probit4\",\"Probit5\")\r\r\ny <- cbind(z,x)\r\r\nkable(y, caption = \"Logit and Probit AICS\", col.names = c(\"\",\"\"))\r\r\n\r\r\n\r\r\n\r\r\nTable 5: Logit and Probit AICS\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nlogit1\r\r\n\r\r\n\r\r\n6306.83\r\r\n\r\r\n\r\r\nLogit2\r\r\n\r\r\n\r\r\n6302.3\r\r\n\r\r\n\r\r\nLogit3\r\r\n\r\r\n\r\r\n6303.56\r\r\n\r\r\n\r\r\nlogit4\r\r\n\r\r\n\r\r\n6290.77\r\r\n\r\r\n\r\r\nlogit5\r\r\n\r\r\n\r\r\n6292.76\r\r\n\r\r\n\r\r\nProbit1\r\r\n\r\r\n\r\r\n6306.43\r\r\n\r\r\n\r\r\nProbit2\r\r\n\r\r\n\r\r\n6301.83\r\r\n\r\r\n\r\r\nProbit3\r\r\n\r\r\n\r\r\n6303.1\r\r\n\r\r\n\r\r\nProbit4\r\r\n\r\r\n\r\r\n6290.39\r\r\n\r\r\n\r\r\nProbit5\r\r\n\r\r\n\r\r\n6292.39\r\r\n\r\r\n\r\r\nL_new <- logit1.4_new$aic\r\r\nP_new <- probit1.4_new$aic\r\r\n\r\r\na <- round(c(L_new,P_new),2)\r\r\nb <- c(\"Logit Model\",\" Probit Model\")\r\r\nc <- cbind(b,a)\r\r\nkable(c, caption = \"New Logit and Probit AICS\", col.names = c(\"\",\"\"))\r\r\n\r\r\n\r\r\n\r\r\nTable 5: New Logit and Probit AICS\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nLogit Model\r\r\n\r\r\n\r\r\n6251.15\r\r\n\r\r\n\r\r\nProbit Model\r\r\n\r\r\n\r\r\n6251.58\r\r\n\r\r\n\r\r\nResults\r\r\nThe lowest AIC for the first logit and probit models were both in models 4. The logit model had an AIC of 6292.76 and the probit model had an AIC of 6290.39. The new models had lower AICs, 6251.15 for the logit model, and 6251.58 for the probit model. \r\r\nLooking at the Models\r\r\n\r\r\n\r\r\n================================================================================\r\r\n                                            LPM Models                          \r\r\n                 ---------------------------------------------------------------\r\r\n                 lm1          lm2          lm3          lm4          Lm4_linear \r\r\n--------------------------------------------------------------------------------\r\r\n(Intercept)         0.14 ***     0.07 ***     0.05         0.07         0.20 ***\r\r\n                   (0.01)       (0.02)       (0.04)       (0.04)       (0.03)   \r\r\nfamsize            -0.01 ***    -0.01 ***    -0.01 ***    -0.01 ***    -0.01 ***\r\r\n                   (0.00)       (0.00)       (0.00)       (0.00)       (0.00)   \r\r\nfathalc             0.04 ***     0.05 ***     0.05 ***     0.05 ***     0.04 ***\r\r\n                   (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nlivealc             0.01         0.01         0.01         0.01         0.01    \r\r\n                   (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nexhealth           -0.03 ***    -0.03 ***    -0.03 ***    -0.02 ***    -0.02 ***\r\r\n                   (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nethanol                          0.03 ***     0.05         0.06                 \r\r\n                                (0.01)       (0.04)       (0.04)                \r\r\nethanolsq                                    -0.00        -0.01                 \r\r\n                                             (0.01)       (0.01)                \r\r\neducsq                                                    -0.00 ***             \r\r\n                                                          (0.00)                \r\r\nlog(educsq + 1)                                                        -0.01 *  \r\r\n                                                                       (0.01)   \r\r\n--------------------------------------------------------------------------------\r\r\nR^2                 0.01         0.01         0.01         0.01         0.01    \r\r\nAdj. R^2            0.01         0.01         0.01         0.01         0.01    \r\r\nNum. obs.        9822         9822         9822         9822         9822       \r\r\n================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nOrdered Logit of Alcohol Abuse (Odds Ratio)\r\r\n==============================================\r\r\n                       Dependent variable:    \r\r\n                   ---------------------------\r\r\n                              abuse           \r\r\n----------------------------------------------\r\r\nfamsize                     0.845***          \r\r\n                            p = 0.000         \r\r\n                                              \r\r\nfathalc                     1.539***          \r\r\n                            p = 0.002         \r\r\n                                              \r\r\nlivealc                       1.117           \r\r\n                            p = 0.402         \r\r\n                                              \r\r\nexhealth                    0.767***          \r\r\n                           p = 0.0003         \r\r\n                                              \r\r\nI(log(educsq + 1))           0.876**          \r\r\n                            p = 0.026         \r\r\n                                              \r\r\nConstant                    0.335***          \r\r\n                           p = 0.0005         \r\r\n                                              \r\r\n----------------------------------------------\r\r\nObservations                  9,822           \r\r\nLog Likelihood             -3,119.576         \r\r\nAkaike Inf. Crit.           6,251.151         \r\r\n==============================================\r\r\nNote:              *p<0.1; **p<0.05; ***p<0.01\r\r\n\r\r\nProbit models do not have odd’s ratios, so I will speak to the LPM and Logit models. When looking at the moving from Lm1 to lm2, the covariate fathalc increases from .04 to .05 when we include the covariate ethanol. When going from model 4 to 5, educsq increase from very small <-.00 to -.01, but it went from down in signficance to p<.05 from p<.001. When looking at lm4 compared to Lm4_linear, we saw a decrease in fathalc from .05 to .04, but the significance stayed the same. When looking at Lm4_linear, if your father is an alcoholic, that is associated with a 4% (p<.001) increase in abusing alcohol. Continuing to look at Lmr_linear, a one unit increase in famsize is associated with a 1% (p<.001) decrease in abusing alcohol.\r\r\nWhen looking at the Logit model. If one’s father is an alcholic, the likelihood of abusing alcohol versus not abusing alcohol is 1.539 times higher with p = .002. If one lives with an alcoholic, they are 1.117 more times likely to abuse alcohol. For A one-unit increase in famsize, the likelihood of abusing alcohol is .845 times as likely.\r\r\nFinally I will create synthetic individuals and make predictions of the likelihood of an individual to abuse alcohol based on the preset conditions. These can be seen below. \r\r\n\r\r\n\r\r\nx_values = list(famsize = c(1,8), fathalc = c(1,0), livealc = c(1,0),\r\r\n                exhealth = c(0,1), educsq = c(0, mean(alcohol$educsq)))\r\r\n\r\r\nlm_predict <- predict(lm4_linear,x_values)\r\r\nlogit_predict <- predict(logit1.4_new, x_values, type= \"response\")\r\r\n\r\r\n\r\r\nprobit_predict <- predict(probit1.4_new, x_values, type = \"response\")\r\r\n\r\r\npredictions <- round(cbind(lm_predict, logit_predict, probit_predict),4)*100\r\r\nkable(predictions, caption = \"Alcohol Abuse Probabilites\",\r\r\n      col.names = c(\"LPM\",\"Logit\",\"Probit\"),\r\r\n      row.names = TRUE)\r\r\n\r\r\n\r\r\n\r\r\nTable 6: Alcohol Abuse Probabilites\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nLPM\r\r\n\r\r\n\r\r\nLogit\r\r\n\r\r\n\r\r\nProbit\r\r\n\r\r\n\r\r\n1\r\r\n\r\r\n\r\r\n24.43\r\r\n\r\r\n\r\r\n32.72\r\r\n\r\r\n\r\r\n30.91\r\r\n\r\r\n\r\r\n2\r\r\n\r\r\n\r\r\n0.24\r\r\n\r\r\n\r\r\n3.24\r\r\n\r\r\n\r\r\n2.99\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nI made predictions for 2 different individuals. Individual 1 has a family size of 1, their father is an alcoholic, they live with an alcoholic, they are not in perfect health, and I used 0 as the educsq variable. For individual 2, I pretty much did the opposite from individual 1 except for the educsq covariate, in which I used the mean. From looking at the LPM and Logit model above, I would expect the probability for individual 1 when compared to individual 2 to be much higher.\r\r\nAs expected, individual 1 has a much higher probability of abusing alcohol, 24%, 33% and 30% for the LPM, Logit, and Probit models respectively. Individual 2 has a much lower probability of abusing alcohol, .24%, 3.2% and 3% for the LPM, Logit, and Probit models respectively.\r\r\nConclusion\r\r\nOverall, I found this project very useful. If I were to do this project over again, I would probably choose a different dataset, or use a different indicator as my outcome variable. When I saw the very low R squared value for my initial models, I thought I could increase that number by choosing the most the right covariates. I was able to increase the R squared value by 37% but it was still quite low.\r\r\nAutomating the linear models was quite difficult. The models would not run smoothly in my loops, there were many different issues that were quite time consuming. For example, to loop through a Linear Model and print the results, the variables in the LM function needed to be in formula state before running the loop, or you would have to do that in the loop. They needed to be pasted as a formula before looped through. I found the loops with regression, not very user friendly. I also found that it was not very easy to loop and print summaries from models, I believe this was due to the functionality of LM, Glm functions.\r\r\nEven though the model did not explain much variation in the data, I do believe I was able to pick the best variables to predict whether an individual would abuse alcohol. I was not able to fully automate the process of picking the best fit model. I did not have enough time to focus on this part, I was able to automate part of the process. I found it a bit difficult to extract the linearHypothesis data to use for automation, but I was able to make this process a bit easier when applying to other data sets.\r\r\nI have a lot to learn, and I look forward to continuing my study into linear regression.\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/looking-into-alcohol-data-set/distill-preview.png",
    "last_modified": "2022-02-06T20:18:48-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/crime-in-the-time-of-covid/",
    "title": "Crime in the Time of Covid",
    "description": "A closer look into the relationship between crime and covid.",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-04",
    "categories": [],
    "contents": "\r\r\nIntroduction\r\r\nIn this hectic time of Covid 19 the world is changing, and its changing quickly. So quickly in fact that we barely have enough data to understand how these pandemic is really affecting our society. In this paper I will take a closer look into the effect of the covid Pandemic on Crime rates in the United States. I’ve obtained Crime Statistics from the Federal Bureau of Investigation (FBI), and Covid Statistics from CDC.\r\r\nUtilizing the FBI’s and the CDC’s publicly available data, I’m proposing that Crime rates in the are United States are inversely related to the current covid 19 Hype in the United States. In other words, the stronger the covid 19 hype in a particular state, the lower the expected crimes.\r\r\nMethod\r\r\nGather, Clean and Prep\r\r\nThe first Step is to read in the Data. I obtained the Arrests by State records for the United States for 2020 and 2019. After I read in the data, the next step is to clean the data in order to prepare for the analysis\r\r\nWhen pulling in the data, I noticed multiple issues. For example the column that included the State name was not showing the correct state for each row. The State names showed numbers when they should only have the name. The crime descriptor columns were written in ways that didn’t make sense. To fix these issues I used the following code below. Since the 2019 and 2020 csv sheets are in the same format, once I fixed one sheet, I just needed to repeat the same steps for the 2019 code.\r\r\n\r\r\n\r\r\n# 2020 Cleaning\r\r\nUSA_2020_crimes <- crime_2020 %>% fill(State)\r\r\nUSA_2020_crimes<-  USA_2020_crimes %>% mutate(States = removeNumbers(State))\r\r\nCrime_data <- rename(USA_2020_crimes,  \"Property Crimes\" = \"Property\\ncrime2\", \r\r\n\"Rape\" = \"Rape3\", \r\r\n\"Aggravated Assault\" = \"Aggravated\\nassault\",\r\r\n \"Drug and Abuse Violations\" = \"Drug \\nabuse\\nviolations\", \r\r\n\"Violent Crimes\" = \"Violent\\ncrime2\", \"Murder\" = \"Murder and\\nnonnegligent\\nmanslaughter\" )\r\r\nCrime_data$States[Crime_data$States == \"FLORIDA, \"]<- \"FLORIDA\"\r\r\n\r\r\n\r\r\n#2019 Cleaning\r\r\nUSA_2019_crimes <- crime_2019 %>% fill(State)\r\r\nUSA_2019_crimes<-  USA_2019_crimes %>% mutate(States = removeNumbers(State))\r\r\nCrime_data_2019 <- rename(USA_2019_crimes,  \"Property Crimes\" = \"Property\\ncrime2\", \r\r\n\"Rape\" = \"Rape3\",\r\r\n\"Aggravated Assault\" = \"Aggravated\\nassault\",\r\r\n\"Drug and Abuse Violations\" = \"Drug \\nabuse\\nviolations\",\r\r\n\"Violent Crimes\" = \"Violent\\ncrime2\", \"Murder\" = \"Murder and\\nnonnegligent\\nmanslaughter\" )\r\r\nCrime_data_2019$States[Crime_data_2019$States == \"FLORIDA, \" ] <- \"FLORIDA\"\r\r\n\r\r\n\r\r\n\r\r\nInitial Plots\r\r\nNow that the data is cleaned, I can now take initial glimpses into the data I’ve collected. To perform this I used the package “usmap”. The way this package works is that you use the fips_info from the usmap package to tell the plot “usmap” function which states, citys and regions you plan to map. For this project, I am focusing on all States. So before I can plot my crime data, I first need to join the fips_info with my crime data. That way the plot us map function knows which state to correctly link what ever values I plan on plotting. For this project I decided to focus on only a few crimes, Property crimes, Aggravated Assault, and Murder.\r\r\n\r\r\n\r\r\nlibrary(usmap)\r\r\nstate_info <- fips_info()\r\r\nstate_info <- state_info %>% mutate(\"States\" = toupper(full)) \r\r\ncrimes <- c(\"Property Crimes\", \"Aggravated Assault\",\"Murder\")\r\r\n\r\r\n# Plotting 2020 Crimes\r\r\nfor (crime in crimes){\r\r\n  x <-  Crime_data %>% filter(X2 ==\"Total all ages\") %>% select(States,crime) %>% \r\r\n    distinct(States, .keep_all = TRUE)\r\r\n  y <- full_join(x,state_info, by = \"States\")\r\r\n  z <- plot_usmap(data = y, values = crime, labels = TRUE, color = \"gray\")+ \r\r\nscale_fill_continuous(low = \"white\", high = \"blue\", name = paste(crime), \r\r\nlabel = scales::comma)+\r\r\n    labs(title = paste(crime,\"in the US\"), subtitle = \"2020 Crime DATA\")+\r\r\n    theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) +\r\r\ntheme(legend.position = \"right\")\r\r\n  print(z)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n# Plotting 2019 \r\r\nfor (crime in crimes){\r\r\n  x <-  Crime_data_2019 %>% filter(X2 ==\"Total all ages\") %>% select(States,crime) %>% distinct(States, .keep_all = TRUE)\r\r\n  y <- full_join(x,state_info, by = \"States\")\r\r\n  z <- plot_usmap(data = y, values = crime, labels = TRUE, color = \"gray\")+ scale_fill_continuous(low = \"white\", high = \"blue\", name = paste(crime), label = scales::comma)+\r\r\n    labs(title = paste(crime,\"in the US\"), subtitle = \"2019 Crime DATA\")+\r\r\n    theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n  print(z)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n2020 and 2019 Results\r\r\nIn all three figures for both 2020 and 2019, CA has the highest amount of crimes. There may be a lot of crime there, but this could also be due to the amount of population. Due to this issue, with out having the per-capita results, this data could be misleading. In order to really understand whats going on here I will need to dive deeper into the data.\r\r\nFuther Analysis\r\r\nTo truly understand what is happening, the total crimes themselves itself don’t matter. Instead I want to see the % change of crimes from 2019 to 2020. I will do this for all three crimes listed. Once I have the % change in crimes listed, then I can compare agains the covid data.\r\r\n\r\r\n\r\r\nCrime_data_2020_1 <- Crime_data %>% filter(X2 ==\"Total all ages\") %>% select(States,`Property Crimes`,`Aggravated Assault`,Murder) %>% distinct(States, .keep_all = TRUE)\r\r\n\r\r\nCrime_data_2019_1 <- Crime_data_2019 %>%  filter(X2 ==\"Total all ages\") %>% select(States,`Property Crimes`,`Aggravated Assault`,Murder) %>% distinct(States, .keep_all = TRUE) %>% rename(\"Property crime 2019\" = `Property Crimes`, \"Aggravated Assault 2019\"= `Aggravated Assault`, \"Murder 2019\" = Murder)\r\r\n\r\r\n# Join DATA and Create Percent change columns\r\r\n\r\r\nCrime_Change_data <- inner_join(Crime_data_2019_1,Crime_data_2020_1, by = \"States\")\r\r\nCrime_Change_data <- Crime_Change_data %>% mutate(\r\r\n  \"Property Crime % Change\" = (`Property Crimes` - `Property crime 2019`)/(`Property crime 2019`),\r\r\n  \"Aggravated Assualt % Change\" = (`Aggravated Assault` - `Aggravated Assault 2019`)/ (`Aggravated Assault 2019`),\r\r\n  \"Murder % Change\" = (Murder - `Murder 2019`)/`Murder 2019`\r\r\n)\r\r\n\r\r\n\r\r\n\r\r\nThe code block above shows how I combined the 2020 and 2019 data and then created a calculated column for each crime.\r\r\nPlot Percent Change of Crime DATA\r\r\n\r\r\n\r\r\ncrimes <- c(\"Property Crime % Change\", \"Aggravated Assualt % Change\",\"Murder % Change\")\r\r\n\r\r\n# Plotting 2020 Crimes\r\r\nfor (crime in crimes){\r\r\n  y <- full_join(Crime_Change_data,state_info, by = \"States\")\r\r\n  z <- plot_usmap(data = y, values = crime, labels = TRUE, color = \"gray\")+ \r\r\n     scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \r\r\n   midpoint = 0, space = \"Lab\", \r\r\n   name=\"% Change\")+\r\r\n    #scale_fill_continuous(low = \"yellow\", high = \"blue\", name = paste(crime), label = scales::comma)+\r\r\n    labs(title = paste(crime,\"in the US\"), subtitle = \"2020 Crime DATA\")+\r\r\n    theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n  print(z)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nThese results are much better then the previous maps above. As we can see California and Texas are no longer in the top states for any of the crimes.\r\r\nFor Property crimes we an increase in Georgia and West Virginia, and a decrease in Pennsylvania and Delaware. For Aggravated Assault, Pennsylvania was the lowest, with others showing little to no increase. It looks like Georgia and Alabama were the only states with an increase from 2019 to 2020 for aggravated assault. It looks like murder did increase country wide except for PA and NM.\r\r\nTo truly understand these changes, I will also give a table of the top and bottom states for each crimes % change.\r\r\n\r\r\n\r\r\nlibrary(knitr)\r\r\n\r\r\ncrimes <- c(\"Property Crime % Change\", \"Aggravated Assualt % Change\",\"Murder % Change\")\r\r\n\r\r\nCrime_Change_data %>% arrange(`Property Crime % Change`) %>% select(States,`Property Crime % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Decrease in Property Crime\")\r\r\n\r\r\n\r\r\nTable 1: Largest Decrease in Property Crime\r\r\nStates\r\r\nProperty Crime % Change\r\r\nPENNSYLVANIA\r\r\n-0.9700970\r\r\nMARYLAND\r\r\n-0.9150567\r\r\nALABAMA\r\r\n-0.6379310\r\r\nHAWAII\r\r\n-0.5644000\r\r\nMISSISSIPPI\r\r\n-0.5119228\r\r\n\r\r\nCrime_Change_data %>% arrange(desc(`Property Crime % Change`)) %>% select(States, `Property Crime % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Increase in Property Crime\")\r\r\n\r\r\n\r\r\nTable 1: Largest Increase in Property Crime\r\r\nStates\r\r\nProperty Crime % Change\r\r\nWEST VIRGINIA\r\r\n0.8412447\r\r\nGEORGIA\r\r\n0.5394141\r\r\nWYOMING\r\r\n0.0521376\r\r\nMISSOURI\r\r\n0.0196389\r\r\nNORTH CAROLINA\r\r\n-0.0105206\r\r\n\r\r\nCrime_Change_data %>% arrange(`Aggravated Assualt % Change`) %>% select(States,`Aggravated Assualt % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Decrease in Assault\")\r\r\n\r\r\n\r\r\nTable 1: Largest Decrease in Assault\r\r\nStates\r\r\nAggravated Assualt % Change\r\r\nPENNSYLVANIA\r\r\n-0.9767876\r\r\nMARYLAND\r\r\n-0.9482368\r\r\nHAWAII\r\r\n-0.4385965\r\r\nKENTUCKY\r\r\n-0.3846154\r\r\nMISSISSIPPI\r\r\n-0.3435028\r\r\n\r\r\nCrime_Change_data %>% arrange(desc(`Aggravated Assualt % Change`)) %>% select(States, `Aggravated Assualt % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Increase in Assault\")\r\r\n\r\r\n\r\r\nTable 1: Largest Increase in Assault\r\r\nStates\r\r\nAggravated Assualt % Change\r\r\nALABAMA\r\r\n6.0909091\r\r\nGEORGIA\r\r\n1.7259380\r\r\nSOUTH DAKOTA\r\r\n0.8287129\r\r\nNORTH DAKOTA\r\r\n0.4169381\r\r\nWEST VIRGINIA\r\r\n0.3105968\r\r\n\r\r\nCrime_Change_data %>% arrange(`Murder % Change`) %>% select(States,`Murder % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Decrease in Murder\")\r\r\n\r\r\n\r\r\nTable 1: Largest Decrease in Murder\r\r\nStates\r\r\nMurder % Change\r\r\nPENNSYLVANIA\r\r\n-0.9714286\r\r\nMARYLAND\r\r\n-0.9370079\r\r\nNEW HAMPSHIRE\r\r\n-0.8666667\r\r\nNEW MEXICO\r\r\n-0.5918367\r\r\nKENTUCKY\r\r\n-0.3229814\r\r\n\r\r\nCrime_Change_data %>% arrange(desc(`Murder % Change`)) %>% select(States, `Murder % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Increase in Murder\")\r\r\n\r\r\n\r\r\nTable 1: Largest Increase in Murder\r\r\nStates\r\r\nMurder % Change\r\r\nDISTRICT OF COLUMBIA\r\r\nInf\r\r\nGEORGIA\r\r\n1.8857143\r\r\nALABAMA\r\r\n1.0000000\r\r\nWEST VIRGINIA\r\r\n0.8750000\r\r\nINDIANA\r\r\n0.8309859\r\r\n\r\r\nResults\r\r\nThe Tables above show the top 5 states per crime for largest increase in percentage and the top 5 states for the largest decrease in percentage. The largest decrease for all three crimes is from Pennsylvania and that was at 1%. The largest increase was for Alabama, which was an 6% increase in aggravated assault.\r\r\nMethod Continued\r\r\nNow that I have a good idea of What states saw the largest increases and decreases in crime. I will now correlate that over to Covid Data provided by CDC.\r\r\nCovid DATA\r\r\nWhen dealing with Covid, there are many different metrics that can be utilized. We could look at daily counts of infected, daily deaths, hospitalizations, or even total vaccinations. For this analysis I am only going to focus on the amount of deaths recorded in 2020 by state.\r\r\nThe plan is to compare the % change by the three crimes listed above by yearly total of deaths caused by Covid 19.\r\r\nCleaning and Prepping DATA\r\r\n\r\r\n\r\r\n#str(covid_data)\r\r\ncovid_data$`End Date` <- as.Date(covid_data$`End Date`,\"%m/%d/%Y\")\r\r\n#str(covid_data)\r\r\n#covid_data\r\r\nfiltered_covid <- covid_data %>% filter(`End Date`< \"2021-01-01\") %>% filter(`Place of Death` == \"Total - All Places of Death\" & State != \"United States\" & Group == \"By Year\") %>% mutate(\"States\" = toupper(State) ) %>% select(States, `COVID-19 Deaths`)\r\r\n\r\r\n\r\r\n\r\r\nI First needed to clean and prep the data. I changed the dates column from a character type to a date type and filtered out the unnecessary information. I then plotted the deaths and normalized that data across all states using the plot “usmap” function.\r\r\n\r\r\n\r\r\n covid_mapdata <- full_join(filtered_covid,state_info, by = \"States\")\r\r\ndata_map <- covid_mapdata %>% select(fips, `COVID-19 Deaths`) %>% rename(\"Deaths\" = `COVID-19 Deaths`)\r\r\n\r\r\ndata_map <- data.frame(data_map)\r\r\n\r\r\n\r\r\nplot_usmap(data = data_map, values = \"Deaths\",labels = TRUE, color = \"gray\")+\r\r\n  scale_fill_continuous(low = \"white\", high = \"blue\", name = \"Covid Fatalities\", label = scales::comma)+\r\r\n    labs(title = \"Covid Fatalitys in 2020\")+\r\r\n   theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nIt looks like California and Texas had the largest amount of deaths around 30K. This info can be misleading, since states with lower populations will most likely have lower deaths but those deaths could be a higher percentage of the total population. To rectify this I will bring in the 2020 population data from the US Census for 2020, and I will create a field that is the amount of deaths per 100K population.\r\r\n\r\r\n\r\r\npopulation_2020 <- readxl::read_xlsx(\"2020 population.xlsx\")\r\r\n\r\r\n\r\r\n\r\r\npopulation_2020$State <- toupper(population_2020$State)\r\r\npopulation_2020 <- rename(population_2020, \"States\" = State)\r\r\n\r\r\nCovid_data_updated <- inner_join(covid_mapdata, population_2020, by =\"States\")\r\r\nCovid_data_updated <- Covid_data_updated %>% mutate(\r\r\n  Per_capita = (`COVID-19 Deaths`/`2020 Census`)*100000\r\r\n)\r\r\n\r\r\n\r\r\nplot_usmap(data = Covid_data_updated, values = \"Per_capita\",labels = TRUE, color = \"gray\")+\r\r\n  scale_fill_continuous(low = \"white\", high = \"blue\", name = \"Covid Fatalities\", label = scales::comma)+\r\r\n    labs(title = \"Covid Fatalitys in 2020\")+\r\r\n   theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nAs we can see, the chart looks much different than the previous one. It looks like North Dakota, South Dakota and Delaware had the highest fatalities per 100k population. Hawaii had the lowest. I will now create a data table to look at the top and bottom % per crime vs the top and bottom percentage per fatalities.\r\r\n\r\r\n\r\r\nTop_covid <- Covid_data_updated %>% arrange(desc(Per_capita)) %>% select(States,Per_capita) %>% slice(1:5)\r\r\n\r\r\nLow_covid <- Covid_data_updated %>% arrange((Per_capita)) %>% select(States,Per_capita) %>% slice(1:5)\r\r\n\r\r\n\r\r\nTop_covid %>% kable(caption = \"Highest Covid Fatalities by State\")\r\r\n\r\r\n\r\r\nTable 2: Highest Covid Fatalities by State\r\r\nStates\r\r\nPer_capita\r\r\nNEW JERSEY\r\r\n195.3172\r\r\nNORTH DAKOTA\r\r\n194.1999\r\r\nSOUTH DAKOTA\r\r\n193.4210\r\r\nRHODE ISLAND\r\r\n174.7801\r\r\nCONNECTICUT\r\r\n174.4619\r\r\n\r\r\nLow_covid %>% kable(caption = \"Lowest Covid Fatalities by State\")\r\r\n\r\r\n\r\r\nTable 2: Lowest Covid Fatalities by State\r\r\nStates\r\r\nPer_capita\r\r\nVERMONT\r\r\n22.54784\r\r\nHAWAII\r\r\n25.08124\r\r\nMAINE\r\r\n34.13197\r\r\nALASKA\r\r\n34.63364\r\r\nOREGON\r\r\n38.04349\r\r\n\r\r\nx1<- Crime_Change_data %>% arrange(`Property Crime % Change`) %>% select(States,`Property Crime % Change`) %>% slice(1:5) \r\r\n\r\r\ny_1<-Crime_Change_data %>% arrange(desc(`Property Crime % Change`)) %>% select(States, `Property Crime % Change`) %>% slice(1:5) \r\r\n\r\r\nx2<- Crime_Change_data %>% arrange(`Aggravated Assualt % Change`) %>% select(States,`Aggravated Assualt % Change`) %>% slice(1:5) \r\r\ny2<- Crime_Change_data %>% arrange(desc(`Aggravated Assualt % Change`)) %>% select(States, `Aggravated Assualt % Change`) %>% slice(1:5)\r\r\n\r\r\nx3<- Crime_Change_data %>% arrange(`Murder % Change`) %>% select(States,`Murder % Change`) %>% slice(1:5)\r\r\ny3<- Crime_Change_data %>% arrange(desc(`Murder % Change`)) %>% select(States, `Murder % Change`) %>% slice(1:5)\r\r\n\r\r\n\r\r\n\r\r\ncbind(Top_covid,x1,x2,x3) \r\r\n\r\r\n\r\r\n        States Per_capita       States Property Crime % Change\r\r\n1   NEW JERSEY   195.3172 PENNSYLVANIA              -0.9700970\r\r\n2 NORTH DAKOTA   194.1999     MARYLAND              -0.9150567\r\r\n3 SOUTH DAKOTA   193.4210      ALABAMA              -0.6379310\r\r\n4 RHODE ISLAND   174.7801       HAWAII              -0.5644000\r\r\n5  CONNECTICUT   174.4619  MISSISSIPPI              -0.5119228\r\r\n        States Aggravated Assualt % Change        States\r\r\n1 PENNSYLVANIA                  -0.9767876  PENNSYLVANIA\r\r\n2     MARYLAND                  -0.9482368      MARYLAND\r\r\n3       HAWAII                  -0.4385965 NEW HAMPSHIRE\r\r\n4     KENTUCKY                  -0.3846154    NEW MEXICO\r\r\n5  MISSISSIPPI                  -0.3435028      KENTUCKY\r\r\n  Murder % Change\r\r\n1      -0.9714286\r\r\n2      -0.9370079\r\r\n3      -0.8666667\r\r\n4      -0.5918367\r\r\n5      -0.3229814\r\r\n\r\r\ncbind(Low_covid,y_1,y2,y3)\r\r\n\r\r\n\r\r\n   States Per_capita         States Property Crime % Change\r\r\n1 VERMONT   22.54784  WEST VIRGINIA              0.84124473\r\r\n2  HAWAII   25.08124        GEORGIA              0.53941411\r\r\n3   MAINE   34.13197        WYOMING              0.05213764\r\r\n4  ALASKA   34.63364       MISSOURI              0.01963886\r\r\n5  OREGON   38.04349 NORTH CAROLINA             -0.01052062\r\r\n         States Aggravated Assualt % Change               States\r\r\n1       ALABAMA                   6.0909091 DISTRICT OF COLUMBIA\r\r\n2       GEORGIA                   1.7259380              GEORGIA\r\r\n3  SOUTH DAKOTA                   0.8287129              ALABAMA\r\r\n4  NORTH DAKOTA                   0.4169381        WEST VIRGINIA\r\r\n5 WEST VIRGINIA                   0.3105968              INDIANA\r\r\n  Murder % Change\r\r\n1             Inf\r\r\n2       1.8857143\r\r\n3       1.0000000\r\r\n4       0.8750000\r\r\n5       0.8309859\r\r\n\r\r\n\r\r\n\r\r\nlibrary(knitr)\r\r\n\r\r\nunemployment_data <- read_csv(\"unemployement_rates.csv\")\r\r\nunemployment_data$States = toupper(unemployment_data$States)\r\r\nunemployment_data <- unemployment_data %>% mutate(UnemploymentRate_change = (`unemployment rate 2020`- `unemployment rate 2019`)/`unemployment rate 2020`)\r\r\n\r\r\nunemployment_data %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 12\r\r\n  States     `Pop 2019` `pop 2020` `labor force 201~ `labor force 202~\r\r\n  <chr>           <dbl>      <dbl>             <dbl>             <dbl>\r\r\n1 UNITED ST~     259175     260329            163539            160742\r\r\n2 NORTHEAST       45145      45097             28598             28013\r\r\n3 NEW ENGLA~      12136      12162              8072              7841\r\r\n4 CONNECTIC~       2885       2883              1917              1873\r\r\n5 MAINE            1112       1118               696               677\r\r\n6 MASSACHUS~       5636       5648              3782              3658\r\r\n# ... with 7 more variables: employed 2019 <dbl>,\r\r\n#   employed 2020 <dbl>, Unemployed 2019 <dbl>,\r\r\n#   unemployed 2020 <dbl>, unemployment rate 2019 <dbl>,\r\r\n#   unemployment rate 2020 <dbl>, UnemploymentRate_change <dbl>\r\r\n\r\r\nCrime_Change_data %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 10\r\r\n  States     `Property crime 20~ `Aggravated Assault 20~ `Murder 2019`\r\r\n  <chr>                    <dbl>                   <dbl>         <dbl>\r\r\n1 ALABAMA                    522                      11             4\r\r\n2 ALASKA                    3006                    2077            46\r\r\n3 ARIZONA                  29328                    8967           294\r\r\n4 ARKANSAS                 13433                    4120           166\r\r\n5 CALIFORNIA               88854                   83584          1284\r\r\n6 COLORADO                 25261                    6226           152\r\r\n# ... with 6 more variables: Property Crimes <dbl>,\r\r\n#   Aggravated Assault <dbl>, Murder <dbl>,\r\r\n#   Property Crime % Change <dbl>, Aggravated Assualt % Change <dbl>,\r\r\n#   Murder % Change <dbl>\r\r\n\r\r\nCovid_data_updated %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 9\r\r\n  States    `COVID-19 Deaths` abbr  fips  full      Rank `2020 Census`\r\r\n  <chr>                 <dbl> <chr> <chr> <chr>    <dbl>         <dbl>\r\r\n1 ALABAMA                6706 AL    01    Alabama     24       5024279\r\r\n2 ALASKA                  254 AK    02    Alaska      48        733391\r\r\n3 ARIZONA                9321 AZ    04    Arizona     14       7151502\r\r\n4 ARKANSAS               4027 AR    05    Arkansas    33       3011524\r\r\n5 CALIFORN~             33524 CA    06    Califor~     1      39538223\r\r\n6 COLORADO               5073 CO    08    Colorado    21       5773714\r\r\n# ... with 2 more variables: Percent of Total <dbl>, Per_capita <dbl>\r\r\n\r\r\nTotal_DATA_1 <- inner_join(Crime_Change_data,unemployment_data, by = \"States\")\r\r\nTotal_DATA_2 <- inner_join(Total_DATA_1, Covid_data_updated, by = \"States\")\r\r\nTotal_DATA_2 %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 29\r\r\n  States     `Property crime 20~ `Aggravated Assault 20~ `Murder 2019`\r\r\n  <chr>                    <dbl>                   <dbl>         <dbl>\r\r\n1 ALABAMA                    522                      11             4\r\r\n2 ALASKA                    3006                    2077            46\r\r\n3 ARIZONA                  29328                    8967           294\r\r\n4 ARKANSAS                 13433                    4120           166\r\r\n5 CALIFORNIA               88854                   83584          1284\r\r\n6 COLORADO                 25261                    6226           152\r\r\n# ... with 25 more variables: Property Crimes <dbl>,\r\r\n#   Aggravated Assault <dbl>, Murder <dbl>,\r\r\n#   Property Crime % Change <dbl>, Aggravated Assualt % Change <dbl>,\r\r\n#   Murder % Change <dbl>, Pop 2019 <dbl>, pop 2020 <dbl>,\r\r\n#   labor force 2019 <dbl>, labor force 2020 <dbl>,\r\r\n#   employed 2019 <dbl>, employed 2020 <dbl>, Unemployed 2019 <dbl>,\r\r\n#   unemployed 2020 <dbl>, unemployment rate 2019 <dbl>,\r\r\n#   unemployment rate 2020 <dbl>, UnemploymentRate_change <dbl>,\r\r\n#   COVID-19 Deaths <dbl>, abbr <chr>, fips <chr>, full <chr>,\r\r\n#   Rank <dbl>, 2020 Census <dbl>, Percent of Total <dbl>,\r\r\n#   Per_capita <dbl>\r\r\n\r\r\ncorr_data<- Total_DATA_2 %>% select(`Property Crime % Change`:`Murder % Change`,UnemploymentRate_change,`COVID-19 Deaths`,Per_capita)\r\r\ncorr_data <- corr_data %>% rename(\"COVID 19 Deaths Per Capita\" = Per_capita)\r\r\ncor(corr_data) %>% round(3)%>%kable()\r\r\n\r\r\n\r\r\n\r\r\nProperty Crime % Change\r\r\nAggravated Assualt % Change\r\r\nMurder % Change\r\r\nUnemploymentRate_change\r\r\nCOVID-19 Deaths\r\r\nCOVID 19 Deaths Per Capita\r\r\nProperty Crime % Change\r\r\n1.000\r\r\n0.098\r\r\n0.651\r\r\n-0.148\r\r\n-0.112\r\r\n-0.112\r\r\nAggravated Assualt % Change\r\r\n0.098\r\r\n1.000\r\r\n0.552\r\r\n-0.069\r\r\n-0.046\r\r\n0.103\r\r\nMurder % Change\r\r\n0.651\r\r\n0.552\r\r\n1.000\r\r\n-0.030\r\r\n-0.002\r\r\n0.056\r\r\nUnemploymentRate_change\r\r\n-0.148\r\r\n-0.069\r\r\n-0.030\r\r\n1.000\r\r\n0.262\r\r\n-0.114\r\r\nCOVID-19 Deaths\r\r\n-0.112\r\r\n-0.046\r\r\n-0.002\r\r\n0.262\r\r\n1.000\r\r\n0.192\r\r\nCOVID 19 Deaths Per Capita\r\r\n-0.112\r\r\n0.103\r\r\n0.056\r\r\n-0.114\r\r\n0.192\r\r\n1.000\r\r\n\r\r\n\r\r\n\r\r\nlibrary(reshape2)\r\r\n# Create a heatmap for cor matrix\r\r\ncorr_matrix <- cor(corr_data)\r\r\nmelted <- melt(corr_matrix)\r\r\nmelted %>% head()\r\r\n\r\r\n\r\r\n                         Var1                    Var2       value\r\r\n1     Property Crime % Change Property Crime % Change  1.00000000\r\r\n2 Aggravated Assualt % Change Property Crime % Change  0.09833195\r\r\n3             Murder % Change Property Crime % Change  0.65074643\r\r\n4     UnemploymentRate_change Property Crime % Change -0.14820711\r\r\n5             COVID-19 Deaths Property Crime % Change -0.11166874\r\r\n6  COVID 19 Deaths Per Capita Property Crime % Change -0.11235439\r\r\n\r\r\nmelted$Var1 <- as.character(melted$Var1)\r\r\nmelted$Var2 <- as.character(melted$Var2)\r\r\n\r\r\n\r\r\nggplot(data = melted, aes(Var2, Var1, fill = value))+\r\r\n geom_tile(color = \"white\")+\r\r\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \r\r\n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \r\r\n   name=\"Corr Matrix\") +\r\r\n  theme_minimal()+ \r\r\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \r\r\n    size = 12, hjust = 1))+\r\r\n coord_fixed()+\r\r\n  xlab(\"\")+\r\r\n  ylab(\"\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n# Corr Matrix''''\r\r\ncorr_data%>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 6\r\r\n  `Property Crime ~ `Aggravated Ass~ `Murder % Chang~ UnemploymentRat~\r\r\n              <dbl>            <dbl>            <dbl>            <dbl>\r\r\n1           -0.638           6.09               1                0.492\r\r\n2           -0.203           0.0197            -0.283            0.308\r\r\n3           -0.0922          0.0181             0.156            0.380\r\r\n4           -0.187           0.122              0.175            0.426\r\r\n5           -0.179          -0.00353            0.241            0.584\r\r\n6           -0.151           0.0895             0.553            0.630\r\r\n# ... with 2 more variables: COVID-19 Deaths <dbl>,\r\r\n#   COVID 19 Deaths Per Capita <dbl>\r\r\n\r\r\n pivot_data <- corr_data %>% pivot_longer(!c(`COVID-19 Deaths`,`COVID 19 Deaths Per Capita`),\r\r\n                                          \r\r\n    names_to = \"Factor\", \r\r\n    values_to = \"Percent_Change\",\r\r\n  )\r\r\n\r\r\n pivot_data\r\r\n\r\r\n\r\r\n# A tibble: 200 x 4\r\r\n   `COVID-19 Deaths` `COVID 19 Deaths Pe~ Factor        Percent_Change\r\r\n               <dbl>                <dbl> <chr>                  <dbl>\r\r\n 1              6706                133.  Property Cri~        -0.638 \r\r\n 2              6706                133.  Aggravated A~         6.09  \r\r\n 3              6706                133.  Murder % Cha~         1     \r\r\n 4              6706                133.  Unemployment~         0.492 \r\r\n 5               254                 34.6 Property Cri~        -0.203 \r\r\n 6               254                 34.6 Aggravated A~         0.0197\r\r\n 7               254                 34.6 Murder % Cha~        -0.283 \r\r\n 8               254                 34.6 Unemployment~         0.308 \r\r\n 9              9321                130.  Property Cri~        -0.0922\r\r\n10              9321                130.  Aggravated A~         0.0181\r\r\n# ... with 190 more rows\r\r\n\r\r\n pivot_data %>% ggplot()+\r\r\n   geom_point(aes(x = `COVID-19 Deaths`, y = Percent_Change ), color = \"deepskyblue2\")+geom_smooth(aes(x = `COVID-19 Deaths`, y = Percent_Change), se = F)+\r\r\n   facet_wrap(~Factor, scales = \"free\")\r\r\n\r\r\n\r\r\n\r\r\n  pivot_data\r\r\n\r\r\n\r\r\n# A tibble: 200 x 4\r\r\n   `COVID-19 Deaths` `COVID 19 Deaths Pe~ Factor        Percent_Change\r\r\n               <dbl>                <dbl> <chr>                  <dbl>\r\r\n 1              6706                133.  Property Cri~        -0.638 \r\r\n 2              6706                133.  Aggravated A~         6.09  \r\r\n 3              6706                133.  Murder % Cha~         1     \r\r\n 4              6706                133.  Unemployment~         0.492 \r\r\n 5               254                 34.6 Property Cri~        -0.203 \r\r\n 6               254                 34.6 Aggravated A~         0.0197\r\r\n 7               254                 34.6 Murder % Cha~        -0.283 \r\r\n 8               254                 34.6 Unemployment~         0.308 \r\r\n 9              9321                130.  Property Cri~        -0.0922\r\r\n10              9321                130.  Aggravated A~         0.0181\r\r\n# ... with 190 more rows\r\r\n\r\r\n pivot_data %>% ggplot()+\r\r\n   geom_point(aes(x = `COVID 19 Deaths Per Capita`, y = Percent_Change ), color = \"deepskyblue2\")+geom_smooth(aes(x = `COVID 19 Deaths Per Capita`, y = Percent_Change), se = F)+\r\r\n   facet_wrap(~Factor, scales = \"free\")\r\r\n\r\r\n\r\r\n\r\r\n pivot_data \r\r\n\r\r\n\r\r\n# A tibble: 200 x 4\r\r\n   `COVID-19 Deaths` `COVID 19 Deaths Pe~ Factor        Percent_Change\r\r\n               <dbl>                <dbl> <chr>                  <dbl>\r\r\n 1              6706                133.  Property Cri~        -0.638 \r\r\n 2              6706                133.  Aggravated A~         6.09  \r\r\n 3              6706                133.  Murder % Cha~         1     \r\r\n 4              6706                133.  Unemployment~         0.492 \r\r\n 5               254                 34.6 Property Cri~        -0.203 \r\r\n 6               254                 34.6 Aggravated A~         0.0197\r\r\n 7               254                 34.6 Murder % Cha~        -0.283 \r\r\n 8               254                 34.6 Unemployment~         0.308 \r\r\n 9              9321                130.  Property Cri~        -0.0922\r\r\n10              9321                130.  Aggravated A~         0.0181\r\r\n# ... with 190 more rows\r\r\n\r\r\n\r\r\n\r\r\nlibrary(texreg); library(lmtest)\r\r\n\r\r\n# Run Regression Analysis\r\r\n\r\r\n# Outcome Variable - Crime\r\r\n# Assualt\r\r\n\r\r\nlpm_assault <- lm(`Aggravated Assualt % Change`  ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita` +\r\r\n                    UnemploymentRate_change , data = corr_data)\r\r\n\r\r\n\r\r\n\r\r\nlpm_Property_crime <- lm(`Property Crime % Change` ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita`\r\r\n                         +UnemploymentRate_change, data = corr_data)\r\r\n\r\r\n\r\r\nlpm_Murder<- lm(`Murder % Change` ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita` \r\r\n                +UnemploymentRate_change, data = corr_data)\r\r\n\r\r\n\r\r\n\r\r\nscreenreg(list(lpm_assault,lpm_Property_crime,lpm_Murder), custom.header = list(\"Crime LPM's\" = 1:3),custom.model.names = c(\"Assault\",\"Murder\",\"Property\"))\r\r\n\r\r\n\r\r\n\r\r\n=======================================================\r\r\n                                     Crime LPM's       \r\r\n                              -------------------------\r\r\n                              Assault  Murder  Property\r\r\n-------------------------------------------------------\r\r\n(Intercept)                    0.10     0.08    0.14   \r\r\n                              (0.83)   (0.24)  (0.43)  \r\r\n`COVID-19 Deaths`             -0.00    -0.00   -0.00   \r\r\n                              (0.00)   (0.00)  (0.00)  \r\r\n`COVID 19 Deaths Per Capita`   0.00    -0.00    0.00   \r\r\n                              (0.00)   (0.00)  (0.00)  \r\r\nUnemploymentRate_change       -0.39    -0.40   -0.10   \r\r\n                              (1.41)   (0.41)  (0.73)  \r\r\n-------------------------------------------------------\r\r\nR^2                            0.02     0.04    0.00   \r\r\nAdj. R^2                      -0.05    -0.02   -0.06   \r\r\nNum. obs.                     50       50      50      \r\r\n=======================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\nsummary(lpm_assault)\r\r\n\r\r\n\r\r\n\r\r\nCall:\r\r\nlm(formula = `Aggravated Assualt % Change` ~ `COVID-19 Deaths` + \r\r\n    `COVID 19 Deaths Per Capita` + UnemploymentRate_change, data = corr_data)\r\r\n\r\r\nResiduals:\r\r\n    Min      1Q  Median      3Q     Max \r\r\n-1.0910 -0.2011 -0.0874  0.0075  5.9100 \r\r\n\r\r\nCoefficients:\r\r\n                               Estimate Std. Error t value Pr(>|t|)\r\r\n(Intercept)                   1.020e-01  8.286e-01   0.123    0.903\r\r\n`COVID-19 Deaths`            -6.872e-06  1.926e-05  -0.357    0.723\r\r\n`COVID 19 Deaths Per Capita`  2.367e-03  3.281e-03   0.721    0.474\r\r\nUnemploymentRate_change      -3.885e-01  1.414e+00  -0.275    0.785\r\r\n\r\r\nResidual standard error: 0.962 on 46 degrees of freedom\r\r\nMultiple R-squared:  0.01673,   Adjusted R-squared:  -0.04739 \r\r\nF-statistic: 0.2609 on 3 and 46 DF,  p-value: 0.8531\r\r\n\r\r\n\r\r\n\r\r\n# Assualt\r\r\nLpm_unemployment <- lm(UnemploymentRate_change  ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita` + `Property Crime % Change`+`Murder % Change`+`Aggravated Assualt % Change` , data = corr_data)\r\r\n\r\r\nsummary(Lpm_unemployment)\r\r\n\r\r\n\r\r\n\r\r\nCall:\r\r\nlm(formula = UnemploymentRate_change ~ `COVID-19 Deaths` + `COVID 19 Deaths Per Capita` + \r\r\n    `Property Crime % Change` + `Murder % Change` + `Aggravated Assualt % Change`, \r\r\n    data = corr_data)\r\r\n\r\r\nResiduals:\r\r\n     Min       1Q   Median       3Q      Max \r\r\n-0.22078 -0.05388 -0.01327  0.05639  0.22991 \r\r\n\r\r\nCoefficients:\r\r\n                                Estimate Std. Error t value Pr(>|t|)\r\r\n(Intercept)                    5.005e-01  4.224e-02  11.849 2.78e-15\r\r\n`COVID-19 Deaths`              3.508e-06  1.949e-06   1.800   0.0787\r\r\n`COVID 19 Deaths Per Capita`  -4.620e-04  3.426e-04  -1.349   0.1843\r\r\n`Property Crime % Change`     -1.083e-01  7.765e-02  -1.395   0.1699\r\r\n`Murder % Change`              5.317e-02  5.187e-02   1.025   0.3109\r\r\n`Aggravated Assualt % Change` -1.603e-02  2.021e-02  -0.793   0.4318\r\r\n                                 \r\r\n(Intercept)                   ***\r\r\n`COVID-19 Deaths`             .  \r\r\n`COVID 19 Deaths Per Capita`     \r\r\n`Property Crime % Change`        \r\r\n`Murder % Change`                \r\r\n`Aggravated Assualt % Change`    \r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nResidual standard error: 0.1003 on 44 degrees of freedom\r\r\nMultiple R-squared:  0.1363,    Adjusted R-squared:  0.03815 \r\r\nF-statistic: 1.389 on 5 and 44 DF,  p-value: 0.247\r\r\n\r\r\nResults\r\r\nFor this project, I decided to just focus on property crimes, murder, and aggravated assault as the crimes of interest in this analysis. When property crimes, most of the states saw a decrease in property crime in 2020 when compared to 2021. Pennsylvania saw the largest decrease in property crime while Georgia and West Virginia saw the largest increases in property crime. It appears that most of the states saw an increase in murders from 2019 to 2020, with Georgia seeing the largest increase. There was very little percent change in either direction across all states for aggravated assault, except for Alabama, where we saw a 6% increase in aggravated assault. The correlation Matrix shows a high correlation between murder percent change and aggravated assault percent change. It also looks like murder has a high correlation with property crime percent change. COVID-19 deaths shows a negative correlation with property crimes and a positive correlation with unemployment rate. COVID-19 deaths per-capita, surprisingly, has a negative correlation with unemployment rate. I created three unrestricted linear regression models, one for each crime. When looking at Figure 8, the murder linear model explained the largest amount of variance at 4%. None of the models had any statistically significant covariates.\r\r\nConclusion\r\r\nCrime rates in the United States are inversely proportional to the number of COVID-19 deaths for the crimes analyzed in this project. When deaths per capita were introduced, COVID-19 deaths per capita were proportional to assault and property crimes but inversely related to murder. Surprisingly, unemployment rates seem to be inversely proportional to crime rates.\r\r\nIt should be noted that none of the models have any covariates that are statistically significant (all p-values were >.05). For future research, I would recommend creating a larger sample size and looking into new crimes. It would also be beneficial to create smaller groups by states and regions and incorporating local policies such as stay-at-home orders.\r\r\nReferences\r\r\nCrime, https://crime-data-explorer.fr.cloud.gov/pages/home. “Table 1. Employment Status of the Civilian Noninstitutional Population 16 Years of Age and over by Region, Division, and State, 2019-20 Annual Averages.” U.S. Bureau of Labor Statistics, U.S. Bureau of Labor Statistics, 3 Mar. 2021, https://www.bls.gov/news.release/srgune.t01.htm. “Provisional COVID-19 Deaths by Place of Death and State.” Centers for Disease Control and Prevention, Centers for Disease Control and Prevention, https://data.cdc.gov/NCHS/Provisional-COVID-19-Deaths-by-Place-of-Death-and-/uggs-hy5q.\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/crime-in-the-time-of-covid/distill-preview.png",
    "last_modified": "2022-02-04T10:10:34-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Intro Message",
    "description": "Welcome to my Blog.",
    "author": [
      {
        "name": "Peter",
        "url": {}
      }
    ],
    "date": "2022-02-01",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-03T15:08:59-05:00",
    "input_file": {}
  }
]
