[
  {
    "path": "posts/networks-hw-2/",
    "title": "Networks Hw 2",
    "description": "A closer look at Enrons Emails",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\r\nLooking at Nodes and Edges:\r\r\n\r\r\n\r\r\nls()\r\r\n\r\r\n\r\r\n[1] \"network_edgelist\" \"network_igraph\"   \"network_statnet\" \r\r\n\r\r\nvcount(network_igraph)\r\r\n\r\r\n\r\r\n[1] 184\r\r\n\r\r\necount(network_igraph)\r\r\n\r\r\n\r\r\n[1] 125409\r\r\n\r\r\nprint(network_statnet)\r\r\n\r\r\n\r\r\n Network attributes:\r\r\n  vertices = 184 \r\r\n  directed = TRUE \r\r\n  hyper = FALSE \r\r\n  loops = FALSE \r\r\n  multiple = FALSE \r\r\n  bipartite = FALSE \r\r\n  total edges= 3010 \r\r\n    missing edges= 0 \r\r\n    non-missing edges= 3010 \r\r\n\r\r\n Vertex attribute names: \r\r\n    vertex.names \r\r\n\r\r\n Edge attribute names not shown \r\r\n\r\r\n#print(network_igraph)\r\r\n\r\r\n\r\r\n\r\r\nIt looks like the igraph and statnet variables are showing different edges. The network igraph is showing 184 nodes and 125409 edges. The network statnet is showing 184 nodes, and 3010 edges.\r\r\n\r\r\nWeighted, Directed, Single Mode Network?\r\r\n\r\r\n\r\r\nis_bipartite(network_igraph)\r\r\n\r\r\n\r\r\n[1] FALSE\r\r\n\r\r\nis_directed(network_igraph)\r\r\n\r\r\n\r\r\n[1] TRUE\r\r\n\r\r\nis_weighted(network_igraph)\r\r\n\r\r\n\r\r\n[1] FALSE\r\r\n\r\r\nUsing the Network Igraph set, we have a single mode network, which is directed, and is not weighted.\r\r\n\r\r\nLooking at Vertex and Edge Attributes:\r\r\n\r\r\n\r\r\nvertex_attr_names(network_igraph)\r\r\n\r\r\n\r\r\n[1] \"Email\" \"Name\"  \"Note\" \r\r\n\r\r\nnetwork::list.vertex.attributes(network_statnet)\r\r\n\r\r\n\r\r\n[1] \"na\"           \"vertex.names\"\r\r\n\r\r\nedge_attr_names(network_igraph)\r\r\n\r\r\n\r\r\n[1] \"Time\"      \"Reciptype\" \"Topic\"     \"LDC_topic\"\r\r\n\r\r\nnetwork::list.edge.attributes(network_statnet)\r\r\n\r\r\n\r\r\n[1] \"LDC_topic\"      \"LDC_topic_desc\" \"LDC_topic_name\"\r\r\n[4] \"na\"             \"Reciptype\"      \"Time\"          \r\r\n[7] \"Topic\"         \r\r\n\r\r\nIgraph Attribute Names: Email, Name, Note\r\r\nIgraph edge names: Time, Reciptype, Topic, LDC_topic\r\r\nStatnet attribute names: na, vertex.names\r\r\nstatnet edge names: LDC_topic, LDC_topic_desc, LDC_topic_name, na, Reciptype, Time, Topic\r\r\n\r\r\nAccessing Attribute DATA:\r\r\n\r\r\n\r\r\nV(network_igraph)$Name %>% head()\r\r\n\r\r\n\r\r\n[1] \"Albert Meyers\"    \"Thomas Martin\"    \"Andrea Ring\"     \r\r\n[4] \"Andrew Lewis\"     \"Andy Zipper\"      \"Jeffrey Shankman\"\r\r\n\r\r\nV(network_igraph)$Email %>% head()\r\r\n\r\r\n\r\r\n[1] \"albert.meyers\" \"a..martin\"     \"andrea.ring\"   \"andrew.lewis\" \r\r\n[5] \"andy.zipper\"   \"a..shankman\"  \r\r\n\r\r\nV(network_igraph)$Note %>% head()\r\r\n\r\r\n\r\r\n[1] \"Employee, Specialist\"         \"Vice President\"              \r\r\n[3] \"NA\"                           \"Director\"                    \r\r\n[5] \"Vice President, Enron Online\" \"President, Enron Global Mkts\"\r\r\n\r\r\n(network_igraph)$Carrier %>% head()\r\r\n\r\r\n\r\r\nNULL\r\r\n\r\r\nhead(network_statnet %v% \"vertex.names\")\r\r\n\r\r\n\r\r\n[1] 1 2 3 4 5 6\r\r\n\r\r\nhead(network_statnet %e% \"Time\")\r\r\n\r\r\n\r\r\n[1] \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\"\r\r\n[4] \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\"\r\r\n\r\r\nhead(network_statnet %e% \"LDC_topic\")\r\r\n\r\r\n\r\r\n[1] \"-1\" \"-1\" \"-1\" \"-1\" \"-1\" \"-1\"\r\r\n\r\r\n\r\r\nSummarizing Attribute DATA\r\r\n\r\r\n\r\r\nsummary(E(network_igraph)$Time)\r\r\n\r\r\n\r\r\n   Length     Class      Mode \r\r\n   125409 character character \r\r\n\r\r\nsummary(network_statnet %e% \"Distance\")\r\r\n\r\r\n\r\r\nLength  Class   Mode \r\r\n     0   NULL   NULL \r\r\n\r\r\n #### Dyad Census\r\r\n\r\r\n\r\r\nigraph::dyad.census(network_igraph)\r\r\n\r\r\n\r\r\n$mut\r\r\n[1] 30600\r\r\n\r\r\n$asym\r\r\n[1] 64208\r\r\n\r\r\n$null\r\r\n[1] -77972\r\r\n\r\r\nsna::dyad.census(network_statnet)\r\r\n\r\r\n\r\r\n     Mut Asym  Null\r\r\n[1,] 913 1184 14739\r\r\n\r\r\nThe dyad census for null using igraph is coming up -77,972. This seems wrong.\r\r\n\r\r\nTriad Census\r\r\n\r\r\n\r\r\nigraph::triad.census(network_igraph)\r\r\n\r\r\n\r\r\n [1] 700234  19530 249694   8409   2695   5176   7060  13227   1180\r\r\n[10]     59   6781   1023   1137    786   2782   1611\r\r\n\r\r\nsna::triad.census(network_statnet)\r\r\n\r\r\n\r\r\n        003    012    102 021D 021U 021C 111D  111U 030T 030C  201\r\r\n[1,] 700234 150250 118974 8409 2695 5176 7060 13227 1180   59 6781\r\r\n     120D 120U 120C  210  300\r\r\n[1,] 1023 1137  786 2782 1611\r\r\n\r\r\n\r\r\nTransitivity\r\r\n\r\r\n\r\r\ntransitivity(network_igraph)\r\r\n\r\r\n\r\r\n[1] 0.3725138\r\r\n\r\r\ngtrans(network_statnet)\r\r\n\r\r\n\r\r\n[1] 0.3580924\r\r\n\r\r\ntransitivity(network_igraph, type = \"global\")\r\r\n\r\r\n\r\r\n[1] 0.3725138\r\r\n\r\r\ntransitivity(network_igraph, type = \"average\")\r\r\n\r\r\n\r\r\n[1] 0.5055302\r\r\n\r\r\ntransitivity(network_igraph, type = \"local\") %>% head()\r\r\n\r\r\n\r\r\n[1] 0.0023288309 0.0013788877 0.0008393993 0.0031740105 0.0007847921\r\r\n[6] 0.0017129438\r\r\n\r\r\nThe transitivity for igraph and statnet data sets were pretty close.\r\r\nThe global transitivity is .3725 while the average transitivity is higher at .5. This means that actors with fewer connections will have higher transitivity. This could be due to overweighted groups or this could be similar to different departments that know a lot of people in their department, but do not know others in the other departments.\r\r\n\r\r\nLocal Transitivity\r\r\n\r\r\n\r\r\nNames <- V(network_igraph)$Name\r\r\nNames %>% head()\r\r\n\r\r\n\r\r\n[1] \"Albert Meyers\"    \"Thomas Martin\"    \"Andrea Ring\"     \r\r\n[4] \"Andrew Lewis\"     \"Andy Zipper\"      \"Jeffrey Shankman\"\r\r\n\r\r\nLocal_transivity <- transitivity(network_igraph, type = \"local\")\r\r\n\r\r\ntransitivity_tibble <- tibble(Names = Names, Local_transivity = Local_transivity)\r\r\n\r\r\ntransitivity_tibble %>% arrange(desc(Local_transivity))\r\r\n\r\r\n\r\r\n# A tibble: 184 x 2\r\r\n   Names            Local_transivity\r\r\n   <chr>                       <dbl>\r\r\n 1 Thomas Martin             0.0571 \r\r\n 2 Joe Quenet                0.0179 \r\r\n 3 Mark Haedicke             0.0159 \r\r\n 4 Kim Ward                  0.0157 \r\r\n 5 Peter Keavey              0.0146 \r\r\n 6 Monika Causholli          0.0134 \r\r\n 7 David Delainey            0.00917\r\r\n 8 Susan Pereira             0.00909\r\r\n 9 Larry Campbell            0.00810\r\r\n10 NA                        0.00641\r\r\n# ... with 174 more rows\r\r\n\r\r\ngtrans(network_statnet)\r\r\n\r\r\n\r\r\n[1] 0.3580924\r\r\n\r\r\nFor some reason I am unable to pull local transitivity by type using the method used in HW 1 (vids = V()). I’m not sure if these local transivitys are correct. I ordered it by descending so the largest transivity would be thomas Martin at .05.\r\r\n\r\r\nDistances in the Network\r\r\n\r\r\n\r\r\n#distances(network_igraph, \"Thomas Martin\",\"Andrea Ring\")\r\r\n\r\r\n\r\r\n\r\r\naverage.path.length(network_igraph)\r\r\n\r\r\n\r\r\n[1] 2.390464\r\r\n\r\r\naverage.path.length(network_igraph, directed = F)\r\r\n\r\r\n\r\r\n[1] 2.085787\r\r\n\r\r\nI took these vertex names, so I’m a bit confused why these are not showing up correctly.\r\r\n\r\r\nIdentifying Isolates\r\r\n\r\r\n\r\r\nigraph::components(network_igraph)\r\r\n\r\r\n\r\r\n$membership\r\r\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n [33] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n [65] 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n [97] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1\r\r\n[129] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n[161] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\r\n\r\r\n$csize\r\r\n[1] 182   1   1\r\r\n\r\r\n$no\r\r\n[1] 3\r\r\n\r\r\n#Isolates\r\r\nisolates(network_statnet)\r\r\n\r\r\n\r\r\n[1]  72 118\r\r\n\r\r\nas.vector(network_statnet %v% \"vertex.names\")[c(isolates(network_statnet))]\r\r\n\r\r\n\r\r\n[1]  72 118\r\r\n\r\r\nFor some reason it seems that Network statnet vertex.names is only showing numbers, not actually the names. I wonder if the statnet was set up incorrectly.\r\r\nThere are two isolates. \r\r\nDensity\r\r\n\r\r\n\r\r\ngraph.density(network_igraph)\r\r\n\r\r\n\r\r\n[1] 3.72443\r\r\n\r\r\nnetwork.density(network_statnet)\r\r\n\r\r\n\r\r\n[1] 0.08939178\r\r\n\r\r\ngraph.density(network_igraph, loops = TRUE)\r\r\n\r\r\n\r\r\n[1] 3.704188\r\r\n\r\r\ngden(network_statnet, diag = FALSE)\r\r\n\r\r\n\r\r\n[1] 0.08939178\r\r\n\r\r\nThe Igraph density is over 1, and the network density is around 9%. Very different densities.\r\r\n\r\r\nVertex Degrees\r\r\n\r\r\n\r\r\nigraph::degree(network_igraph) %>% head()\r\r\n\r\r\n\r\r\n[1] 114 428 391 104 957 381\r\r\n\r\r\nsna::degree(network_statnet) %>% head()\r\r\n\r\r\n\r\r\n[1] 10 32 21  9 59 30\r\r\n\r\r\nThere is a significant difference between degrees from the igraph dataset compared to the statnet data.\r\r\n\r\r\n\r\r\n\r\r\nnetwork_degree <- data.frame(Name = V(network_igraph)$Name,\r\r\n                             degree = igraph::degree(network_igraph, loops =FALSE))\r\r\nnetwork_degree %>% arrange(desc(degree)) %>% slice(1:10)\r\r\n\r\r\n\r\r\n              Name degree\r\r\n1    Jeff Dasovich  13967\r\r\n2    James Steffes   9404\r\r\n3       Tana Jones   9307\r\r\n4  Richard Shapiro   8994\r\r\n5               NA   6591\r\r\n6      Steven Kean   6384\r\r\n7    John Lavorato   6177\r\r\n8  Michael Grigsby   5860\r\r\n9      Mark Taylor   5693\r\r\n10  Louise Kitchen   5362\r\r\n\r\r\nJeff Dasovich has the highest number of degrees around 13K. He must be very high up in the company.\r\r\n\r\r\nDegree in Directed Networks\r\r\n\r\r\n\r\r\nsna::degree(network_statnet, cmode = \"indegree\")%>% head()\r\r\n\r\r\n\r\r\n[1]  4 21 10  6 30 17\r\r\n\r\r\nsna::degree(network_statnet, cmode = \"outdegree\") %>% head()\r\r\n\r\r\n\r\r\n[1]  6 11 11  3 29 13\r\r\n\r\r\nigraph::degree(network_igraph, mode =\"in\", loops = FALSE) %>% head()\r\r\n\r\r\n\r\r\n[1]  78 334 224  88 614 210\r\r\n\r\r\nigraph::degree(network_igraph, mode =\"out\", loops = FALSE)%>%head()\r\r\n\r\r\n\r\r\n[1]  36  92 167  16 325 169\r\r\n\r\r\nDegree_network <- data.frame(Name = V(network_igraph)$Name,\r\r\n           total_degrees = igraph::degree(network_igraph, loops = FALSE),\r\r\n           in_degree = igraph::degree(network_igraph, mode =\"in\", loops = FALSE),\r\r\n           out_degree = igraph::degree(network_igraph, mode =\"out\", loops = FALSE) ) %>% arrange(desc(total_degrees))\r\r\n\r\r\nDegree_network %>% slice(1:10)\r\r\n\r\r\n\r\r\n              Name total_degrees in_degree out_degree\r\r\n1    Jeff Dasovich         13967      2612      11355\r\r\n2    James Steffes          9404      4988       4416\r\r\n3       Tana Jones          9307      2268       7039\r\r\n4  Richard Shapiro          8994      6893       2101\r\r\n5               NA          6591      2698       3893\r\r\n6      Steven Kean          6384      2676       3708\r\r\n7    John Lavorato          6177      3352       2825\r\r\n8  Michael Grigsby          5860      1097       4763\r\r\n9      Mark Taylor          5693      3694       1999\r\r\n10  Louise Kitchen          5362      2087       3275\r\r\n\r\r\nAs expected from someone high up in the company. They would mostly have out degree connections, with a select few in degree connections.\r\r\n\r\r\nSummary Statistics\r\r\n\r\r\n\r\r\nsummary(Degree_network)\r\r\n\r\r\n\r\r\n     Name           total_degrees       in_degree     \r\r\n Length:184         Min.   :    0.0   Min.   :   0.0  \r\r\n Class :character   1st Qu.:  212.8   1st Qu.: 150.5  \r\r\n Mode  :character   Median :  512.5   Median : 314.0  \r\r\n                    Mean   : 1184.0   Mean   : 592.0  \r\r\n                    3rd Qu.: 1401.2   3rd Qu.: 655.2  \r\r\n                    Max.   :13967.0   Max.   :6893.0  \r\r\n   out_degree      \r\r\n Min.   :    0.00  \r\r\n 1st Qu.:   30.75  \r\r\n Median :  159.00  \r\r\n Mean   :  591.99  \r\r\n 3rd Qu.:  600.50  \r\r\n Max.   :11355.00  \r\r\n\r\r\n\r\r\nDegree Distribution\r\r\n\r\r\n\r\r\nhist(Degree_network$total_degrees, main = \"Enron Degree Distribution\", xlab = \"Degree\")\r\r\n\r\r\n\r\r\n\r\r\nhist(Degree_network$out_degree, main =\"Enron Out-Degree Distribution\", xlab = \"Degree\")\r\r\n\r\r\n\r\r\n\r\r\nhist(Degree_network$in_degree, main = \"Enron In-Degree Distribution\", xlab = \"Degree\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMost people in the company have limited number of degrees of connections, while their are a select few with many connections.\r\r\nNetwork Degree Centralization\r\r\n\r\r\n\r\r\n#centralization(network_statnet, degree, cmode= \"indegree\")\r\r\n#centralization(network_statnet, degree, cmode = \"outdegree\")\r\r\n\r\r\n\r\r\ncentr_degree(network_igraph, loops = FALSE, mode = \"in\")$centralization\r\r\n\r\r\n\r\r\n[1] 34.61991\r\r\n\r\r\ncentr_degree(network_igraph, loops = FALSE, mode = \"out\")$centralization\r\r\n\r\r\n\r\r\n[1] 59.13566\r\r\n\r\r\nThere is a higher centralization for out-degree nodes compared to in-degree nodes.\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/networks-hw-2/distill-preview.png",
    "last_modified": "2022-02-10T18:21:55-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/looking-into-alcohol-data-set/",
    "title": "Looking into Alcohol Data set",
    "description": "Trying to predict alcoholism based on a list of predictors",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-06",
    "categories": [],
    "contents": "\r\r\nIntroduction\r\r\nAlcohol abuse affects many different populations in the world, and it can ruin lives. Using the “Alcohol” data set from the Wooldridge package, I have created multiple models to help predict the likelihood of an individual abusing alcohol based on multiple predictors or covariates. The alcohol data set came with 33 variables, and 9822 observations. Here is a list of the 30 variables: abuse, status, unemrate, age, educ, married, famsize, white, exhealth, vghealth, goodhealth, fairhealth, northeast, midwest, south, centcity, outercity, qrt1, qrt2, qrt3, beertax, cigtax, ethanol, mothalc, fathalc, livealc, inwf, employ, agesq, beertaxsq, cigtaxsq, ethanolsq, educsq\r\r\nAt first glance I found it quite hard to decide on which variable to use in my model. Which variables have the strongest correlation to predicting whether someone will abuse alcohol? How do we choose? To start off, I choose variables for my models based on criteria that I believe would influence an individual to abuse alcohol. After I created those models, I determined how accurate those models were by using the R squared value for the LPM, and the AIC for the logit and Probit models. To create better models, I developed an automated process using the R^2 values from each variable in the model against the outcome variable (abuse). The process is outlined below. This paper has two objectives: 1. Create a best fit model that will help determine how likely an individual will abuse alcohol. 2. Create an automated process that identifies that the top N variables, and the best fit model to predict likelihood based on an outcome variable.\r\r\nMethod\r\r\nFor my initial models, I’ve decided that the variables that should influence the likelihood of an individual abusing alcohol are: status, age, education, fathalc, mothalc, beertax, and married. Some of these variables speak for them self. Fathalc and mothalc are used to determine whether the mother and fathers are alcoholics, 1 for yes and 0 for no. Status is used to identify if someone is out of the workforce, unemployed or employed using the 1, 2 and 3 respectively. Below are tables showing the distribution of each variable I have chosen.\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nage\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :25.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:31.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :38.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :39.18\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:46.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :59.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nbeertax\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :0.045\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:0.145\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :0.259\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :0.426\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:0.446\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :2.370\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\neduc\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. : 0.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:12.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :13.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :13.31\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:16.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :19.00\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nfathalc\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :0.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:0.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :0.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :0.1543\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:0.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :1.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nmarried\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :0.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:1.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :1.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :0.8164\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:1.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :1.0000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nstatus\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :1.000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:3.000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :3.000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :2.829\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:3.000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :3.000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nmothalc\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMin. :0.00000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n1st Qu.:0.00000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMedian :0.00000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMean :0.04042\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n3rd Qu.:0.00000\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nMax. :1.00000\r\r\n\r\r\n\r\r\n\r\r\nModels\r\r\nI’ve decided to use three types of models: LPM, Logit and Probit. I will create 5 nested models, and for ease of comparison, the LPM’s, Logits and Probits covariates will all match for each nested model. For example lpm1, logit1, and probit1 all use status, age, education, mother alcholic, and father alcoholic as covariates.\r\r\nBelow are the models used for LPM, Logit, and Probits.\r\r\n\r\r\n\r\r\n\r\r\n#LPM\r\r\nlpm1 <- lm(abuse ~ status+ age+ educ + fathalc, data = alcohol)\r\r\nlpm2 <-lm(abuse ~ status+ age+ educ+ fathalc +mothalc, data = alcohol)\r\r\nlpm3 <- lm(abuse ~ status+ age+ educ+ fathalc+ mothalc + beertax, \r\r\n           data = alcohol)\r\r\nlpm4 <- lm(abuse ~ status+ age+ educ+ fathalc+ mothalc + beertax+ married, \r\r\n           data = alcohol)\r\r\nlpm5 <- lm(abuse ~ status + age + educ + fathalc+ mothalc + beertax + married + \r\r\n             fathalc:mothalc, data = alcohol)\r\r\n\r\r\n# Logit\r\r\nlogit1 <- glm(abuse ~ status+ age+ educ+ fathalc, \r\r\n              family = binomial(link = logit), data = alcohol)\r\r\nlogit2 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc, \r\r\n              family = binomial(link = logit), data = alcohol)\r\r\nlogit3 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + beertax, \r\r\n              family = binomial(link = logit), data = alcohol)\r\r\nlogit4 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + beertax+ \r\r\n                married, family = binomial(link = logit), data = alcohol)\r\r\nlogit5 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + beertax+ \r\r\n                married+ fathalc:mothalc, \r\r\n              family = binomial(link = logit), data = alcohol)\r\r\n\r\r\n# Probit\r\r\nprobit1 <- glm(abuse ~ status+ age+ educ+ fathalc, \r\r\n               family = binomial(link = probit), data = alcohol)\r\r\nprobit2 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc , \r\r\n               family = binomial(link = probit), data = alcohol)\r\r\nprobit3 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + \r\r\n                 beertax, family = binomial(link = probit), data = alcohol)\r\r\nprobit4 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + \r\r\n        beertax+ married, family = binomial(link = probit), data = alcohol)\r\r\nprobit5 <- glm(abuse ~ status+ age+ educ+ fathalc+ mothalc + \r\r\n                            beertax+ married+ fathalc:mothalc, \r\r\n                family = binomial(link = probit), data = alcohol)\r\r\n\r\r\n\r\r\n\r\r\nInital Observations\r\r\nBelow are the beta coefficients for the LPM, Logit and Probit models:\r\r\n\r\r\n\r\r\n================================================================================\r\r\n                                              LPM's                             \r\r\n                 ---------------------------------------------------------------\r\r\n                 Model 1      Model 2      Model 3      Model 4      Model 5    \r\r\n--------------------------------------------------------------------------------\r\r\n(Intercept)         0.15 ***     0.15 ***     0.15 ***     0.16 ***     0.16 ***\r\r\n                   (0.03)       (0.03)       (0.03)       (0.03)       (0.03)   \r\r\nstatus             -0.01        -0.01        -0.01        -0.01        -0.01    \r\r\n                   (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nage                 0.00         0.00         0.00         0.00         0.00    \r\r\n                   (0.00)       (0.00)       (0.00)       (0.00)       (0.00)   \r\r\neduc               -0.00 **     -0.00 **     -0.00 **     -0.00 **     -0.00 ** \r\r\n                   (0.00)       (0.00)       (0.00)       (0.00)       (0.00)   \r\r\nfathalc             0.05 ***     0.05 ***     0.05 ***     0.05 ***     0.05 ***\r\r\n                   (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nmothalc                          0.04 **      0.04 **      0.05 **      0.04 *  \r\r\n                                (0.02)       (0.02)       (0.02)       (0.02)   \r\r\nbeertax                                      -0.01        -0.01        -0.01    \r\r\n                                             (0.01)       (0.01)       (0.01)   \r\r\nmarried                                                   -0.03 ***    -0.03 ***\r\r\n                                                          (0.01)       (0.01)   \r\r\nfathalc:mothalc                                                         0.01    \r\r\n                                                                       (0.03)   \r\r\n--------------------------------------------------------------------------------\r\r\nR^2                 0.01         0.01         0.01         0.01         0.01    \r\r\nAdj. R^2            0.01         0.01         0.01         0.01         0.01    \r\r\nNum. obs.        9822         9822         9822         9822         9822       \r\r\n================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n=====================================================================================\r\r\n                                                Logits                               \r\r\n                 --------------------------------------------------------------------\r\r\n                 Model 1       Model 2       Model 3       Model 4       Model 5     \r\r\n-------------------------------------------------------------------------------------\r\r\n(Intercept)         -0.99 ***     -1.01 ***     -0.99 ***     -0.96 ***     -0.96 ***\r\r\n                    (0.14)        (0.14)        (0.14)        (0.14)        (0.14)   \r\r\nstatus              -0.06         -0.06         -0.06         -0.04         -0.04    \r\r\n                    (0.03)        (0.03)        (0.03)        (0.03)        (0.03)   \r\r\nage                  0.00          0.00          0.00          0.00          0.00    \r\r\n                    (0.00)        (0.00)        (0.00)        (0.00)        (0.00)   \r\r\neduc                -0.02 **      -0.02 **      -0.02 **      -0.02 **      -0.02 ** \r\r\n                    (0.01)        (0.01)        (0.01)        (0.01)        (0.01)   \r\r\nfathalc              0.28 ***      0.26 ***      0.26 ***      0.26 ***      0.26 ***\r\r\n                    (0.04)        (0.04)        (0.04)        (0.04)        (0.05)   \r\r\nmothalc                            0.21 **       0.21 **       0.22 **       0.22 *  \r\r\n                                  (0.08)        (0.08)        (0.08)        (0.11)   \r\r\nbeertax                                         -0.03         -0.03         -0.03    \r\r\n                                                (0.04)        (0.04)        (0.04)   \r\r\nmarried                                                       -0.18 ***     -0.18 ***\r\r\n                                                              (0.05)        (0.05)   \r\r\nfathalc:mothalc                                                             -0.01    \r\r\n                                                                            (0.16)   \r\r\n-------------------------------------------------------------------------------------\r\r\nAIC               6306.43       6301.83       6303.10       6290.39       6292.39    \r\r\nBIC               6342.39       6344.98       6353.45       6347.93       6357.12    \r\r\nLog Likelihood   -3148.21      -3144.91      -3144.55      -3137.20      -3137.19    \r\r\nDeviance          6296.43       6289.83       6289.10       6274.39       6274.39    \r\r\nNum. obs.         9822          9822          9822          9822          9822       \r\r\n=====================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\n=====================================================================================\r\r\n                                                Probits                              \r\r\n                 --------------------------------------------------------------------\r\r\n                 Model 1       Model 2       Model 3       Model 4       Model 5     \r\r\n-------------------------------------------------------------------------------------\r\r\n(Intercept)         -1.64 ***     -1.67 ***     -1.63 ***     -1.57 ***     -1.57 ***\r\r\n                    (0.27)        (0.27)        (0.27)        (0.27)        (0.27)   \r\r\nstatus              -0.11         -0.10         -0.10         -0.08         -0.08    \r\r\n                    (0.06)        (0.06)        (0.06)        (0.06)        (0.06)   \r\r\nage                  0.00          0.00          0.00          0.01          0.01    \r\r\n                    (0.00)        (0.00)        (0.00)        (0.00)        (0.00)   \r\r\neduc                -0.03 **      -0.03 **      -0.03 **      -0.04 **      -0.04 ** \r\r\n                    (0.01)        (0.01)        (0.01)        (0.01)        (0.01)   \r\r\nfathalc              0.52 ***      0.49 ***      0.49 ***      0.49 ***      0.50 ***\r\r\n                    (0.08)        (0.08)        (0.08)        (0.08)        (0.09)   \r\r\nmothalc                            0.39 **       0.39 **       0.41 **       0.43 *  \r\r\n                                  (0.15)        (0.15)        (0.15)        (0.20)   \r\r\nbeertax                                         -0.07         -0.06         -0.06    \r\r\n                                                (0.08)        (0.08)        (0.08)   \r\r\nmarried                                                       -0.34 ***     -0.34 ***\r\r\n                                                              (0.09)        (0.09)   \r\r\nfathalc:mothalc                                                             -0.04    \r\r\n                                                                            (0.29)   \r\r\n-------------------------------------------------------------------------------------\r\r\nAIC               6306.83       6302.30       6303.56       6290.77       6292.76    \r\r\nBIC               6342.79       6345.46       6353.90       6348.31       6357.49    \r\r\nLog Likelihood   -3148.42      -3145.15      -3144.78      -3137.39      -3137.38    \r\r\nDeviance          6296.83       6290.30       6289.56       6274.77       6274.76    \r\r\nNum. obs.         9822          9822          9822          9822          9822       \r\r\n=====================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\nInitial Results\r\r\nIt seems like the R^2 value did not change no matter how many observations we included. We were only able to show about .01 variation of the model. The initial models are not explaining much variance in the model, so I am now going to attempt to automate the process and explain more variation in the model. We can’t really tell too much from the logit and probit models, but we can keep track of the AIC, for when we compare these results to the new models I will develop.\r\r\nVariables with High Correlation\r\r\nThe first step in the process to pick my variables for my linear models would be to identify the covariates with the highest R squared value when running a linear model against the outcome variable “abuse”. Below are the process and results:\r\r\n\r\r\n\r\r\ncolumns <- (colnames(alcohol))\r\r\n\r\r\noutcome <- \"abuse\"\r\r\nmodels <- lapply(paste(outcome,\" ~\", columns), as.formula)\r\r\ny <- NULL\r\r\n\r\r\n\r\r\nfor (model in models){\r\r\n  linearmodel <- lm(model, data = alcohol)\r\r\n  x <- summary(linearmodel)\r\r\n  print(paste(format(model), \"R^2 value: \", round(x$r.squared,3)*100, \"%\"))\r\r\n  y <- rbind(y, data.frame(variable = as.character(model[3]),\r\r\n              \"Rvalue_Percent\" = round(x$r.squared,3)*100))\r\r\n}\r\r\n\r\r\nnew_data <- y[order(-y$Rvalue_Percent),]\r\r\nnew_data %>% slice(1:8)\r\r\ntop_8 <- new_data$variable[1:8]\r\r\n\r\r\n\r\r\n\r\r\nThe code above was used to run 32 different linear models with the abuse as the outcome variable. I then created a data frame in a loop that extracted the R^2 as a percentage. I then organized the table from highest to lowest and grabbed the top 8 variables.  The top 8 variables are below:\r\r\n\r\r\n\r\r\nvariable\r\r\n\r\r\n\r\r\nRvalue_Percent\r\r\n\r\r\n\r\r\nfamsize\r\r\n\r\r\n\r\r\n0.5\r\r\n\r\r\n\r\r\nfathalc\r\r\n\r\r\n\r\r\n0.4\r\r\n\r\r\n\r\r\nlivealc\r\r\n\r\r\n\r\r\n0.3\r\r\n\r\r\n\r\r\nexhealth\r\r\n\r\r\n\r\r\n0.2\r\r\n\r\r\n\r\r\nethanol\r\r\n\r\r\n\r\r\n0.2\r\r\n\r\r\n\r\r\nethanolsq\r\r\n\r\r\n\r\r\n0.2\r\r\n\r\r\n\r\r\neducsq\r\r\n\r\r\n\r\r\n0.2\r\r\n\r\r\n\r\r\nstatus\r\r\n\r\r\n\r\r\n0.1\r\r\n\r\r\n\r\r\nThe R values above are in percentage form. It should be noted that it seems the correlation between abuse and these variables list above is quite low. The highest correlation is family size and that is only .5 %.\r\r\nCreating New Models\r\r\nNow that I’ve identified the covariates with the highest correlation, the next step is to create models for my LPM, Logit, and Probit. Instead of rewriting the code for each model, I have created a framework that can also be applied to other data sets. Building onto the code that was used above to identify the top 8 variables, I then created 8 variables based on the results from above.\r\r\nFor the LM function and the GLM function to run, I first needed to create 5 variables (x1-x5). These variables need to be in the formula format, which was created using the lapply function. I then simply created 5 more models for the LPMS, Logits and Probits below, using the 5 variables I created. See the process below:\r\r\n\r\r\n\r\r\noutcome_variable <- \"abuse\"\r\r\n\r\r\nvar1 <- top_8[1]\r\r\nvar2 <- top_8[2]\r\r\nvar3 <- top_8[3]\r\r\nvar4 <- top_8[4]\r\r\nvar5 <- top_8[5]\r\r\nvar6 <- top_8[6]\r\r\nvar7 <- top_8[7]\r\r\nvar8 <- top_8[8]\r\r\n\r\r\n\r\r\nx1 <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",var4)\r\r\n             , as.formula)\r\r\nx2 <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",var4,\r\r\n                   \"+\",var5), as.formula)\r\r\nx3 <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",var4\r\r\n                   ,\"+\",var5,\"+\",var6), as.formula)\r\r\nx4 <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",var4\r\r\n                   ,\"+\",var5,\"+\",var6,\"+\",var7), as.formula)\r\r\nx5 <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",var4\r\r\n                   ,\"+\",var5,\"+\",var6,\"+\",var7,\"+\",var8), as.formula)\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n#LPMS\r\r\nattach(alcohol)\r\r\nlm1 <- lm(x1[[1]])\r\r\nlm2 <- lm(x2[[1]])\r\r\nlm3 <- lm(x3[[1]])\r\r\nlm4 <- lm(x4[[1]])\r\r\nlm5 <- lm(x5[[1]])\r\r\n\r\r\n\r\r\n#Logits\r\r\n\r\r\nlogit1.1 <- glm(x1[[1]], family = binomial(link = logit))\r\r\nlogit1.2 <- glm(x2[[1]], family = binomial(link = logit))\r\r\nlogit1.3 <- glm(x3[[1]], family = binomial(link = logit))\r\r\nlogit1.4 <- glm(x4[[1]], family = binomial(link = logit))\r\r\nlogit1.5 <- glm(x5[[1]], family = binomial(link = logit))\r\r\n\r\r\n\r\r\n\r\r\n#Probits\r\r\n\r\r\nprobit1.1 <- glm(x1[[1]], family = binomial(link = probit))\r\r\nprobit1.2 <- glm(x2[[1]], family = binomial(link = probit))\r\r\nprobit1.3 <- glm(x3[[1]], family = binomial(link = probit))\r\r\nprobit1.4 <- glm(x4[[1]], family = binomial(link = probit))\r\r\nprobit1.5 <- glm(x5[[1]], family = binomial(link = probit))\r\r\n\r\r\ndetach(alcohol)\r\r\n\r\r\n\r\r\n\r\r\nBelow are the tables showing our new LPM, Logit and Probit models, using the new variables with the highest R squared values.\r\r\n\r\r\n\r\r\n============================================================================\r\r\n                                          LPM's                             \r\r\n             ---------------------------------------------------------------\r\r\n             Model 1      Model 2      Model 3      Model 4      Model 5    \r\r\n----------------------------------------------------------------------------\r\r\n(Intercept)     0.14 ***     0.07 ***     0.05         0.07         0.09    \r\r\n               (0.01)       (0.02)       (0.04)       (0.04)       (0.05)   \r\r\nfamsize        -0.01 ***    -0.01 ***    -0.01 ***    -0.01 ***    -0.01 ***\r\r\n               (0.00)       (0.00)       (0.00)       (0.00)       (0.00)   \r\r\nfathalc         0.04 ***     0.05 ***     0.05 ***     0.05 ***     0.05 ***\r\r\n               (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nlivealc         0.01         0.01         0.01         0.01         0.01    \r\r\n               (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nexhealth       -0.03 ***    -0.03 ***    -0.03 ***    -0.02 ***    -0.02 ***\r\r\n               (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nethanol                      0.03 ***     0.05         0.06         0.06    \r\r\n                            (0.01)       (0.04)       (0.04)       (0.04)   \r\r\nethanolsq                                -0.00        -0.01        -0.01    \r\r\n                                         (0.01)       (0.01)       (0.01)   \r\r\neducsq                                                -0.00 ***    -0.00 ***\r\r\n                                                      (0.00)       (0.00)   \r\r\nstatus                                                             -0.01    \r\r\n                                                                   (0.01)   \r\r\n----------------------------------------------------------------------------\r\r\nR^2             0.01         0.01         0.01         0.01         0.01    \r\r\nAdj. R^2        0.01         0.01         0.01         0.01         0.01    \r\r\nNum. obs.    9822         9822         9822         9822         9822       \r\r\n============================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\n====================================================================================\r\r\n                                               Logits                               \r\r\n                --------------------------------------------------------------------\r\r\n                Model 1       Model 2       Model 3       Model 4       Model 5     \r\r\n------------------------------------------------------------------------------------\r\r\n(Intercept)        -1.77 ***     -2.47 ***     -2.95 ***     -2.75 ***     -2.61 ***\r\r\n                   (0.08)        (0.20)        (0.51)        (0.51)        (0.53)   \r\r\nfamsize            -0.17 ***     -0.16 ***     -0.16 ***     -0.17 ***     -0.16 ***\r\r\n                   (0.02)        (0.02)        (0.02)        (0.02)        (0.02)   \r\r\nfathalc             0.43 **       0.45 **       0.44 **       0.44 **       0.44 ** \r\r\n                   (0.14)        (0.14)        (0.14)        (0.14)        (0.14)   \r\r\nlivealc             0.12          0.10          0.10          0.09          0.08    \r\r\n                   (0.13)        (0.13)        (0.13)        (0.13)        (0.13)   \r\r\nexhealth           -0.29 ***     -0.30 ***     -0.30 ***     -0.25 ***     -0.25 ***\r\r\n                   (0.07)        (0.07)        (0.07)        (0.07)        (0.07)   \r\r\nethanol                           0.34 ***      0.76          0.85 *        0.85 *  \r\r\n                                 (0.09)        (0.43)        (0.43)        (0.43)   \r\r\nethanolsq                                      -0.09         -0.11         -0.11    \r\r\n                                               (0.09)        (0.09)        (0.09)   \r\r\neducsq                                                       -0.00 ***     -0.00 ***\r\r\n                                                             (0.00)        (0.00)   \r\r\nstatus                                                                     -0.05    \r\r\n                                                                           (0.06)   \r\r\n------------------------------------------------------------------------------------\r\r\nAIC              6253.86       6240.92       6241.87       6230.89       6232.08    \r\r\nBIC              6289.82       6284.08       6292.22       6288.43       6296.81    \r\r\nLog Likelihood  -3121.93      -3114.46      -3113.94      -3107.45      -3107.04    \r\r\nDeviance         6243.86       6228.92       6227.87       6214.89       6214.08    \r\r\nNum. obs.        9822          9822          9822          9822          9822       \r\r\n====================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n====================================================================================\r\r\n                                               Probits                              \r\r\n                --------------------------------------------------------------------\r\r\n                Model 1       Model 2       Model 3       Model 4       Model 5     \r\r\n------------------------------------------------------------------------------------\r\r\n(Intercept)        -1.07 ***     -1.44 ***     -1.67 ***     -1.57 ***     -1.49 ***\r\r\n                   (0.04)        (0.10)        (0.26)        (0.26)        (0.28)   \r\r\nfamsize            -0.08 ***     -0.08 ***     -0.08 ***     -0.08 ***     -0.08 ***\r\r\n                   (0.01)        (0.01)        (0.01)        (0.01)        (0.01)   \r\r\nfathalc             0.22 **       0.23 **       0.23 **       0.23 **       0.23 ** \r\r\n                   (0.07)        (0.07)        (0.07)        (0.07)        (0.07)   \r\r\nlivealc             0.07          0.06          0.06          0.05          0.05    \r\r\n                   (0.07)        (0.07)        (0.07)        (0.07)        (0.07)   \r\r\nexhealth           -0.15 ***     -0.15 ***     -0.16 ***     -0.13 ***     -0.13 ***\r\r\n                   (0.04)        (0.04)        (0.04)        (0.04)        (0.04)   \r\r\nethanol                           0.18 ***      0.39          0.44 *        0.44 *  \r\r\n                                 (0.05)        (0.22)        (0.22)        (0.22)   \r\r\nethanolsq                                      -0.05         -0.06         -0.06    \r\r\n                                               (0.05)        (0.05)        (0.05)   \r\r\neducsq                                                       -0.00 ***     -0.00 ***\r\r\n                                                             (0.00)        (0.00)   \r\r\nstatus                                                                     -0.03    \r\r\n                                                                           (0.03)   \r\r\n------------------------------------------------------------------------------------\r\r\nAIC              6254.48       6241.18       6242.18       6231.09       6232.06    \r\r\nBIC              6290.44       6284.33       6292.53       6288.63       6296.79    \r\r\nLog Likelihood  -3122.24      -3114.59      -3114.09      -3107.55      -3107.03    \r\r\nDeviance         6244.48       6229.18       6228.18       6215.09       6214.06    \r\r\nNum. obs.        9822          9822          9822          9822          9822       \r\r\n====================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\nResults\r\r\nIt looks like the LPM R^2 values are still at .01. I believe this is due to the originally values being very small and .01 is the smallest value we can visualize on screen reg. After we identify the LPM model to use, I will extract the actual R^2 value and compare those to the actuals in the orginal LPM’s to see how much variance we reduced by using the new models. I will perform a similar method for the Logit and Probits, but with comparing the AIC and BIC values. Now lets identify the best fitting models.\r\r\n\r\r\nDetermining the best Fit models\r\r\nLPM’s\r\r\nTo identify the best fitting LPM model, I will use the LinearHypothesis function and compare the F values determine which models are the best fit, and whether they are statistically significant. To automate this process, I first created vectors with the corresponding variables in each model, which can be seen in m1 through m5. When using the Linear Hypothesis function, we also need to identify the difference in variables when comparing those models. For example, if we are comparing m5 to m1, then we would need the variables that model 5 and model 1 don’t share. We would need to identify the difference in variables for each model. To automate this process, I am using the function setdiff. Setdiff allows me to quickly identify the difference between vectors of strings. Once I identified the difference, I know can run the LinearHypothesis function. Below is the code used to perform these tasks and the results of each LinearHypothesis:\r\r\n\r\r\n\r\r\n#Hypothesis Tests\r\r\n\r\r\nm1 <- c(top_8[1:4])\r\r\nm2 <- c(top_8[1:5])\r\r\nm3 <- c(top_8[1:6])\r\r\nm4 <- c(top_8[1:7])\r\r\nm5 <- c(top_8[1:8])\r\r\n\r\r\n\r\r\nm5m1 <- setdiff(m5,m1)\r\r\nm5m2 <- setdiff(m5,m2)\r\r\nm5m3 <- setdiff(m5,m3)\r\r\nm5m4 <- setdiff(m5,m4)\r\r\nm4m3 <- setdiff(m4,m3)\r\r\n\r\r\n\r\r\nlinearHypothesis(lm5,m5m1)\r\r\n\r\r\n\r\r\nLinear hypothesis test\r\r\n\r\r\nHypothesis:\r\r\nethanol = 0\r\r\nethanolsq = 0\r\r\neducsq = 0\r\r\nstatus = 0\r\r\n\r\r\nModel 1: restricted model\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n\r\r\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)    \r\r\n1   9817 867.84                                 \r\r\n2   9813 865.21  4    2.6262 7.4463 5.52e-06 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlinearHypothesis(lm5,m5m2)\r\r\n\r\r\n\r\r\nLinear hypothesis test\r\r\n\r\r\nHypothesis:\r\r\nethanolsq = 0\r\r\neducsq = 0\r\r\nstatus = 0\r\r\n\r\r\nModel 1: restricted model\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n\r\r\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \r\r\n1   9816 866.44                                \r\r\n2   9813 865.21  3    1.2246 4.6296 0.003073 **\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlinearHypothesis(lm5,m5m3)\r\r\n\r\r\n\r\r\nLinear hypothesis test\r\r\n\r\r\nHypothesis:\r\r\neducsq = 0\r\r\nstatus = 0\r\r\n\r\r\nModel 1: restricted model\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n\r\r\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \r\r\n1   9815 866.42                                \r\r\n2   9813 865.21  2    1.2026 6.8196 0.001097 **\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlinearHypothesis(lm5,m5m4)\r\r\n\r\r\n\r\r\nLinear hypothesis test\r\r\n\r\r\nHypothesis:\r\r\nstatus = 0\r\r\n\r\r\nModel 1: restricted model\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n\r\r\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\r\r\n1   9814 865.32                           \r\r\n2   9813 865.21  1   0.10315 1.1699 0.2794\r\r\n\r\r\nlinearHypothesis(lm4,m4m3)\r\r\n\r\r\n\r\r\nLinear hypothesis test\r\r\n\r\r\nHypothesis:\r\r\neducsq = 0\r\r\n\r\r\nModel 1: restricted model\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq\r\r\n\r\r\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \r\r\n1   9815 866.42                                  \r\r\n2   9814 865.32  1    1.0994 12.469 0.0004156 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\n\r\r\nLPM Results\r\r\nAbove are the LinearHypothesis results from comparing the LPM models. From the Linear Hypthoesis tests, I have determined that model 4 is more parsimonious than model 5 (F = 1.17, p >.05). Now that I have identified the Model 4 as the LPM model, I will now perform a series of tests to determine if there is any non-linearity in the model and if we can trust the covariates in the model for predictions.\r\r\n\r\r\n\r\r\nTable 1: Vifs for Each Variable\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nfamsize\r\r\n\r\r\n\r\r\n1.010065\r\r\n\r\r\n\r\r\nfathalc\r\r\n\r\r\n\r\r\n2.493831\r\r\n\r\r\n\r\r\nlivealc\r\r\n\r\r\n\r\r\n2.501981\r\r\n\r\r\n\r\r\nexhealth\r\r\n\r\r\n\r\r\n1.042810\r\r\n\r\r\n\r\r\nethanol\r\r\n\r\r\n\r\r\n24.059350\r\r\n\r\r\n\r\r\nethanolsq\r\r\n\r\r\n\r\r\n24.032920\r\r\n\r\r\n\r\r\neducsq\r\r\n\r\r\n\r\r\n1.055115\r\r\n\r\r\n\r\r\nAfter running the vif test, I have identified that Ethanol and Ethanolsq both have vifs over 24, therefore we will drop them from the model. Now we will run the reset test in order to test for linearity in the model.\r\r\n\r\r\n\r\r\n    RESET test\r\r\n\r\r\ndata:  lm4\r\r\nRESET = 4.1787, df1 = 2, df2 = 9812, p-value = 0.01534\r\r\n\r\r\nThe p-value from the reset value test is below .05, therefor there is non-linearity in the model. Let’s try dropping the ethanol and ethanolsq covariates and check the linearity again.\r\r\n\r\r\n\r\r\nx4_new <- lapply(paste(outcome_variable, \" ~\", var1, \"+\", var2, \"+\", var3,\"+\",\r\r\n                       var4,\"+\",var7), as.formula)\r\r\n\r\r\nattach(alcohol)\r\r\nlm4_new <- lm(x4_new[[1]])\r\r\ndetach(alcohol)\r\r\n\r\r\nresettest(lm4_new)\r\r\n\r\r\n\r\r\n\r\r\n    RESET test\r\r\n\r\r\ndata:  lm4_new\r\r\nRESET = 3.1639, df1 = 2, df2 = 9814, p-value = 0.0423\r\r\n\r\r\nThe Pvalue is still below .05, signifying non-linearity. There is still non linearity, but the its closer to the .05 mark. Lets check the residuals and outliers for non linearity as well. \r\r\n\r\r\n\r\r\npredicted <- lm4_new$fitted.values\r\r\nresiduals <- lm4_new$residuals\r\r\nplot(predicted,residuals)+ abline(h = 0)\r\r\n\r\r\n\r\r\n\r\r\ninteger(0)\r\r\n\r\r\nThis looks like nonlinearity to me. Let’s check into the outliers using the rstudent function.\r\r\n\r\r\n\r\r\nTable 2: Outlier Distribution Min and Max\r\r\n\r\r\n\r\r\nmax\r\r\n\r\r\n\r\r\nmin\r\r\n\r\r\n\r\r\n-0.68\r\r\n\r\r\n\r\r\n3.36\r\r\n\r\r\n\r\r\nWhen looking at the histogram, there are outliers slightly to the right of 3. The table also confirms slight outliers with a max standard deviation of 3.36. The outlier is not too large, so I am not going to delete/omit any data.\r\r\nNow lets check the Homoskedasticity assumption using the BP test function.\r\r\n\r\r\n\r\r\n    studentized Breusch-Pagan test\r\r\n\r\r\ndata:  lm4_new\r\r\nBP = 115.15, df = 5, p-value < 2.2e-16\r\r\n\r\r\nThe Model is significant with a p-value < .05 and the model does violate the homoscedasticity assumption. We have heteroskedasticity, and we need to estimate the robust errors. Below are the errors using the coeftest function. \r\r\n\r\r\n\r\r\nt test of coefficients:\r\r\n\r\r\n               Estimate  Std. Error t value  Pr(>|t|)    \r\r\n(Intercept)  0.16323181  0.01031484 15.8249 < 2.2e-16 ***\r\r\nfamsize     -0.01408535  0.00197431 -7.1343 1.042e-12 ***\r\r\nfathalc      0.04453690  0.01309825  3.4002 0.0006760 ***\r\r\nlivealc      0.00959970  0.01212474  0.7917 0.4285287    \r\r\nexhealth    -0.02133789  0.00621045 -3.4358 0.0005932 ***\r\r\neducsq      -0.00013612  0.00004134 -3.2928 0.0009955 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nWe still have the current issue of non-linearity in our data. Through trial and error, I have identified that one of our variables needed to be transformed. To figure this out, I changed each variable, first squaring then logging them. With that variable transformed, I then used the reset test to check for linearity. After multiple trials I was able to identity that edusq needed to be logged.\r\r\n\r\r\n\r\r\nlm4_linear <- lm(abuse~ famsize + fathalc + livealc+ exhealth + I(log(educsq+1)), data = alcohol)\r\r\n\r\r\nresettest(lm4_linear)\r\r\n\r\r\n\r\r\n\r\r\n    RESET test\r\r\n\r\r\ndata:  lm4_linear\r\r\nRESET = 2.473, df1 = 2, df2 = 9814, p-value = 0.08438\r\r\n\r\r\nAfter running the reset test using the logged variable, we know have a p-value >.05, which means our model has linearity. Please see the results above.\r\r\nNext we will identify the Logit and Probit Models\r\r\nNow that we have identified the best fitting LPM Model, we know need to determine what is the best fitting model for the logit and probit models. In order to perform this task we will use the likelihood ratio tests.\r\r\n\r\r\n\r\r\nlrtest(logit1.1,logit1.2)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol\r\r\n  #Df  LogLik Df Chisq Pr(>Chisq)    \r\r\n1   5 -3121.9                        \r\r\n2   6 -3114.5  1 14.94   0.000111 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlrtest(logit1.2,logit1.3)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)\r\r\n1   6 -3114.5                     \r\r\n2   7 -3113.9  1 1.0528     0.3049\r\r\n\r\r\nlrtest(logit1.3, logit1.4)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \r\r\n1   7 -3113.9                         \r\r\n2   8 -3107.4  1 12.976  0.0003156 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlrtest(logit1.4,logit1.5)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)\r\r\n1   8 -3107.4                     \r\r\n2   9 -3107.0  1 0.8124     0.3674\r\r\n\r\r\nFor The logit models, 2 is a better fit than 3 with (chisq = 1.0528, pr(>chisq >.05)). Model 4 is a better fit than model 5 with (chisq =.8124, pr(>chisq >.05)). I will choose logit1.4 as the logit model.\r\r\n\r\r\n\r\r\nlrtest(probit1.1,probit1.2)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \r\r\n1   5 -3122.2                         \r\r\n2   6 -3114.6  1 15.299  9.177e-05 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlrtest(probit1.2,probit1.3)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)\r\r\n1   6 -3114.6                     \r\r\n2   7 -3114.1  1 0.9922     0.3192\r\r\n\r\r\nlrtest(probit1.3,probit1.4)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \r\r\n1   7 -3114.1                         \r\r\n2   8 -3107.6  1 13.091  0.0002966 ***\r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nlrtest(probit1.4,probit1.5)\r\r\n\r\r\n\r\r\nLikelihood ratio test\r\r\n\r\r\nModel 1: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq\r\r\nModel 2: abuse ~ famsize + fathalc + livealc + exhealth + ethanol + ethanolsq + \r\r\n    educsq + status\r\r\n  #Df  LogLik Df  Chisq Pr(>Chisq)\r\r\n1   8 -3107.6                     \r\r\n2   9 -3107.0  1 1.0317     0.3098\r\r\n\r\r\nThe probit models have similar results when compared to the logit models. For The probit models, 2 is a better fit than 3 (chisq = .992, pr(>chisq >.05)). Model 4 is a better fit than (chisq =1.0317, pr(>chisq >.05)). I will choose probit1.4 as the probit model.\r\r\nFixing up logit models\r\r\nNow that we have our logit and Probit models, logit1.4 and Probit1.4. We also need to check the vifs like did in the LPM models. Her are the results below:\r\r\n\r\r\n\r\r\nTable 3: Logit Vifs\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nfamsize\r\r\n\r\r\n\r\r\n1.009437\r\r\n\r\r\n\r\r\nfathalc\r\r\n\r\r\n\r\r\n2.776656\r\r\n\r\r\n\r\r\nlivealc\r\r\n\r\r\n\r\r\n2.782741\r\r\n\r\r\n\r\r\nexhealth\r\r\n\r\r\n\r\r\n1.038261\r\r\n\r\r\n\r\r\nethanol\r\r\n\r\r\n\r\r\n24.005089\r\r\n\r\r\n\r\r\nethanolsq\r\r\n\r\r\n\r\r\n23.982810\r\r\n\r\r\n\r\r\neducsq\r\r\n\r\r\n\r\r\n1.050736\r\r\n\r\r\n\r\r\nTable 3: Probit Vifs\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nfamsize\r\r\n\r\r\n\r\r\n1.009902\r\r\n\r\r\n\r\r\nfathalc\r\r\n\r\r\n\r\r\n2.669483\r\r\n\r\r\n\r\r\nlivealc\r\r\n\r\r\n\r\r\n2.676435\r\r\n\r\r\n\r\r\nexhealth\r\r\n\r\r\n\r\r\n1.039080\r\r\n\r\r\n\r\r\nethanol\r\r\n\r\r\n\r\r\n24.036773\r\r\n\r\r\n\r\r\nethanolsq\r\r\n\r\r\n\r\r\n24.012263\r\r\n\r\r\n\r\r\neducsq\r\r\n\r\r\n\r\r\n1.052182\r\r\n\r\r\n\r\r\nWe are seeing similar results for what we identified in the LPM model. We need to get rid of the ethanol and ethanolsq covariates from the logit and probit models. I will also include the logged variable from the LPM from our earlier research. \r\r\nCreating new logit and Probit Models\r\r\n\r\r\n\r\r\nlogit1.4_new <- glm(abuse~ famsize + fathalc+ livealc+exhealth+ I(log(educsq+1))\r\r\n                    ,family = binomial(link = logit), data = alcohol)\r\r\nprobit1.4_new <- glm(abuse~ famsize + fathalc+ livealc+exhealth+ I(log(educsq+1)\r\r\n                                                                   ),family = \r\r\n                       binomial(link = probit), data = alcohol)\r\r\n\r\r\n  resettest(logit1.4_new)\r\r\n\r\r\n\r\r\n\r\r\n    RESET test\r\r\n\r\r\ndata:  logit1.4_new\r\r\nRESET = 2.473, df1 = 2, df2 = 9814, p-value = 0.08438\r\r\n\r\r\n  resettest(probit1.4_new)\r\r\n\r\r\n\r\r\n\r\r\n    RESET test\r\r\n\r\r\ndata:  probit1.4_new\r\r\nRESET = 2.473, df1 = 2, df2 = 9814, p-value = 0.08438\r\r\n\r\r\nThe new logit and probit models have now been created. I also did a quick reset test on each one. Both p-values are greater than .05, we have linearity in each model. Before we start speaking to the LPM, Logit and probit models, let’s look into the difference in R squared values from the LPM models and the AIC from the new logit and probit models to the old models.\r\r\nLPM Comparisons\r\r\n\r\r\n\r\r\na1 <- summary(lpm1)\r\r\na2 <- summary(lpm2)\r\r\na3 <- summary(lpm3)\r\r\na4 <- summary(lpm4)\r\r\na5 <- summary(lpm5)\r\r\na6 <-summary(lm4_linear)\r\r\na1 <-a1$r.squared\r\r\na2 <-a2$r.squared\r\r\na3 <-a3$r.squared\r\r\na4 <-a4$r.squared\r\r\na5 <-a5$r.squared\r\r\na6 <-a6$r.squared\r\r\n\r\r\nstring <- c(\"lpm1\",\"lpm2\",\"lpm3\",\"lpm4\",\"lpm5\",\"Newest LPM\")\r\r\nstring2 <- round(c(a1,a2,a3,a4,a5,a6),5)\r\r\ntable <- cbind(string,string2)\r\r\nkable(table, caption = \"R squared value by LPM\", col.names = c(\"\",\"\"))\r\r\n\r\r\n\r\r\n\r\r\nTable 4: R squared value by LPM\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nlpm1\r\r\n\r\r\n\r\r\n0.00584\r\r\n\r\r\n\r\r\nlpm2\r\r\n\r\r\n\r\r\n0.00664\r\r\n\r\r\n\r\r\nlpm3\r\r\n\r\r\n\r\r\n0.00672\r\r\n\r\r\n\r\r\nlpm4\r\r\n\r\r\n\r\r\n0.00827\r\r\n\r\r\n\r\r\nlpm5\r\r\n\r\r\n\r\r\n0.00829\r\r\n\r\r\n\r\r\nNewest LPM\r\r\n\r\r\n\r\r\n0.01142\r\r\n\r\r\n\r\r\nResults\r\r\nWe can see from the table above that with our current LPM model, we were able to increase the variance explained from .00829 (lpm5, which had the highest R squared value) all the way to .01142. This may seem small, but when looking at it from a percentage change, we were able to increase our R squared value by 37%. The automated method was able to increase the variance explained and increased our R squared values.\r\r\n#logit and Probit Comparisons\r\r\n\r\r\n\r\r\nL_AIC1 <- logit1$aic\r\r\nL_AIC2 <- logit2$aic\r\r\nL_AIC3 <- logit3$aic\r\r\nL_AIC4 <- logit4$aic\r\r\nL_AIC5 <- logit5$aic\r\r\nP_AIC1 <- probit1$aic\r\r\nP_AIC2 <- probit2$aic\r\r\nP_AIC3 <- probit3$aic\r\r\nP_AIC4 <- probit4$aic\r\r\nP_AIC5 <- probit5$aic\r\r\n\r\r\nx <- round(c(L_AIC1, L_AIC2 ,L_AIC3 ,L_AIC4, L_AIC5 , P_AIC1 ,P_AIC2 ,P_AIC3 , P_AIC4 ,P_AIC5 ),2)\r\r\nz <- c(\"logit1\",\"Logit2\",\"Logit3\",\"logit4\",\"logit5\",\"Probit1\",\"Probit2\",\"Probit3\",\"Probit4\",\"Probit5\")\r\r\ny <- cbind(z,x)\r\r\nkable(y, caption = \"Logit and Probit AICS\", col.names = c(\"\",\"\"))\r\r\n\r\r\n\r\r\n\r\r\nTable 5: Logit and Probit AICS\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nlogit1\r\r\n\r\r\n\r\r\n6306.83\r\r\n\r\r\n\r\r\nLogit2\r\r\n\r\r\n\r\r\n6302.3\r\r\n\r\r\n\r\r\nLogit3\r\r\n\r\r\n\r\r\n6303.56\r\r\n\r\r\n\r\r\nlogit4\r\r\n\r\r\n\r\r\n6290.77\r\r\n\r\r\n\r\r\nlogit5\r\r\n\r\r\n\r\r\n6292.76\r\r\n\r\r\n\r\r\nProbit1\r\r\n\r\r\n\r\r\n6306.43\r\r\n\r\r\n\r\r\nProbit2\r\r\n\r\r\n\r\r\n6301.83\r\r\n\r\r\n\r\r\nProbit3\r\r\n\r\r\n\r\r\n6303.1\r\r\n\r\r\n\r\r\nProbit4\r\r\n\r\r\n\r\r\n6290.39\r\r\n\r\r\n\r\r\nProbit5\r\r\n\r\r\n\r\r\n6292.39\r\r\n\r\r\n\r\r\nL_new <- logit1.4_new$aic\r\r\nP_new <- probit1.4_new$aic\r\r\n\r\r\na <- round(c(L_new,P_new),2)\r\r\nb <- c(\"Logit Model\",\" Probit Model\")\r\r\nc <- cbind(b,a)\r\r\nkable(c, caption = \"New Logit and Probit AICS\", col.names = c(\"\",\"\"))\r\r\n\r\r\n\r\r\n\r\r\nTable 5: New Logit and Probit AICS\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nLogit Model\r\r\n\r\r\n\r\r\n6251.15\r\r\n\r\r\n\r\r\nProbit Model\r\r\n\r\r\n\r\r\n6251.58\r\r\n\r\r\n\r\r\nResults\r\r\nThe lowest AIC for the first logit and probit models were both in models 4. The logit model had an AIC of 6292.76 and the probit model had an AIC of 6290.39. The new models had lower AICs, 6251.15 for the logit model, and 6251.58 for the probit model. \r\r\nLooking at the Models\r\r\n\r\r\n\r\r\n================================================================================\r\r\n                                            LPM Models                          \r\r\n                 ---------------------------------------------------------------\r\r\n                 lm1          lm2          lm3          lm4          Lm4_linear \r\r\n--------------------------------------------------------------------------------\r\r\n(Intercept)         0.14 ***     0.07 ***     0.05         0.07         0.20 ***\r\r\n                   (0.01)       (0.02)       (0.04)       (0.04)       (0.03)   \r\r\nfamsize            -0.01 ***    -0.01 ***    -0.01 ***    -0.01 ***    -0.01 ***\r\r\n                   (0.00)       (0.00)       (0.00)       (0.00)       (0.00)   \r\r\nfathalc             0.04 ***     0.05 ***     0.05 ***     0.05 ***     0.04 ***\r\r\n                   (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nlivealc             0.01         0.01         0.01         0.01         0.01    \r\r\n                   (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nexhealth           -0.03 ***    -0.03 ***    -0.03 ***    -0.02 ***    -0.02 ***\r\r\n                   (0.01)       (0.01)       (0.01)       (0.01)       (0.01)   \r\r\nethanol                          0.03 ***     0.05         0.06                 \r\r\n                                (0.01)       (0.04)       (0.04)                \r\r\nethanolsq                                    -0.00        -0.01                 \r\r\n                                             (0.01)       (0.01)                \r\r\neducsq                                                    -0.00 ***             \r\r\n                                                          (0.00)                \r\r\nlog(educsq + 1)                                                        -0.01 *  \r\r\n                                                                       (0.01)   \r\r\n--------------------------------------------------------------------------------\r\r\nR^2                 0.01         0.01         0.01         0.01         0.01    \r\r\nAdj. R^2            0.01         0.01         0.01         0.01         0.01    \r\r\nNum. obs.        9822         9822         9822         9822         9822       \r\r\n================================================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nOrdered Logit of Alcohol Abuse (Odds Ratio)\r\r\n==============================================\r\r\n                       Dependent variable:    \r\r\n                   ---------------------------\r\r\n                              abuse           \r\r\n----------------------------------------------\r\r\nfamsize                     0.845***          \r\r\n                            p = 0.000         \r\r\n                                              \r\r\nfathalc                     1.539***          \r\r\n                            p = 0.002         \r\r\n                                              \r\r\nlivealc                       1.117           \r\r\n                            p = 0.402         \r\r\n                                              \r\r\nexhealth                    0.767***          \r\r\n                           p = 0.0003         \r\r\n                                              \r\r\nI(log(educsq + 1))           0.876**          \r\r\n                            p = 0.026         \r\r\n                                              \r\r\nConstant                    0.335***          \r\r\n                           p = 0.0005         \r\r\n                                              \r\r\n----------------------------------------------\r\r\nObservations                  9,822           \r\r\nLog Likelihood             -3,119.576         \r\r\nAkaike Inf. Crit.           6,251.151         \r\r\n==============================================\r\r\nNote:              *p<0.1; **p<0.05; ***p<0.01\r\r\n\r\r\nProbit models do not have odd’s ratios, so I will speak to the LPM and Logit models. When looking at the moving from Lm1 to lm2, the covariate fathalc increases from .04 to .05 when we include the covariate ethanol. When going from model 4 to 5, educsq increase from very small <-.00 to -.01, but it went from down in signficance to p<.05 from p<.001. When looking at lm4 compared to Lm4_linear, we saw a decrease in fathalc from .05 to .04, but the significance stayed the same. When looking at Lm4_linear, if your father is an alcoholic, that is associated with a 4% (p<.001) increase in abusing alcohol. Continuing to look at Lmr_linear, a one unit increase in famsize is associated with a 1% (p<.001) decrease in abusing alcohol.\r\r\nWhen looking at the Logit model. If one’s father is an alcholic, the likelihood of abusing alcohol versus not abusing alcohol is 1.539 times higher with p = .002. If one lives with an alcoholic, they are 1.117 more times likely to abuse alcohol. For A one-unit increase in famsize, the likelihood of abusing alcohol is .845 times as likely.\r\r\nFinally I will create synthetic individuals and make predictions of the likelihood of an individual to abuse alcohol based on the preset conditions. These can be seen below. \r\r\n\r\r\n\r\r\nx_values = list(famsize = c(1,8), fathalc = c(1,0), livealc = c(1,0),\r\r\n                exhealth = c(0,1), educsq = c(0, mean(alcohol$educsq)))\r\r\n\r\r\nlm_predict <- predict(lm4_linear,x_values)\r\r\nlogit_predict <- predict(logit1.4_new, x_values, type= \"response\")\r\r\n\r\r\n\r\r\nprobit_predict <- predict(probit1.4_new, x_values, type = \"response\")\r\r\n\r\r\npredictions <- round(cbind(lm_predict, logit_predict, probit_predict),4)*100\r\r\nkable(predictions, caption = \"Alcohol Abuse Probabilites\",\r\r\n      col.names = c(\"LPM\",\"Logit\",\"Probit\"),\r\r\n      row.names = TRUE)\r\r\n\r\r\n\r\r\n\r\r\nTable 6: Alcohol Abuse Probabilites\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nLPM\r\r\n\r\r\n\r\r\nLogit\r\r\n\r\r\n\r\r\nProbit\r\r\n\r\r\n\r\r\n1\r\r\n\r\r\n\r\r\n24.43\r\r\n\r\r\n\r\r\n32.72\r\r\n\r\r\n\r\r\n30.91\r\r\n\r\r\n\r\r\n2\r\r\n\r\r\n\r\r\n0.24\r\r\n\r\r\n\r\r\n3.24\r\r\n\r\r\n\r\r\n2.99\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nI made predictions for 2 different individuals. Individual 1 has a family size of 1, their father is an alcoholic, they live with an alcoholic, they are not in perfect health, and I used 0 as the educsq variable. For individual 2, I pretty much did the opposite from individual 1 except for the educsq covariate, in which I used the mean. From looking at the LPM and Logit model above, I would expect the probability for individual 1 when compared to individual 2 to be much higher.\r\r\nAs expected, individual 1 has a much higher probability of abusing alcohol, 24%, 33% and 30% for the LPM, Logit, and Probit models respectively. Individual 2 has a much lower probability of abusing alcohol, .24%, 3.2% and 3% for the LPM, Logit, and Probit models respectively.\r\r\nConclusion\r\r\nOverall, I found this project very useful. If I were to do this project over again, I would probably choose a different dataset, or use a different indicator as my outcome variable. When I saw the very low R squared value for my initial models, I thought I could increase that number by choosing the most the right covariates. I was able to increase the R squared value by 37% but it was still quite low.\r\r\nAutomating the linear models was quite difficult. The models would not run smoothly in my loops, there were many different issues that were quite time consuming. For example, to loop through a Linear Model and print the results, the variables in the LM function needed to be in formula state before running the loop, or you would have to do that in the loop. They needed to be pasted as a formula before looped through. I found the loops with regression, not very user friendly. I also found that it was not very easy to loop and print summaries from models, I believe this was due to the functionality of LM, Glm functions.\r\r\nEven though the model did not explain much variation in the data, I do believe I was able to pick the best variables to predict whether an individual would abuse alcohol. I was not able to fully automate the process of picking the best fit model. I did not have enough time to focus on this part, I was able to automate part of the process. I found it a bit difficult to extract the linearHypothesis data to use for automation, but I was able to make this process a bit easier when applying to other data sets.\r\r\nI have a lot to learn, and I look forward to continuing my study into linear regression.\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/looking-into-alcohol-data-set/distill-preview.png",
    "last_modified": "2022-02-06T20:18:48-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/crime-in-the-time-of-covid/",
    "title": "Crime in the Time of Covid",
    "description": "A closer look into the relationship between crime and covid.",
    "author": [
      {
        "name": "Peter Sullivan",
        "url": {}
      }
    ],
    "date": "2022-02-04",
    "categories": [],
    "contents": "\r\r\nIntroduction\r\r\nIn this hectic time of Covid 19 the world is changing, and its changing quickly. So quickly in fact that we barely have enough data to understand how these pandemic is really affecting our society. In this paper I will take a closer look into the effect of the covid Pandemic on Crime rates in the United States. I’ve obtained Crime Statistics from the Federal Bureau of Investigation (FBI), and Covid Statistics from CDC.\r\r\nUtilizing the FBI’s and the CDC’s publicly available data, I’m proposing that Crime rates in the are United States are inversely related to the current covid 19 Hype in the United States. In other words, the stronger the covid 19 hype in a particular state, the lower the expected crimes.\r\r\nMethod\r\r\nGather, Clean and Prep\r\r\nThe first Step is to read in the Data. I obtained the Arrests by State records for the United States for 2020 and 2019. After I read in the data, the next step is to clean the data in order to prepare for the analysis\r\r\nWhen pulling in the data, I noticed multiple issues. For example the column that included the State name was not showing the correct state for each row. The State names showed numbers when they should only have the name. The crime descriptor columns were written in ways that didn’t make sense. To fix these issues I used the following code below. Since the 2019 and 2020 csv sheets are in the same format, once I fixed one sheet, I just needed to repeat the same steps for the 2019 code.\r\r\n\r\r\n\r\r\n# 2020 Cleaning\r\r\nUSA_2020_crimes <- crime_2020 %>% fill(State)\r\r\nUSA_2020_crimes<-  USA_2020_crimes %>% mutate(States = removeNumbers(State))\r\r\nCrime_data <- rename(USA_2020_crimes,  \"Property Crimes\" = \"Property\\ncrime2\", \r\r\n\"Rape\" = \"Rape3\", \r\r\n\"Aggravated Assault\" = \"Aggravated\\nassault\",\r\r\n \"Drug and Abuse Violations\" = \"Drug \\nabuse\\nviolations\", \r\r\n\"Violent Crimes\" = \"Violent\\ncrime2\", \"Murder\" = \"Murder and\\nnonnegligent\\nmanslaughter\" )\r\r\nCrime_data$States[Crime_data$States == \"FLORIDA, \"]<- \"FLORIDA\"\r\r\n\r\r\n\r\r\n#2019 Cleaning\r\r\nUSA_2019_crimes <- crime_2019 %>% fill(State)\r\r\nUSA_2019_crimes<-  USA_2019_crimes %>% mutate(States = removeNumbers(State))\r\r\nCrime_data_2019 <- rename(USA_2019_crimes,  \"Property Crimes\" = \"Property\\ncrime2\", \r\r\n\"Rape\" = \"Rape3\",\r\r\n\"Aggravated Assault\" = \"Aggravated\\nassault\",\r\r\n\"Drug and Abuse Violations\" = \"Drug \\nabuse\\nviolations\",\r\r\n\"Violent Crimes\" = \"Violent\\ncrime2\", \"Murder\" = \"Murder and\\nnonnegligent\\nmanslaughter\" )\r\r\nCrime_data_2019$States[Crime_data_2019$States == \"FLORIDA, \" ] <- \"FLORIDA\"\r\r\n\r\r\n\r\r\n\r\r\nInitial Plots\r\r\nNow that the data is cleaned, I can now take initial glimpses into the data I’ve collected. To perform this I used the package “usmap”. The way this package works is that you use the fips_info from the usmap package to tell the plot “usmap” function which states, citys and regions you plan to map. For this project, I am focusing on all States. So before I can plot my crime data, I first need to join the fips_info with my crime data. That way the plot us map function knows which state to correctly link what ever values I plan on plotting. For this project I decided to focus on only a few crimes, Property crimes, Aggravated Assault, and Murder.\r\r\n\r\r\n\r\r\nlibrary(usmap)\r\r\nstate_info <- fips_info()\r\r\nstate_info <- state_info %>% mutate(\"States\" = toupper(full)) \r\r\ncrimes <- c(\"Property Crimes\", \"Aggravated Assault\",\"Murder\")\r\r\n\r\r\n# Plotting 2020 Crimes\r\r\nfor (crime in crimes){\r\r\n  x <-  Crime_data %>% filter(X2 ==\"Total all ages\") %>% select(States,crime) %>% \r\r\n    distinct(States, .keep_all = TRUE)\r\r\n  y <- full_join(x,state_info, by = \"States\")\r\r\n  z <- plot_usmap(data = y, values = crime, labels = TRUE, color = \"gray\")+ \r\r\nscale_fill_continuous(low = \"white\", high = \"blue\", name = paste(crime), \r\r\nlabel = scales::comma)+\r\r\n    labs(title = paste(crime,\"in the US\"), subtitle = \"2020 Crime DATA\")+\r\r\n    theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) +\r\r\ntheme(legend.position = \"right\")\r\r\n  print(z)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n# Plotting 2019 \r\r\nfor (crime in crimes){\r\r\n  x <-  Crime_data_2019 %>% filter(X2 ==\"Total all ages\") %>% select(States,crime) %>% distinct(States, .keep_all = TRUE)\r\r\n  y <- full_join(x,state_info, by = \"States\")\r\r\n  z <- plot_usmap(data = y, values = crime, labels = TRUE, color = \"gray\")+ scale_fill_continuous(low = \"white\", high = \"blue\", name = paste(crime), label = scales::comma)+\r\r\n    labs(title = paste(crime,\"in the US\"), subtitle = \"2019 Crime DATA\")+\r\r\n    theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n  print(z)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n2020 and 2019 Results\r\r\nIn all three figures for both 2020 and 2019, CA has the highest amount of crimes. There may be a lot of crime there, but this could also be due to the amount of population. Due to this issue, with out having the per-capita results, this data could be misleading. In order to really understand whats going on here I will need to dive deeper into the data.\r\r\nFuther Analysis\r\r\nTo truly understand what is happening, the total crimes themselves itself don’t matter. Instead I want to see the % change of crimes from 2019 to 2020. I will do this for all three crimes listed. Once I have the % change in crimes listed, then I can compare agains the covid data.\r\r\n\r\r\n\r\r\nCrime_data_2020_1 <- Crime_data %>% filter(X2 ==\"Total all ages\") %>% select(States,`Property Crimes`,`Aggravated Assault`,Murder) %>% distinct(States, .keep_all = TRUE)\r\r\n\r\r\nCrime_data_2019_1 <- Crime_data_2019 %>%  filter(X2 ==\"Total all ages\") %>% select(States,`Property Crimes`,`Aggravated Assault`,Murder) %>% distinct(States, .keep_all = TRUE) %>% rename(\"Property crime 2019\" = `Property Crimes`, \"Aggravated Assault 2019\"= `Aggravated Assault`, \"Murder 2019\" = Murder)\r\r\n\r\r\n# Join DATA and Create Percent change columns\r\r\n\r\r\nCrime_Change_data <- inner_join(Crime_data_2019_1,Crime_data_2020_1, by = \"States\")\r\r\nCrime_Change_data <- Crime_Change_data %>% mutate(\r\r\n  \"Property Crime % Change\" = (`Property Crimes` - `Property crime 2019`)/(`Property crime 2019`),\r\r\n  \"Aggravated Assualt % Change\" = (`Aggravated Assault` - `Aggravated Assault 2019`)/ (`Aggravated Assault 2019`),\r\r\n  \"Murder % Change\" = (Murder - `Murder 2019`)/`Murder 2019`\r\r\n)\r\r\n\r\r\n\r\r\n\r\r\nThe code block above shows how I combined the 2020 and 2019 data and then created a calculated column for each crime.\r\r\nPlot Percent Change of Crime DATA\r\r\n\r\r\n\r\r\ncrimes <- c(\"Property Crime % Change\", \"Aggravated Assualt % Change\",\"Murder % Change\")\r\r\n\r\r\n# Plotting 2020 Crimes\r\r\nfor (crime in crimes){\r\r\n  y <- full_join(Crime_Change_data,state_info, by = \"States\")\r\r\n  z <- plot_usmap(data = y, values = crime, labels = TRUE, color = \"gray\")+ \r\r\n     scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \r\r\n   midpoint = 0, space = \"Lab\", \r\r\n   name=\"% Change\")+\r\r\n    #scale_fill_continuous(low = \"yellow\", high = \"blue\", name = paste(crime), label = scales::comma)+\r\r\n    labs(title = paste(crime,\"in the US\"), subtitle = \"2020 Crime DATA\")+\r\r\n    theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n  print(z)\r\r\n}\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nThese results are much better then the previous maps above. As we can see California and Texas are no longer in the top states for any of the crimes.\r\r\nFor Property crimes we an increase in Georgia and West Virginia, and a decrease in Pennsylvania and Delaware. For Aggravated Assault, Pennsylvania was the lowest, with others showing little to no increase. It looks like Georgia and Alabama were the only states with an increase from 2019 to 2020 for aggravated assault. It looks like murder did increase country wide except for PA and NM.\r\r\nTo truly understand these changes, I will also give a table of the top and bottom states for each crimes % change.\r\r\n\r\r\n\r\r\nlibrary(knitr)\r\r\n\r\r\ncrimes <- c(\"Property Crime % Change\", \"Aggravated Assualt % Change\",\"Murder % Change\")\r\r\n\r\r\nCrime_Change_data %>% arrange(`Property Crime % Change`) %>% select(States,`Property Crime % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Decrease in Property Crime\")\r\r\n\r\r\n\r\r\nTable 1: Largest Decrease in Property Crime\r\r\nStates\r\r\nProperty Crime % Change\r\r\nPENNSYLVANIA\r\r\n-0.9700970\r\r\nMARYLAND\r\r\n-0.9150567\r\r\nALABAMA\r\r\n-0.6379310\r\r\nHAWAII\r\r\n-0.5644000\r\r\nMISSISSIPPI\r\r\n-0.5119228\r\r\n\r\r\nCrime_Change_data %>% arrange(desc(`Property Crime % Change`)) %>% select(States, `Property Crime % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Increase in Property Crime\")\r\r\n\r\r\n\r\r\nTable 1: Largest Increase in Property Crime\r\r\nStates\r\r\nProperty Crime % Change\r\r\nWEST VIRGINIA\r\r\n0.8412447\r\r\nGEORGIA\r\r\n0.5394141\r\r\nWYOMING\r\r\n0.0521376\r\r\nMISSOURI\r\r\n0.0196389\r\r\nNORTH CAROLINA\r\r\n-0.0105206\r\r\n\r\r\nCrime_Change_data %>% arrange(`Aggravated Assualt % Change`) %>% select(States,`Aggravated Assualt % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Decrease in Assault\")\r\r\n\r\r\n\r\r\nTable 1: Largest Decrease in Assault\r\r\nStates\r\r\nAggravated Assualt % Change\r\r\nPENNSYLVANIA\r\r\n-0.9767876\r\r\nMARYLAND\r\r\n-0.9482368\r\r\nHAWAII\r\r\n-0.4385965\r\r\nKENTUCKY\r\r\n-0.3846154\r\r\nMISSISSIPPI\r\r\n-0.3435028\r\r\n\r\r\nCrime_Change_data %>% arrange(desc(`Aggravated Assualt % Change`)) %>% select(States, `Aggravated Assualt % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Increase in Assault\")\r\r\n\r\r\n\r\r\nTable 1: Largest Increase in Assault\r\r\nStates\r\r\nAggravated Assualt % Change\r\r\nALABAMA\r\r\n6.0909091\r\r\nGEORGIA\r\r\n1.7259380\r\r\nSOUTH DAKOTA\r\r\n0.8287129\r\r\nNORTH DAKOTA\r\r\n0.4169381\r\r\nWEST VIRGINIA\r\r\n0.3105968\r\r\n\r\r\nCrime_Change_data %>% arrange(`Murder % Change`) %>% select(States,`Murder % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Decrease in Murder\")\r\r\n\r\r\n\r\r\nTable 1: Largest Decrease in Murder\r\r\nStates\r\r\nMurder % Change\r\r\nPENNSYLVANIA\r\r\n-0.9714286\r\r\nMARYLAND\r\r\n-0.9370079\r\r\nNEW HAMPSHIRE\r\r\n-0.8666667\r\r\nNEW MEXICO\r\r\n-0.5918367\r\r\nKENTUCKY\r\r\n-0.3229814\r\r\n\r\r\nCrime_Change_data %>% arrange(desc(`Murder % Change`)) %>% select(States, `Murder % Change`) %>% slice(1:5) %>% kable(caption = \"Largest Increase in Murder\")\r\r\n\r\r\n\r\r\nTable 1: Largest Increase in Murder\r\r\nStates\r\r\nMurder % Change\r\r\nDISTRICT OF COLUMBIA\r\r\nInf\r\r\nGEORGIA\r\r\n1.8857143\r\r\nALABAMA\r\r\n1.0000000\r\r\nWEST VIRGINIA\r\r\n0.8750000\r\r\nINDIANA\r\r\n0.8309859\r\r\n\r\r\nResults\r\r\nThe Tables above show the top 5 states per crime for largest increase in percentage and the top 5 states for the largest decrease in percentage. The largest decrease for all three crimes is from Pennsylvania and that was at 1%. The largest increase was for Alabama, which was an 6% increase in aggravated assault.\r\r\nMethod Continued\r\r\nNow that I have a good idea of What states saw the largest increases and decreases in crime. I will now correlate that over to Covid Data provided by CDC.\r\r\nCovid DATA\r\r\nWhen dealing with Covid, there are many different metrics that can be utilized. We could look at daily counts of infected, daily deaths, hospitalizations, or even total vaccinations. For this analysis I am only going to focus on the amount of deaths recorded in 2020 by state.\r\r\nThe plan is to compare the % change by the three crimes listed above by yearly total of deaths caused by Covid 19.\r\r\nCleaning and Prepping DATA\r\r\n\r\r\n\r\r\n#str(covid_data)\r\r\ncovid_data$`End Date` <- as.Date(covid_data$`End Date`,\"%m/%d/%Y\")\r\r\n#str(covid_data)\r\r\n#covid_data\r\r\nfiltered_covid <- covid_data %>% filter(`End Date`< \"2021-01-01\") %>% filter(`Place of Death` == \"Total - All Places of Death\" & State != \"United States\" & Group == \"By Year\") %>% mutate(\"States\" = toupper(State) ) %>% select(States, `COVID-19 Deaths`)\r\r\n\r\r\n\r\r\n\r\r\nI First needed to clean and prep the data. I changed the dates column from a character type to a date type and filtered out the unnecessary information. I then plotted the deaths and normalized that data across all states using the plot “usmap” function.\r\r\n\r\r\n\r\r\n covid_mapdata <- full_join(filtered_covid,state_info, by = \"States\")\r\r\ndata_map <- covid_mapdata %>% select(fips, `COVID-19 Deaths`) %>% rename(\"Deaths\" = `COVID-19 Deaths`)\r\r\n\r\r\ndata_map <- data.frame(data_map)\r\r\n\r\r\n\r\r\nplot_usmap(data = data_map, values = \"Deaths\",labels = TRUE, color = \"gray\")+\r\r\n  scale_fill_continuous(low = \"white\", high = \"blue\", name = \"Covid Fatalities\", label = scales::comma)+\r\r\n    labs(title = \"Covid Fatalitys in 2020\")+\r\r\n   theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nIt looks like California and Texas had the largest amount of deaths around 30K. This info can be misleading, since states with lower populations will most likely have lower deaths but those deaths could be a higher percentage of the total population. To rectify this I will bring in the 2020 population data from the US Census for 2020, and I will create a field that is the amount of deaths per 100K population.\r\r\n\r\r\n\r\r\npopulation_2020 <- readxl::read_xlsx(\"2020 population.xlsx\")\r\r\n\r\r\n\r\r\n\r\r\npopulation_2020$State <- toupper(population_2020$State)\r\r\npopulation_2020 <- rename(population_2020, \"States\" = State)\r\r\n\r\r\nCovid_data_updated <- inner_join(covid_mapdata, population_2020, by =\"States\")\r\r\nCovid_data_updated <- Covid_data_updated %>% mutate(\r\r\n  Per_capita = (`COVID-19 Deaths`/`2020 Census`)*100000\r\r\n)\r\r\n\r\r\n\r\r\nplot_usmap(data = Covid_data_updated, values = \"Per_capita\",labels = TRUE, color = \"gray\")+\r\r\n  scale_fill_continuous(low = \"white\", high = \"blue\", name = \"Covid Fatalities\", label = scales::comma)+\r\r\n    labs(title = \"Covid Fatalitys in 2020\")+\r\r\n   theme(panel.background = element_rect(color = \"black\", fill = \"lightblue\")) + theme(legend.position = \"right\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\nResults\r\r\nAs we can see, the chart looks much different than the previous one. It looks like North Dakota, South Dakota and Delaware had the highest fatalities per 100k population. Hawaii had the lowest. I will now create a data table to look at the top and bottom % per crime vs the top and bottom percentage per fatalities.\r\r\n\r\r\n\r\r\nTop_covid <- Covid_data_updated %>% arrange(desc(Per_capita)) %>% select(States,Per_capita) %>% slice(1:5)\r\r\n\r\r\nLow_covid <- Covid_data_updated %>% arrange((Per_capita)) %>% select(States,Per_capita) %>% slice(1:5)\r\r\n\r\r\n\r\r\nTop_covid %>% kable(caption = \"Highest Covid Fatalities by State\")\r\r\n\r\r\n\r\r\nTable 2: Highest Covid Fatalities by State\r\r\nStates\r\r\nPer_capita\r\r\nNEW JERSEY\r\r\n195.3172\r\r\nNORTH DAKOTA\r\r\n194.1999\r\r\nSOUTH DAKOTA\r\r\n193.4210\r\r\nRHODE ISLAND\r\r\n174.7801\r\r\nCONNECTICUT\r\r\n174.4619\r\r\n\r\r\nLow_covid %>% kable(caption = \"Lowest Covid Fatalities by State\")\r\r\n\r\r\n\r\r\nTable 2: Lowest Covid Fatalities by State\r\r\nStates\r\r\nPer_capita\r\r\nVERMONT\r\r\n22.54784\r\r\nHAWAII\r\r\n25.08124\r\r\nMAINE\r\r\n34.13197\r\r\nALASKA\r\r\n34.63364\r\r\nOREGON\r\r\n38.04349\r\r\n\r\r\nx1<- Crime_Change_data %>% arrange(`Property Crime % Change`) %>% select(States,`Property Crime % Change`) %>% slice(1:5) \r\r\n\r\r\ny_1<-Crime_Change_data %>% arrange(desc(`Property Crime % Change`)) %>% select(States, `Property Crime % Change`) %>% slice(1:5) \r\r\n\r\r\nx2<- Crime_Change_data %>% arrange(`Aggravated Assualt % Change`) %>% select(States,`Aggravated Assualt % Change`) %>% slice(1:5) \r\r\ny2<- Crime_Change_data %>% arrange(desc(`Aggravated Assualt % Change`)) %>% select(States, `Aggravated Assualt % Change`) %>% slice(1:5)\r\r\n\r\r\nx3<- Crime_Change_data %>% arrange(`Murder % Change`) %>% select(States,`Murder % Change`) %>% slice(1:5)\r\r\ny3<- Crime_Change_data %>% arrange(desc(`Murder % Change`)) %>% select(States, `Murder % Change`) %>% slice(1:5)\r\r\n\r\r\n\r\r\n\r\r\ncbind(Top_covid,x1,x2,x3) \r\r\n\r\r\n\r\r\n        States Per_capita       States Property Crime % Change\r\r\n1   NEW JERSEY   195.3172 PENNSYLVANIA              -0.9700970\r\r\n2 NORTH DAKOTA   194.1999     MARYLAND              -0.9150567\r\r\n3 SOUTH DAKOTA   193.4210      ALABAMA              -0.6379310\r\r\n4 RHODE ISLAND   174.7801       HAWAII              -0.5644000\r\r\n5  CONNECTICUT   174.4619  MISSISSIPPI              -0.5119228\r\r\n        States Aggravated Assualt % Change        States\r\r\n1 PENNSYLVANIA                  -0.9767876  PENNSYLVANIA\r\r\n2     MARYLAND                  -0.9482368      MARYLAND\r\r\n3       HAWAII                  -0.4385965 NEW HAMPSHIRE\r\r\n4     KENTUCKY                  -0.3846154    NEW MEXICO\r\r\n5  MISSISSIPPI                  -0.3435028      KENTUCKY\r\r\n  Murder % Change\r\r\n1      -0.9714286\r\r\n2      -0.9370079\r\r\n3      -0.8666667\r\r\n4      -0.5918367\r\r\n5      -0.3229814\r\r\n\r\r\ncbind(Low_covid,y_1,y2,y3)\r\r\n\r\r\n\r\r\n   States Per_capita         States Property Crime % Change\r\r\n1 VERMONT   22.54784  WEST VIRGINIA              0.84124473\r\r\n2  HAWAII   25.08124        GEORGIA              0.53941411\r\r\n3   MAINE   34.13197        WYOMING              0.05213764\r\r\n4  ALASKA   34.63364       MISSOURI              0.01963886\r\r\n5  OREGON   38.04349 NORTH CAROLINA             -0.01052062\r\r\n         States Aggravated Assualt % Change               States\r\r\n1       ALABAMA                   6.0909091 DISTRICT OF COLUMBIA\r\r\n2       GEORGIA                   1.7259380              GEORGIA\r\r\n3  SOUTH DAKOTA                   0.8287129              ALABAMA\r\r\n4  NORTH DAKOTA                   0.4169381        WEST VIRGINIA\r\r\n5 WEST VIRGINIA                   0.3105968              INDIANA\r\r\n  Murder % Change\r\r\n1             Inf\r\r\n2       1.8857143\r\r\n3       1.0000000\r\r\n4       0.8750000\r\r\n5       0.8309859\r\r\n\r\r\n\r\r\n\r\r\nlibrary(knitr)\r\r\n\r\r\nunemployment_data <- read_csv(\"unemployement_rates.csv\")\r\r\nunemployment_data$States = toupper(unemployment_data$States)\r\r\nunemployment_data <- unemployment_data %>% mutate(UnemploymentRate_change = (`unemployment rate 2020`- `unemployment rate 2019`)/`unemployment rate 2020`)\r\r\n\r\r\nunemployment_data %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 12\r\r\n  States     `Pop 2019` `pop 2020` `labor force 201~ `labor force 202~\r\r\n  <chr>           <dbl>      <dbl>             <dbl>             <dbl>\r\r\n1 UNITED ST~     259175     260329            163539            160742\r\r\n2 NORTHEAST       45145      45097             28598             28013\r\r\n3 NEW ENGLA~      12136      12162              8072              7841\r\r\n4 CONNECTIC~       2885       2883              1917              1873\r\r\n5 MAINE            1112       1118               696               677\r\r\n6 MASSACHUS~       5636       5648              3782              3658\r\r\n# ... with 7 more variables: employed 2019 <dbl>,\r\r\n#   employed 2020 <dbl>, Unemployed 2019 <dbl>,\r\r\n#   unemployed 2020 <dbl>, unemployment rate 2019 <dbl>,\r\r\n#   unemployment rate 2020 <dbl>, UnemploymentRate_change <dbl>\r\r\n\r\r\nCrime_Change_data %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 10\r\r\n  States     `Property crime 20~ `Aggravated Assault 20~ `Murder 2019`\r\r\n  <chr>                    <dbl>                   <dbl>         <dbl>\r\r\n1 ALABAMA                    522                      11             4\r\r\n2 ALASKA                    3006                    2077            46\r\r\n3 ARIZONA                  29328                    8967           294\r\r\n4 ARKANSAS                 13433                    4120           166\r\r\n5 CALIFORNIA               88854                   83584          1284\r\r\n6 COLORADO                 25261                    6226           152\r\r\n# ... with 6 more variables: Property Crimes <dbl>,\r\r\n#   Aggravated Assault <dbl>, Murder <dbl>,\r\r\n#   Property Crime % Change <dbl>, Aggravated Assualt % Change <dbl>,\r\r\n#   Murder % Change <dbl>\r\r\n\r\r\nCovid_data_updated %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 9\r\r\n  States    `COVID-19 Deaths` abbr  fips  full      Rank `2020 Census`\r\r\n  <chr>                 <dbl> <chr> <chr> <chr>    <dbl>         <dbl>\r\r\n1 ALABAMA                6706 AL    01    Alabama     24       5024279\r\r\n2 ALASKA                  254 AK    02    Alaska      48        733391\r\r\n3 ARIZONA                9321 AZ    04    Arizona     14       7151502\r\r\n4 ARKANSAS               4027 AR    05    Arkansas    33       3011524\r\r\n5 CALIFORN~             33524 CA    06    Califor~     1      39538223\r\r\n6 COLORADO               5073 CO    08    Colorado    21       5773714\r\r\n# ... with 2 more variables: Percent of Total <dbl>, Per_capita <dbl>\r\r\n\r\r\nTotal_DATA_1 <- inner_join(Crime_Change_data,unemployment_data, by = \"States\")\r\r\nTotal_DATA_2 <- inner_join(Total_DATA_1, Covid_data_updated, by = \"States\")\r\r\nTotal_DATA_2 %>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 29\r\r\n  States     `Property crime 20~ `Aggravated Assault 20~ `Murder 2019`\r\r\n  <chr>                    <dbl>                   <dbl>         <dbl>\r\r\n1 ALABAMA                    522                      11             4\r\r\n2 ALASKA                    3006                    2077            46\r\r\n3 ARIZONA                  29328                    8967           294\r\r\n4 ARKANSAS                 13433                    4120           166\r\r\n5 CALIFORNIA               88854                   83584          1284\r\r\n6 COLORADO                 25261                    6226           152\r\r\n# ... with 25 more variables: Property Crimes <dbl>,\r\r\n#   Aggravated Assault <dbl>, Murder <dbl>,\r\r\n#   Property Crime % Change <dbl>, Aggravated Assualt % Change <dbl>,\r\r\n#   Murder % Change <dbl>, Pop 2019 <dbl>, pop 2020 <dbl>,\r\r\n#   labor force 2019 <dbl>, labor force 2020 <dbl>,\r\r\n#   employed 2019 <dbl>, employed 2020 <dbl>, Unemployed 2019 <dbl>,\r\r\n#   unemployed 2020 <dbl>, unemployment rate 2019 <dbl>,\r\r\n#   unemployment rate 2020 <dbl>, UnemploymentRate_change <dbl>,\r\r\n#   COVID-19 Deaths <dbl>, abbr <chr>, fips <chr>, full <chr>,\r\r\n#   Rank <dbl>, 2020 Census <dbl>, Percent of Total <dbl>,\r\r\n#   Per_capita <dbl>\r\r\n\r\r\ncorr_data<- Total_DATA_2 %>% select(`Property Crime % Change`:`Murder % Change`,UnemploymentRate_change,`COVID-19 Deaths`,Per_capita)\r\r\ncorr_data <- corr_data %>% rename(\"COVID 19 Deaths Per Capita\" = Per_capita)\r\r\ncor(corr_data) %>% round(3)%>%kable()\r\r\n\r\r\n\r\r\n\r\r\nProperty Crime % Change\r\r\nAggravated Assualt % Change\r\r\nMurder % Change\r\r\nUnemploymentRate_change\r\r\nCOVID-19 Deaths\r\r\nCOVID 19 Deaths Per Capita\r\r\nProperty Crime % Change\r\r\n1.000\r\r\n0.098\r\r\n0.651\r\r\n-0.148\r\r\n-0.112\r\r\n-0.112\r\r\nAggravated Assualt % Change\r\r\n0.098\r\r\n1.000\r\r\n0.552\r\r\n-0.069\r\r\n-0.046\r\r\n0.103\r\r\nMurder % Change\r\r\n0.651\r\r\n0.552\r\r\n1.000\r\r\n-0.030\r\r\n-0.002\r\r\n0.056\r\r\nUnemploymentRate_change\r\r\n-0.148\r\r\n-0.069\r\r\n-0.030\r\r\n1.000\r\r\n0.262\r\r\n-0.114\r\r\nCOVID-19 Deaths\r\r\n-0.112\r\r\n-0.046\r\r\n-0.002\r\r\n0.262\r\r\n1.000\r\r\n0.192\r\r\nCOVID 19 Deaths Per Capita\r\r\n-0.112\r\r\n0.103\r\r\n0.056\r\r\n-0.114\r\r\n0.192\r\r\n1.000\r\r\n\r\r\n\r\r\n\r\r\nlibrary(reshape2)\r\r\n# Create a heatmap for cor matrix\r\r\ncorr_matrix <- cor(corr_data)\r\r\nmelted <- melt(corr_matrix)\r\r\nmelted %>% head()\r\r\n\r\r\n\r\r\n                         Var1                    Var2       value\r\r\n1     Property Crime % Change Property Crime % Change  1.00000000\r\r\n2 Aggravated Assualt % Change Property Crime % Change  0.09833195\r\r\n3             Murder % Change Property Crime % Change  0.65074643\r\r\n4     UnemploymentRate_change Property Crime % Change -0.14820711\r\r\n5             COVID-19 Deaths Property Crime % Change -0.11166874\r\r\n6  COVID 19 Deaths Per Capita Property Crime % Change -0.11235439\r\r\n\r\r\nmelted$Var1 <- as.character(melted$Var1)\r\r\nmelted$Var2 <- as.character(melted$Var2)\r\r\n\r\r\n\r\r\nggplot(data = melted, aes(Var2, Var1, fill = value))+\r\r\n geom_tile(color = \"white\")+\r\r\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \r\r\n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \r\r\n   name=\"Corr Matrix\") +\r\r\n  theme_minimal()+ \r\r\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \r\r\n    size = 12, hjust = 1))+\r\r\n coord_fixed()+\r\r\n  xlab(\"\")+\r\r\n  ylab(\"\")\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n\r\r\n# Corr Matrix''''\r\r\ncorr_data%>% head()\r\r\n\r\r\n\r\r\n# A tibble: 6 x 6\r\r\n  `Property Crime ~ `Aggravated Ass~ `Murder % Chang~ UnemploymentRat~\r\r\n              <dbl>            <dbl>            <dbl>            <dbl>\r\r\n1           -0.638           6.09               1                0.492\r\r\n2           -0.203           0.0197            -0.283            0.308\r\r\n3           -0.0922          0.0181             0.156            0.380\r\r\n4           -0.187           0.122              0.175            0.426\r\r\n5           -0.179          -0.00353            0.241            0.584\r\r\n6           -0.151           0.0895             0.553            0.630\r\r\n# ... with 2 more variables: COVID-19 Deaths <dbl>,\r\r\n#   COVID 19 Deaths Per Capita <dbl>\r\r\n\r\r\n pivot_data <- corr_data %>% pivot_longer(!c(`COVID-19 Deaths`,`COVID 19 Deaths Per Capita`),\r\r\n                                          \r\r\n    names_to = \"Factor\", \r\r\n    values_to = \"Percent_Change\",\r\r\n  )\r\r\n\r\r\n pivot_data\r\r\n\r\r\n\r\r\n# A tibble: 200 x 4\r\r\n   `COVID-19 Deaths` `COVID 19 Deaths Pe~ Factor        Percent_Change\r\r\n               <dbl>                <dbl> <chr>                  <dbl>\r\r\n 1              6706                133.  Property Cri~        -0.638 \r\r\n 2              6706                133.  Aggravated A~         6.09  \r\r\n 3              6706                133.  Murder % Cha~         1     \r\r\n 4              6706                133.  Unemployment~         0.492 \r\r\n 5               254                 34.6 Property Cri~        -0.203 \r\r\n 6               254                 34.6 Aggravated A~         0.0197\r\r\n 7               254                 34.6 Murder % Cha~        -0.283 \r\r\n 8               254                 34.6 Unemployment~         0.308 \r\r\n 9              9321                130.  Property Cri~        -0.0922\r\r\n10              9321                130.  Aggravated A~         0.0181\r\r\n# ... with 190 more rows\r\r\n\r\r\n pivot_data %>% ggplot()+\r\r\n   geom_point(aes(x = `COVID-19 Deaths`, y = Percent_Change ), color = \"deepskyblue2\")+geom_smooth(aes(x = `COVID-19 Deaths`, y = Percent_Change), se = F)+\r\r\n   facet_wrap(~Factor, scales = \"free\")\r\r\n\r\r\n\r\r\n\r\r\n  pivot_data\r\r\n\r\r\n\r\r\n# A tibble: 200 x 4\r\r\n   `COVID-19 Deaths` `COVID 19 Deaths Pe~ Factor        Percent_Change\r\r\n               <dbl>                <dbl> <chr>                  <dbl>\r\r\n 1              6706                133.  Property Cri~        -0.638 \r\r\n 2              6706                133.  Aggravated A~         6.09  \r\r\n 3              6706                133.  Murder % Cha~         1     \r\r\n 4              6706                133.  Unemployment~         0.492 \r\r\n 5               254                 34.6 Property Cri~        -0.203 \r\r\n 6               254                 34.6 Aggravated A~         0.0197\r\r\n 7               254                 34.6 Murder % Cha~        -0.283 \r\r\n 8               254                 34.6 Unemployment~         0.308 \r\r\n 9              9321                130.  Property Cri~        -0.0922\r\r\n10              9321                130.  Aggravated A~         0.0181\r\r\n# ... with 190 more rows\r\r\n\r\r\n pivot_data %>% ggplot()+\r\r\n   geom_point(aes(x = `COVID 19 Deaths Per Capita`, y = Percent_Change ), color = \"deepskyblue2\")+geom_smooth(aes(x = `COVID 19 Deaths Per Capita`, y = Percent_Change), se = F)+\r\r\n   facet_wrap(~Factor, scales = \"free\")\r\r\n\r\r\n\r\r\n\r\r\n pivot_data \r\r\n\r\r\n\r\r\n# A tibble: 200 x 4\r\r\n   `COVID-19 Deaths` `COVID 19 Deaths Pe~ Factor        Percent_Change\r\r\n               <dbl>                <dbl> <chr>                  <dbl>\r\r\n 1              6706                133.  Property Cri~        -0.638 \r\r\n 2              6706                133.  Aggravated A~         6.09  \r\r\n 3              6706                133.  Murder % Cha~         1     \r\r\n 4              6706                133.  Unemployment~         0.492 \r\r\n 5               254                 34.6 Property Cri~        -0.203 \r\r\n 6               254                 34.6 Aggravated A~         0.0197\r\r\n 7               254                 34.6 Murder % Cha~        -0.283 \r\r\n 8               254                 34.6 Unemployment~         0.308 \r\r\n 9              9321                130.  Property Cri~        -0.0922\r\r\n10              9321                130.  Aggravated A~         0.0181\r\r\n# ... with 190 more rows\r\r\n\r\r\n\r\r\n\r\r\nlibrary(texreg); library(lmtest)\r\r\n\r\r\n# Run Regression Analysis\r\r\n\r\r\n# Outcome Variable - Crime\r\r\n# Assualt\r\r\n\r\r\nlpm_assault <- lm(`Aggravated Assualt % Change`  ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita` +\r\r\n                    UnemploymentRate_change , data = corr_data)\r\r\n\r\r\n\r\r\n\r\r\nlpm_Property_crime <- lm(`Property Crime % Change` ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita`\r\r\n                         +UnemploymentRate_change, data = corr_data)\r\r\n\r\r\n\r\r\nlpm_Murder<- lm(`Murder % Change` ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita` \r\r\n                +UnemploymentRate_change, data = corr_data)\r\r\n\r\r\n\r\r\n\r\r\nscreenreg(list(lpm_assault,lpm_Property_crime,lpm_Murder), custom.header = list(\"Crime LPM's\" = 1:3),custom.model.names = c(\"Assault\",\"Murder\",\"Property\"))\r\r\n\r\r\n\r\r\n\r\r\n=======================================================\r\r\n                                     Crime LPM's       \r\r\n                              -------------------------\r\r\n                              Assault  Murder  Property\r\r\n-------------------------------------------------------\r\r\n(Intercept)                    0.10     0.08    0.14   \r\r\n                              (0.83)   (0.24)  (0.43)  \r\r\n`COVID-19 Deaths`             -0.00    -0.00   -0.00   \r\r\n                              (0.00)   (0.00)  (0.00)  \r\r\n`COVID 19 Deaths Per Capita`   0.00    -0.00    0.00   \r\r\n                              (0.00)   (0.00)  (0.00)  \r\r\nUnemploymentRate_change       -0.39    -0.40   -0.10   \r\r\n                              (1.41)   (0.41)  (0.73)  \r\r\n-------------------------------------------------------\r\r\nR^2                            0.02     0.04    0.00   \r\r\nAdj. R^2                      -0.05    -0.02   -0.06   \r\r\nNum. obs.                     50       50      50      \r\r\n=======================================================\r\r\n*** p < 0.001; ** p < 0.01; * p < 0.05\r\r\n\r\r\nsummary(lpm_assault)\r\r\n\r\r\n\r\r\n\r\r\nCall:\r\r\nlm(formula = `Aggravated Assualt % Change` ~ `COVID-19 Deaths` + \r\r\n    `COVID 19 Deaths Per Capita` + UnemploymentRate_change, data = corr_data)\r\r\n\r\r\nResiduals:\r\r\n    Min      1Q  Median      3Q     Max \r\r\n-1.0910 -0.2011 -0.0874  0.0075  5.9100 \r\r\n\r\r\nCoefficients:\r\r\n                               Estimate Std. Error t value Pr(>|t|)\r\r\n(Intercept)                   1.020e-01  8.286e-01   0.123    0.903\r\r\n`COVID-19 Deaths`            -6.872e-06  1.926e-05  -0.357    0.723\r\r\n`COVID 19 Deaths Per Capita`  2.367e-03  3.281e-03   0.721    0.474\r\r\nUnemploymentRate_change      -3.885e-01  1.414e+00  -0.275    0.785\r\r\n\r\r\nResidual standard error: 0.962 on 46 degrees of freedom\r\r\nMultiple R-squared:  0.01673,   Adjusted R-squared:  -0.04739 \r\r\nF-statistic: 0.2609 on 3 and 46 DF,  p-value: 0.8531\r\r\n\r\r\n\r\r\n\r\r\n# Assualt\r\r\nLpm_unemployment <- lm(UnemploymentRate_change  ~ `COVID-19 Deaths`+ `COVID 19 Deaths Per Capita` + `Property Crime % Change`+`Murder % Change`+`Aggravated Assualt % Change` , data = corr_data)\r\r\n\r\r\nsummary(Lpm_unemployment)\r\r\n\r\r\n\r\r\n\r\r\nCall:\r\r\nlm(formula = UnemploymentRate_change ~ `COVID-19 Deaths` + `COVID 19 Deaths Per Capita` + \r\r\n    `Property Crime % Change` + `Murder % Change` + `Aggravated Assualt % Change`, \r\r\n    data = corr_data)\r\r\n\r\r\nResiduals:\r\r\n     Min       1Q   Median       3Q      Max \r\r\n-0.22078 -0.05388 -0.01327  0.05639  0.22991 \r\r\n\r\r\nCoefficients:\r\r\n                                Estimate Std. Error t value Pr(>|t|)\r\r\n(Intercept)                    5.005e-01  4.224e-02  11.849 2.78e-15\r\r\n`COVID-19 Deaths`              3.508e-06  1.949e-06   1.800   0.0787\r\r\n`COVID 19 Deaths Per Capita`  -4.620e-04  3.426e-04  -1.349   0.1843\r\r\n`Property Crime % Change`     -1.083e-01  7.765e-02  -1.395   0.1699\r\r\n`Murder % Change`              5.317e-02  5.187e-02   1.025   0.3109\r\r\n`Aggravated Assualt % Change` -1.603e-02  2.021e-02  -0.793   0.4318\r\r\n                                 \r\r\n(Intercept)                   ***\r\r\n`COVID-19 Deaths`             .  \r\r\n`COVID 19 Deaths Per Capita`     \r\r\n`Property Crime % Change`        \r\r\n`Murder % Change`                \r\r\n`Aggravated Assualt % Change`    \r\r\n---\r\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\r\n\r\r\nResidual standard error: 0.1003 on 44 degrees of freedom\r\r\nMultiple R-squared:  0.1363,    Adjusted R-squared:  0.03815 \r\r\nF-statistic: 1.389 on 5 and 44 DF,  p-value: 0.247\r\r\n\r\r\nResults\r\r\nFor this project, I decided to just focus on property crimes, murder, and aggravated assault as the crimes of interest in this analysis. When property crimes, most of the states saw a decrease in property crime in 2020 when compared to 2021. Pennsylvania saw the largest decrease in property crime while Georgia and West Virginia saw the largest increases in property crime. It appears that most of the states saw an increase in murders from 2019 to 2020, with Georgia seeing the largest increase. There was very little percent change in either direction across all states for aggravated assault, except for Alabama, where we saw a 6% increase in aggravated assault. The correlation Matrix shows a high correlation between murder percent change and aggravated assault percent change. It also looks like murder has a high correlation with property crime percent change. COVID-19 deaths shows a negative correlation with property crimes and a positive correlation with unemployment rate. COVID-19 deaths per-capita, surprisingly, has a negative correlation with unemployment rate. I created three unrestricted linear regression models, one for each crime. When looking at Figure 8, the murder linear model explained the largest amount of variance at 4%. None of the models had any statistically significant covariates.\r\r\nConclusion\r\r\nCrime rates in the United States are inversely proportional to the number of COVID-19 deaths for the crimes analyzed in this project. When deaths per capita were introduced, COVID-19 deaths per capita were proportional to assault and property crimes but inversely related to murder. Surprisingly, unemployment rates seem to be inversely proportional to crime rates.\r\r\nIt should be noted that none of the models have any covariates that are statistically significant (all p-values were >.05). For future research, I would recommend creating a larger sample size and looking into new crimes. It would also be beneficial to create smaller groups by states and regions and incorporating local policies such as stay-at-home orders.\r\r\nReferences\r\r\nCrime, https://crime-data-explorer.fr.cloud.gov/pages/home. “Table 1. Employment Status of the Civilian Noninstitutional Population 16 Years of Age and over by Region, Division, and State, 2019-20 Annual Averages.” U.S. Bureau of Labor Statistics, U.S. Bureau of Labor Statistics, 3 Mar. 2021, https://www.bls.gov/news.release/srgune.t01.htm. “Provisional COVID-19 Deaths by Place of Death and State.” Centers for Disease Control and Prevention, Centers for Disease Control and Prevention, https://data.cdc.gov/NCHS/Provisional-COVID-19-Deaths-by-Place-of-Death-and-/uggs-hy5q.\r\r\n\r\r\n\r\r\n\r\r\n",
    "preview": "posts/crime-in-the-time-of-covid/distill-preview.png",
    "last_modified": "2022-02-04T10:10:34-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-03-networkanalysishw1/",
    "title": "NetworkAnalysisHw1",
    "description": "A closer Look into Airport Data.",
    "author": [
      {
        "name": "Peter",
        "url": {}
      }
    ],
    "date": "2022-02-03",
    "categories": [],
    "contents": "\r\n\r\nLoad in DATA. This is the Airport DATA from the Google drive.\r\n\r\nLooking at Nodes and Edges:\r\n\r\n\r\nls()\r\n\r\n\r\n[1] \"network_edgelist\" \"network_igraph\"   \"network_nodes\"   \r\n[4] \"network_statnet\" \r\n\r\nvcount(network_igraph)\r\n\r\n\r\n[1] 755\r\n\r\necount(network_igraph)\r\n\r\n\r\n[1] 23473\r\n\r\nprint(network_statnet)\r\n\r\n\r\n Network attributes:\r\n  vertices = 755 \r\n  directed = TRUE \r\n  hyper = FALSE \r\n  loops = FALSE \r\n  multiple = FALSE \r\n  bipartite = FALSE \r\n  total edges= 8228 \r\n    missing edges= 0 \r\n    non-missing edges= 8228 \r\n\r\n Vertex attribute names: \r\n    City Distance vertex.names \r\n\r\n Edge attribute names not shown \r\n\r\n#print(network_igraph)\r\n\r\n\r\n\r\nRight off the bat, it looks like the igraph and statnet variables are showing different edges. The network igraph is showing 755 nodes and 23473 edges. The network statnet is showing 755 nodes, and 8228 edges. \r\nWeighted, Directed, Single Mode Network?\r\n\r\n\r\nis_bipartite(network_igraph)\r\n\r\n\r\n[1] FALSE\r\n\r\nis_directed(network_igraph)\r\n\r\n\r\n[1] TRUE\r\n\r\nis_weighted(network_igraph)\r\n\r\n\r\n[1] FALSE\r\n\r\nUsing the Network Igraph set, we have a single mode network, which is directed, and is not weighted.\r\n\r\nLooking at Vertex and Edge Attributes:\r\n\r\n\r\nvertex_attr_names(network_igraph)\r\n\r\n\r\n[1] \"name\"     \"City\"     \"Position\"\r\n\r\nnetwork::list.vertex.attributes(network_statnet)\r\n\r\n\r\n[1] \"City\"         \"Distance\"     \"na\"           \"vertex.names\"\r\n\r\nedge_attr_names(network_igraph)\r\n\r\n\r\n[1] \"Carrier\"    \"Departures\" \"Seats\"      \"Passengers\" \"Aircraft\"  \r\n[6] \"Distance\"  \r\n\r\nnetwork::list.edge.attributes(network_statnet)\r\n\r\n\r\n[1] \"Aircraft\"   \"Carrier\"    \"Departures\" \"Distance\"   \"na\"        \r\n[6] \"Passangers\" \"Seats\"      \"weight\"    \r\n\r\nIgraph Attribute Names: name, City, Position\r\nIgraph edge names: Carrier, Departures, Seats, Passengers, Aircraft, Distance\r\nStatnet attribute names: City, Distance, na, vertex.names\r\nstatnet edge names: Aircraft, Carrier, Departures, Distance, na, Passangers, Seats, weight\r\n\r\nAccessing Attribute DATA:\r\n\r\n\r\nV(network_igraph)$name %>% head()\r\n\r\n\r\n[1] \"BGR\" \"BOS\" \"ANC\" \"JFK\" \"LAS\" \"MIA\"\r\n\r\nV(network_igraph)$City %>% head()\r\n\r\n\r\n[1] \"Bangor, ME\"    \"Boston, MA\"    \"Anchorage, AK\" \"New York, NY\" \r\n[5] \"Las Vegas, NV\" \"Miami, FL\"    \r\n\r\nV(network_igraph)$Position %>% head()\r\n\r\n\r\n[1] \"N444827 W0684941\" \"N422152 W0710019\" \"N611028 W1495947\"\r\n[4] \"N403823 W0734644\" \"N360449 W1150908\" \"N254736 W0801726\"\r\n\r\n(network_igraph)$Carrier %>% head()\r\n\r\n\r\nNULL\r\n\r\nhead(network_statnet %v% \"vertex.names\")\r\n\r\n\r\n[1] \"1G4\" \"A23\" \"A27\" \"A29\" \"ABE\" \"ABI\"\r\n\r\nhead(network_statnet %v% \"City\")\r\n\r\n\r\n[1] \"Bangor, ME\"    \"Boston, MA\"    \"Anchorage, AK\" \"New York, NY\" \r\n[5] \"Las Vegas, NV\" \"Miami, FL\"    \r\n\r\nhead(network_statnet %e% \"weight\")\r\n\r\n\r\n[1] \"193\"  \"253\"  \"141\"  \"3135\" \"4097\" \"1353\"\r\n\r\n\r\nSummarizing Attribute DATA\r\n\r\n\r\nsummary(E(network_igraph)$Distance)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n      0     223     496     639     903    6089 \r\n\r\nsummary(network_statnet %e% \"Distance\")\r\n\r\n\r\n   Length     Class      Mode \r\n     8228 character character \r\n\r\nThe way the summary function worked on the statnet set makes me think the statnet dataset is incorrectly set up at the moment.  #### Dyad Census\r\n\r\n\r\ndyad.census(network_igraph)\r\n\r\n\r\n$mut\r\n[1] 10449\r\n\r\n$asym\r\n[1] 2574\r\n\r\n$null\r\n[1] 271612\r\n\r\nsna::dyad.census(network_statnet)\r\n\r\n\r\n      Mut Asym   Null\r\n[1,] 3605 1018 280012\r\n\r\n\r\nTriad Census\r\n\r\n\r\ntriad.census(network_igraph)\r\n\r\n\r\n [1] 68169544   665870  2427052     1445     1289     2465    15322\r\n [8]    19171       91       39   114868      202      376      558\r\n[15]     6422    18671\r\n\r\nsna::triad.census(network_statnet)\r\n\r\n\r\n          003    012     102 021D 021U 021C  111D  111U 030T 030C\r\n[1,] 68169544 712579 2380343 1445 1289 2465 15322 19171   91   39\r\n        201 120D 120U 120C  210   300\r\n[1,] 114868  202  376  558 6422 18671\r\n\r\n\r\nTransivity\r\n\r\n\r\ntransitivity(network_igraph)\r\n\r\n\r\n[1] 0.3384609\r\n\r\ngtrans(network_statnet)\r\n\r\n\r\n[1] 0.3266617\r\n\r\nThe transitivity for igraph and statnet data sets were pretty close. \r\nLocal Transivity\r\n\r\n\r\nfirst_five_names <- V(network_igraph)$name %>% head(5)\r\nfirst_five_names\r\n\r\n\r\n[1] \"BGR\" \"BOS\" \"ANC\" \"JFK\" \"LAS\"\r\n\r\nfirst_five_transivity <- transitivity(network_igraph, type = \"local\", vids = V(network_igraph)[first_five_names])\r\n\r\ncbind(first_five_names,first_five_transivity)\r\n\r\n\r\n     first_five_names first_five_transivity\r\n[1,] \"BGR\"            \"0.581818181818182\"  \r\n[2,] \"BOS\"            \"0.35292389068469\"   \r\n[3,] \"ANC\"            \"0.0824960338445267\" \r\n[4,] \"JFK\"            \"0.385964912280702\"  \r\n[5,] \"LAS\"            \"0.223852116875373\"  \r\n\r\ntransitivity(network_igraph, type = \"global\")\r\n\r\n\r\n[1] 0.3384609\r\n\r\ntransitivity(network_igraph, type = \"average\")\r\n\r\n\r\n[1] 0.6452844\r\n\r\nLA seems to have low transivity while BGR has the highest at .58.\r\n\r\nDistances in the Network\r\n\r\n\r\ndistances(network_igraph, \"BGR\",\"BOS\")\r\n\r\n\r\n    BOS\r\nBGR   1\r\n\r\ndistances(network_igraph,\"BOS\", \"ANC\")\r\n\r\n\r\n    ANC\r\nBOS   2\r\n\r\naverage.path.length(network_igraph)\r\n\r\n\r\n[1] 3.52743\r\n\r\naverage.path.length(network_igraph, directed = F)\r\n\r\n\r\n[1] 3.447169\r\n\r\n\r\nIdentifying Isolates\r\n\r\n\r\nnames(igraph::components(network_igraph))\r\n\r\n\r\n[1] \"membership\" \"csize\"      \"no\"        \r\n\r\ncomponents(network_igraph)$no\r\n\r\n\r\n[1] 6\r\n\r\ncomponents(network_igraph)$csize\r\n\r\n\r\n[1] 745   2   2   3   2   1\r\n\r\ncomponents(network_igraph)$membership %>% head()\r\n\r\n\r\nBGR BOS ANC JFK LAS MIA \r\n  1   1   1   1   1   1 \r\n\r\n#Isolates\r\nisolates(network_statnet)\r\n\r\n\r\n[1] 166\r\n\r\nas.vector(network_statnet %v% \"vertex.names\")[c(isolates(network_statnet))]\r\n\r\n\r\n[1] \"DET\"\r\n\r\nDetroit seems to be the only isolate.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-04T10:11:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Intro Message",
    "description": "Welcome to my Blog.",
    "author": [
      {
        "name": "Peter",
        "url": {}
      }
    ],
    "date": "2022-02-01",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-03T15:08:59-05:00",
    "input_file": {}
  }
]
